{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thlearningcoder/Phishing_Email/blob/main/Phishing_URL_Checker%5B1%5D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iScCyTjEwGkh"
      },
      "source": [
        "# Phishing URL Checker"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install catboost"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6PbiO2h_0zMo",
        "outputId": "59c57a00-2996-4642-f83d-7bd341b578fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting catboost\n",
            "  Downloading catboost-1.2-cp310-cp310-manylinux2014_x86_64.whl (98.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from catboost) (0.20.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from catboost) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from catboost) (1.22.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from catboost) (1.10.1)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.10/dist-packages (from catboost) (1.5.3)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from catboost) (5.13.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from catboost) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2022.7.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (23.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (3.0.9)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.0.7)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (4.39.3)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (8.4.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.4.4)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly->catboost) (8.2.2)\n",
            "Installing collected packages: catboost\n",
            "Successfully installed catboost-1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "RqR0FdS2wGkk"
      },
      "outputs": [],
      "source": [
        "# Importing requried models\n",
        "# Data preprocessing libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# data visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Machine learning models libraries\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "import xgboost as xgb\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "# Deep learning models libraries\n",
        "import tensorflow as tf\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import LSTM, GRU, Dropout, Dense, Input, Flatten, Conv2D, MaxPooling2D, SimpleRNN\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.utils import np_utils, to_categorical\n",
        "\n",
        "# Evalution measures\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
        "from sklearn import metrics\n",
        "\n",
        "# Saving the models\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2s4riOKuwGkl"
      },
      "source": [
        "## Loading the Final Pre-processed Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ygY2ZXIdwGkn"
      },
      "outputs": [],
      "source": [
        "final_dataset = pd.read_csv(\"/content/phishing_after_preprocess.csv\")\n",
        "final_dataset.drop('id',axis=1,inplace=True)\n",
        "final_dataset['Result'] = final_dataset['Result'].replace(-1, 0)\n",
        "\n",
        "# print(final_dataset.head())\n",
        "# print(final_dataset.shape)\n",
        "# print(final_dataset.dtypes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GiaTsup3wGkn",
        "outputId": "50c4cdb5-1946-447b-f235-363a10708357"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1    6157\n",
              "0    4898\n",
              "Name: Result, dtype: int64"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "final_dataset['Result'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UJQ8ZTSswGko"
      },
      "outputs": [],
      "source": [
        "# Dividing the dataset into train and test data for model training and evaluation\n",
        "X = final_dataset.drop('Result', axis=1)\n",
        "y = final_dataset['Result']\n",
        "train_data, test_data, train_labels, test_labels = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# print(len(train_data))\n",
        "# print(len(train_labels))\n",
        "# print(len(test_data))\n",
        "# print(len(test_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Drvpj16bwGko",
        "outputId": "c3e7130a-9e62-4299-fb1d-1c66623d17cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of samples in each class before SMOTE on train data: 1    4902\n",
            "0    3942\n",
            "Name: Result, dtype: int64\n",
            "Number of samples in each class after SMOTE on train data: 1    4902\n",
            "0    4902\n",
            "Name: Result, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Count the number of instances for each class\n",
        "class_counts_before = train_labels.value_counts()\n",
        "\n",
        "# Print the number of samples in each class before SMOTE\n",
        "print(\"Number of samples in each class before SMOTE on train data:\", class_counts_before)\n",
        "\n",
        "# Create an instance of SMOTE\n",
        "smote = SMOTE()\n",
        "\n",
        "# Apply SMOTE to the data\n",
        "train_data, train_labels = smote.fit_resample(train_data, train_labels)\n",
        "\n",
        "# Count the number of instances for each class\n",
        "class_counts_after = train_labels.value_counts()\n",
        "\n",
        "# Print the number of samples in each class after SMOTE\n",
        "print(\"Number of samples in each class after SMOTE on train data:\", class_counts_after)\n",
        "\n",
        "class_counts_before = pd.DataFrame(data={\"Result\": [\"Legitimate\", \"Phishing\"], \"count\": [class_counts_before[1], class_counts_before[0]]})\n",
        "class_counts_after = pd.DataFrame(data={\"Result\": [\"Legitimate\", \"Phishing\"], \"count\": [class_counts_after[1], class_counts_after[0]]})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "id": "0BISS8JewGkp",
        "outputId": "da3ec879-3206-4621-f1d0-24cb8b5a9bf4"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Countplot [Before and after]\n",
        "fig, axes = plt.subplots(1, 2, sharex=True, figsize=(10,5))\n",
        "\n",
        "sns.barplot(ax=axes[0], y=\"count\", x=\"Result\", data=class_counts_before, palette = [\"green\", \"aqua\"], orient=\"v\", width=0.5)\n",
        "\n",
        "for i in axes[0].containers:\n",
        "    axes[0].bar_label(i,)\n",
        "\n",
        "sns.barplot(ax=axes[1], y=\"count\", x=\"Result\", data=class_counts_after, palette = [\"green\", \"aqua\"], orient=\"v\", width=0.5)\n",
        "\n",
        "for i in axes[1].containers:\n",
        "    axes[1].bar_label(i,)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2U2zZovwGkp"
      },
      "source": [
        "# Machine Learning Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HDBcnWfRwGkp"
      },
      "outputs": [],
      "source": [
        "ml_names = ['lr', 'nb', 'dt', 'svm', 'knn']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxjh5xW9wGkq"
      },
      "source": [
        "### 1. Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "5JxDVMKNwGkq",
        "outputId": "3f2a1767-e3da-4111-bbe7-8bb1db649ae4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(max_iter=1000)"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=1000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "# Fit logistic regression model\n",
        "model_lr = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
        "model_lr.fit(train_data, train_labels)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cm = confusion_matrix(train_data, train_labels)\n",
        "print (\"Confusion Matrix for training : \\n\",cm)\n",
        "\n",
        "acc = accuracy_score(train_data, train_labels)*100\n",
        "print(\"Accuracy for training : \",acc)\n",
        "\n",
        "precision = metrics.precision_score(train_data, train_labels)\n",
        "print(\"Precision for training : \",precision)\n",
        "\n",
        "recall = recall_score(train_data, train_labels)\n",
        "print(\"Recall for training : \",recall)\n",
        "\n",
        "f1_score = metrics.f1_score(train_data, train_labels)\n",
        "print(\"F1-Score for training : \",f1_score)\n",
        "\n",
        "print(classification_report(train_data, train_labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "CAOEr5Kcyi2U",
        "outputId": "4fc932f6-6bd9-4845-8080-25726c3e3557"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-512f01091e91>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Confusion Matrix for training : \\n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracy for training : \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mconfusion_matrix\u001b[0;34m(y_true, y_pred, labels, sample_weight, normalize)\u001b[0m\n\u001b[1;32m    315\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m     \"\"\"\n\u001b[0;32m--> 317\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    318\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multiclass\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s is not supported\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m     96\u001b[0m             \"Classification metrics can't handle a mix of {0} and {1} targets\".format(\n\u001b[1;32m     97\u001b[0m                 \u001b[0mtype_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype_pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Classification metrics can't handle a mix of multiclass-multioutput and binary targets"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZTDMo5owGkq"
      },
      "source": [
        "### 2. Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D0j9RWqXwGkq",
        "outputId": "d12d63fa-11cb-4f1a-94f5-94dca18ec499"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GaussianNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GaussianNB</label><div class=\"sk-toggleable__content\"><pre>GaussianNB()</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "GaussianNB()"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Fit Gaussian Naive Bayes model\n",
        "model_nb = GaussianNB()\n",
        "model_nb.fit(train_data, train_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qO3cnEwmwGkr"
      },
      "source": [
        "### 3. Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7elHei_ZwGkr",
        "outputId": "3e2f0963-1223-4f54-e4ea-3103165b9440"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier(max_depth=3, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(max_depth=3, random_state=42)</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "DecisionTreeClassifier(max_depth=3, random_state=42)"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Fit Decision Tree model\n",
        "model_dt = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "model_dt.fit(train_data, train_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97r_IdPnwGkr"
      },
      "source": [
        "### 4. Support Vector Machines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EM36IQArwGks",
        "outputId": "bc9f8362-1a90-4e70-d548-6d1241e1d34a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(C=1, kernel=&#x27;linear&#x27;, probability=True, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(C=1, kernel=&#x27;linear&#x27;, probability=True, random_state=42)</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "SVC(C=1, kernel='linear', probability=True, random_state=42)"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Fit Support Vector Machine Classifier (SVM) model\n",
        "model_svm = SVC(kernel='linear', C=1, random_state=42, probability=True)\n",
        "model_svm.fit(train_data, train_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tjf_ajsIwGks"
      },
      "source": [
        "### 5. K-Nearest Neighbors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L2V91ZlSwGks",
        "outputId": "271f5304-7b88-47bb-f780-01e9f1652965"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KNeighborsClassifier(n_neighbors=10)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsClassifier</label><div class=\"sk-toggleable__content\"><pre>KNeighborsClassifier(n_neighbors=10)</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "KNeighborsClassifier(n_neighbors=10)"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Fit K-Nearest Neighbors model\n",
        "model_knn = KNeighborsClassifier(n_neighbors=10)\n",
        "model_knn.fit(train_data, train_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3tea5X8wGks"
      },
      "source": [
        "# Ensemble Learning models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mbkEGokWwGks"
      },
      "outputs": [],
      "source": [
        "em_names = ['rf', 'bagging', 'xgb', 'ada', 'cb', 'gb', 'stacking', \"vh\", \"vs\", \"blend\"]\n",
        "\n",
        "# Create a list of the models to be ensembled\n",
        "em_models = [('lr', model_lr), ('nb', model_nb), ('dt', model_dt), ('svm', model_svm), ('knn', model_knn)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtQDOXd-wGks"
      },
      "source": [
        "### 1. Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_5q6tZptwGkt",
        "outputId": "759081af-0821-47a1-8a9e-7992628f4657"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-6 {color: black;background-color: white;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(max_depth=3, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" checked><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(max_depth=3, random_state=42)</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "RandomForestClassifier(max_depth=3, random_state=42)"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Fit Random forest model\n",
        "model_rf = RandomForestClassifier(n_estimators=100, max_depth=3, random_state=42)\n",
        "model_rf.fit(train_data, train_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwTCWpnVwGkt"
      },
      "source": [
        "### 2. Bagging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yMgkV5GnwGkt"
      },
      "outputs": [],
      "source": [
        "# Aggregate the predictions made by each BaggingClassifier\n",
        "y_preds = []\n",
        "for name, model in em_models:\n",
        "    # Create a BaggingClassifier object for the current model\n",
        "    bagging_model = BaggingClassifier(model, n_estimators=10, max_samples=0.8, random_state=42)\n",
        "    \n",
        "    # Train the BaggingClassifier on the training set\n",
        "    bagging_model.fit(train_data, train_labels)\n",
        "\n",
        "    # Make predictions on the testing set\n",
        "    y_pred = bagging_model.predict_proba(test_data)[:, 1]\n",
        "    \n",
        "    # Append the predicted probabilities to the list of predictions\n",
        "    y_preds.append(y_pred)\n",
        "\n",
        "# Aggregate the predictions using weighted voting\n",
        "weights = [0.2, 0.2, 0.2, 0.2, 0.2]\n",
        "model_bagging = np.average(y_preds, axis=0, weights=weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZ6miQuCwGkt"
      },
      "source": [
        "### 3. XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3KtPwQHWwGkt",
        "outputId": "63bb69e8-4eaf-4660-e2ad-8042acc2a9e0"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-7 {color: black;background-color: white;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
              "              colsample_bylevel=None, colsample_bynode=None,\n",
              "              colsample_bytree=None, early_stopping_rounds=None,\n",
              "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
              "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
              "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
              "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
              "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
              "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
              "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
              "              predictor=None, random_state=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" checked><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
              "              colsample_bylevel=None, colsample_bynode=None,\n",
              "              colsample_bytree=None, early_stopping_rounds=None,\n",
              "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
              "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
              "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
              "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
              "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
              "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
              "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
              "              predictor=None, random_state=None, ...)</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
              "              colsample_bylevel=None, colsample_bynode=None,\n",
              "              colsample_bytree=None, early_stopping_rounds=None,\n",
              "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
              "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
              "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
              "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
              "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
              "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
              "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
              "              predictor=None, random_state=None, ...)"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Fit XGboost model\n",
        "model_xgb = xgb.XGBClassifier()\n",
        "model_xgb.fit(train_data, train_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrxItScDwGku"
      },
      "source": [
        "### 4. Adaboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ul8N6k1CwGku",
        "outputId": "3a476cef-0d29-451a-d678-9fdcc73dac61"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-8 {color: black;background-color: white;}#sk-container-id-8 pre{padding: 0;}#sk-container-id-8 div.sk-toggleable {background-color: white;}#sk-container-id-8 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-8 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-8 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-8 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-8 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-8 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-8 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-8 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-8 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-8 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-8 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-8 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-8 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-8 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-8 div.sk-item {position: relative;z-index: 1;}#sk-container-id-8 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-8 div.sk-item::before, #sk-container-id-8 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-8 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-8 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-8 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-8 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-8 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-8 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-8 div.sk-label-container {text-align: center;}#sk-container-id-8 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-8 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-8\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>AdaBoostClassifier(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" checked><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">AdaBoostClassifier</label><div class=\"sk-toggleable__content\"><pre>AdaBoostClassifier(random_state=42)</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "AdaBoostClassifier(random_state=42)"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create an AdaBoost classifier with 50 estimators\n",
        "model_ada = AdaBoostClassifier(n_estimators=50, random_state=42)\n",
        "\n",
        "# Fit the AdaBoost classifier to the training data\n",
        "model_ada.fit(train_data, train_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fx5zyTC1wGku"
      },
      "source": [
        "### 5. Catboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ixZxBsOVwGku",
        "outputId": "bc00e339-6320-4e78-fb7c-ce3e2eea1dd5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0:\tlearn: 0.5324410\ttotal: 144ms\tremaining: 14.3s\n",
            "1:\tlearn: 0.4410140\ttotal: 150ms\tremaining: 7.35s\n",
            "2:\tlearn: 0.3684891\ttotal: 155ms\tremaining: 5.02s\n",
            "3:\tlearn: 0.3112870\ttotal: 160ms\tremaining: 3.85s\n",
            "4:\tlearn: 0.2815710\ttotal: 166ms\tremaining: 3.15s\n",
            "5:\tlearn: 0.2545360\ttotal: 171ms\tremaining: 2.68s\n",
            "6:\tlearn: 0.2345690\ttotal: 176ms\tremaining: 2.34s\n",
            "7:\tlearn: 0.2177465\ttotal: 181ms\tremaining: 2.08s\n",
            "8:\tlearn: 0.2012072\ttotal: 186ms\tremaining: 1.88s\n",
            "9:\tlearn: 0.1895561\ttotal: 191ms\tremaining: 1.72s\n",
            "10:\tlearn: 0.1793486\ttotal: 196ms\tremaining: 1.59s\n",
            "11:\tlearn: 0.1730270\ttotal: 202ms\tremaining: 1.48s\n",
            "12:\tlearn: 0.1671964\ttotal: 208ms\tremaining: 1.39s\n",
            "13:\tlearn: 0.1627445\ttotal: 213ms\tremaining: 1.31s\n",
            "14:\tlearn: 0.1594325\ttotal: 219ms\tremaining: 1.24s\n",
            "15:\tlearn: 0.1550009\ttotal: 224ms\tremaining: 1.17s\n",
            "16:\tlearn: 0.1521465\ttotal: 229ms\tremaining: 1.12s\n",
            "17:\tlearn: 0.1493404\ttotal: 234ms\tremaining: 1.06s\n",
            "18:\tlearn: 0.1460430\ttotal: 240ms\tremaining: 1.02s\n",
            "19:\tlearn: 0.1435994\ttotal: 245ms\tremaining: 979ms\n",
            "20:\tlearn: 0.1417055\ttotal: 250ms\tremaining: 940ms\n",
            "21:\tlearn: 0.1389280\ttotal: 255ms\tremaining: 904ms\n",
            "22:\tlearn: 0.1369414\ttotal: 260ms\tremaining: 870ms\n",
            "23:\tlearn: 0.1349272\ttotal: 265ms\tremaining: 838ms\n",
            "24:\tlearn: 0.1331142\ttotal: 270ms\tremaining: 810ms\n",
            "25:\tlearn: 0.1307396\ttotal: 275ms\tremaining: 783ms\n",
            "26:\tlearn: 0.1289570\ttotal: 281ms\tremaining: 759ms\n",
            "27:\tlearn: 0.1272849\ttotal: 286ms\tremaining: 735ms\n",
            "28:\tlearn: 0.1261248\ttotal: 291ms\tremaining: 713ms\n",
            "29:\tlearn: 0.1251491\ttotal: 296ms\tremaining: 691ms\n",
            "30:\tlearn: 0.1238344\ttotal: 301ms\tremaining: 671ms\n",
            "31:\tlearn: 0.1219281\ttotal: 307ms\tremaining: 652ms\n",
            "32:\tlearn: 0.1210265\ttotal: 312ms\tremaining: 634ms\n",
            "33:\tlearn: 0.1199428\ttotal: 317ms\tremaining: 616ms\n",
            "34:\tlearn: 0.1184887\ttotal: 323ms\tremaining: 600ms\n",
            "35:\tlearn: 0.1171850\ttotal: 329ms\tremaining: 585ms\n",
            "36:\tlearn: 0.1163560\ttotal: 336ms\tremaining: 572ms\n",
            "37:\tlearn: 0.1152722\ttotal: 344ms\tremaining: 561ms\n",
            "38:\tlearn: 0.1133156\ttotal: 349ms\tremaining: 546ms\n",
            "39:\tlearn: 0.1125478\ttotal: 355ms\tremaining: 533ms\n",
            "40:\tlearn: 0.1117909\ttotal: 361ms\tremaining: 519ms\n",
            "41:\tlearn: 0.1108952\ttotal: 367ms\tremaining: 506ms\n",
            "42:\tlearn: 0.1098883\ttotal: 372ms\tremaining: 494ms\n",
            "43:\tlearn: 0.1088004\ttotal: 378ms\tremaining: 482ms\n",
            "44:\tlearn: 0.1076010\ttotal: 384ms\tremaining: 470ms\n",
            "45:\tlearn: 0.1064133\ttotal: 390ms\tremaining: 458ms\n",
            "46:\tlearn: 0.1056827\ttotal: 395ms\tremaining: 445ms\n",
            "47:\tlearn: 0.1045375\ttotal: 400ms\tremaining: 433ms\n",
            "48:\tlearn: 0.1038345\ttotal: 405ms\tremaining: 421ms\n",
            "49:\tlearn: 0.1030884\ttotal: 410ms\tremaining: 410ms\n",
            "50:\tlearn: 0.1022073\ttotal: 415ms\tremaining: 399ms\n",
            "51:\tlearn: 0.1013476\ttotal: 420ms\tremaining: 387ms\n",
            "52:\tlearn: 0.1002717\ttotal: 425ms\tremaining: 377ms\n",
            "53:\tlearn: 0.0994891\ttotal: 430ms\tremaining: 366ms\n",
            "54:\tlearn: 0.0985616\ttotal: 435ms\tremaining: 356ms\n",
            "55:\tlearn: 0.0976924\ttotal: 439ms\tremaining: 345ms\n",
            "56:\tlearn: 0.0967775\ttotal: 444ms\tremaining: 335ms\n",
            "57:\tlearn: 0.0961141\ttotal: 449ms\tremaining: 325ms\n",
            "58:\tlearn: 0.0953283\ttotal: 454ms\tremaining: 315ms\n",
            "59:\tlearn: 0.0945790\ttotal: 458ms\tremaining: 306ms\n",
            "60:\tlearn: 0.0937970\ttotal: 463ms\tremaining: 296ms\n",
            "61:\tlearn: 0.0931791\ttotal: 468ms\tremaining: 287ms\n",
            "62:\tlearn: 0.0926204\ttotal: 473ms\tremaining: 278ms\n",
            "63:\tlearn: 0.0920012\ttotal: 478ms\tremaining: 269ms\n",
            "64:\tlearn: 0.0912676\ttotal: 484ms\tremaining: 260ms\n",
            "65:\tlearn: 0.0902214\ttotal: 489ms\tremaining: 252ms\n",
            "66:\tlearn: 0.0899031\ttotal: 495ms\tremaining: 244ms\n",
            "67:\tlearn: 0.0889338\ttotal: 500ms\tremaining: 235ms\n",
            "68:\tlearn: 0.0884375\ttotal: 505ms\tremaining: 227ms\n",
            "69:\tlearn: 0.0878389\ttotal: 510ms\tremaining: 219ms\n",
            "70:\tlearn: 0.0873475\ttotal: 515ms\tremaining: 211ms\n",
            "71:\tlearn: 0.0869932\ttotal: 520ms\tremaining: 202ms\n",
            "72:\tlearn: 0.0862603\ttotal: 525ms\tremaining: 194ms\n",
            "73:\tlearn: 0.0857815\ttotal: 532ms\tremaining: 187ms\n",
            "74:\tlearn: 0.0851483\ttotal: 539ms\tremaining: 180ms\n",
            "75:\tlearn: 0.0847953\ttotal: 544ms\tremaining: 172ms\n",
            "76:\tlearn: 0.0843679\ttotal: 549ms\tremaining: 164ms\n",
            "77:\tlearn: 0.0840476\ttotal: 555ms\tremaining: 157ms\n",
            "78:\tlearn: 0.0836460\ttotal: 561ms\tremaining: 149ms\n",
            "79:\tlearn: 0.0833527\ttotal: 567ms\tremaining: 142ms\n",
            "80:\tlearn: 0.0830315\ttotal: 572ms\tremaining: 134ms\n",
            "81:\tlearn: 0.0823533\ttotal: 578ms\tremaining: 127ms\n",
            "82:\tlearn: 0.0818566\ttotal: 584ms\tremaining: 120ms\n",
            "83:\tlearn: 0.0815485\ttotal: 590ms\tremaining: 112ms\n",
            "84:\tlearn: 0.0811786\ttotal: 596ms\tremaining: 105ms\n",
            "85:\tlearn: 0.0809566\ttotal: 601ms\tremaining: 97.9ms\n",
            "86:\tlearn: 0.0804823\ttotal: 607ms\tremaining: 90.7ms\n",
            "87:\tlearn: 0.0797473\ttotal: 612ms\tremaining: 83.5ms\n",
            "88:\tlearn: 0.0793487\ttotal: 617ms\tremaining: 76.3ms\n",
            "89:\tlearn: 0.0784963\ttotal: 623ms\tremaining: 69.2ms\n",
            "90:\tlearn: 0.0781510\ttotal: 628ms\tremaining: 62.1ms\n",
            "91:\tlearn: 0.0774125\ttotal: 633ms\tremaining: 55ms\n",
            "92:\tlearn: 0.0770046\ttotal: 638ms\tremaining: 48ms\n",
            "93:\tlearn: 0.0762466\ttotal: 644ms\tremaining: 41.1ms\n",
            "94:\tlearn: 0.0758976\ttotal: 649ms\tremaining: 34.2ms\n",
            "95:\tlearn: 0.0753995\ttotal: 656ms\tremaining: 27.3ms\n",
            "96:\tlearn: 0.0748108\ttotal: 662ms\tremaining: 20.5ms\n",
            "97:\tlearn: 0.0742420\ttotal: 668ms\tremaining: 13.6ms\n",
            "98:\tlearn: 0.0734592\ttotal: 674ms\tremaining: 6.8ms\n",
            "99:\tlearn: 0.0730832\ttotal: 679ms\tremaining: 0us\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<catboost.core.CatBoostClassifier at 0x24857752f20>"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create an instance of the CatBoostClassifier\n",
        "model_cb = CatBoostClassifier(iterations=100, learning_rate=0.1, depth=6)\n",
        "\n",
        "# Fit the model on the training data\n",
        "model_cb.fit(train_data, train_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "He1kS1BVwGku"
      },
      "source": [
        "### 6. Gradient Boosting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h29sDtqawGku",
        "outputId": "c17403e6-a023-4f70-e9ed-2f9b0544dc06"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-9 {color: black;background-color: white;}#sk-container-id-9 pre{padding: 0;}#sk-container-id-9 div.sk-toggleable {background-color: white;}#sk-container-id-9 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-9 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-9 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-9 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-9 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-9 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-9 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-9 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-9 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-9 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-9 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-9 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-9 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-9 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-9 div.sk-item {position: relative;z-index: 1;}#sk-container-id-9 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-9 div.sk-item::before, #sk-container-id-9 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-9 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-9 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-9 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-9 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-9 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-9 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-9 div.sk-label-container {text-align: center;}#sk-container-id-9 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-9 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-9\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GradientBoostingClassifier(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" checked><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingClassifier</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingClassifier(random_state=42)</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "GradientBoostingClassifier(random_state=42)"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create a Gradient Boosting Classifier object\n",
        "model_gb = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "\n",
        "# Train the Gradient Boosting Classifier on the training set\n",
        "model_gb.fit(train_data, train_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Zn6us3mwGku"
      },
      "source": [
        "### 7. Stacking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JfQAv-4HwGku",
        "outputId": "5865fc0d-4a40-4424-87ad-2707579f65cb"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-10 {color: black;background-color: white;}#sk-container-id-10 pre{padding: 0;}#sk-container-id-10 div.sk-toggleable {background-color: white;}#sk-container-id-10 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-10 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-10 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-10 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-10 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-10 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-10 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-10 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-10 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-10 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-10 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-10 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-10 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-10 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-10 div.sk-item {position: relative;z-index: 1;}#sk-container-id-10 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-10 div.sk-item::before, #sk-container-id-10 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-10 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-10 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-10 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-10 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-10 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-10 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-10 div.sk-label-container {text-align: center;}#sk-container-id-10 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-10 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-10\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>StackingClassifier(estimators=[(&#x27;lr&#x27;, LogisticRegression(max_iter=1000)),\n",
              "                               (&#x27;nb&#x27;, GaussianNB()),\n",
              "                               (&#x27;dt&#x27;,\n",
              "                                DecisionTreeClassifier(max_depth=3,\n",
              "                                                       random_state=42)),\n",
              "                               (&#x27;svm&#x27;,\n",
              "                                SVC(C=1, kernel=&#x27;linear&#x27;, probability=True,\n",
              "                                    random_state=42)),\n",
              "                               (&#x27;knn&#x27;, KNeighborsClassifier(n_neighbors=10))],\n",
              "                   final_estimator=LogisticRegression())</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" ><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StackingClassifier</label><div class=\"sk-toggleable__content\"><pre>StackingClassifier(estimators=[(&#x27;lr&#x27;, LogisticRegression(max_iter=1000)),\n",
              "                               (&#x27;nb&#x27;, GaussianNB()),\n",
              "                               (&#x27;dt&#x27;,\n",
              "                                DecisionTreeClassifier(max_depth=3,\n",
              "                                                       random_state=42)),\n",
              "                               (&#x27;svm&#x27;,\n",
              "                                SVC(C=1, kernel=&#x27;linear&#x27;, probability=True,\n",
              "                                    random_state=42)),\n",
              "                               (&#x27;knn&#x27;, KNeighborsClassifier(n_neighbors=10))],\n",
              "                   final_estimator=LogisticRegression())</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>lr</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" ><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>nb</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" ><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GaussianNB</label><div class=\"sk-toggleable__content\"><pre>GaussianNB()</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>dt</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\" ><label for=\"sk-estimator-id-13\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(max_depth=3, random_state=42)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>svm</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\" ><label for=\"sk-estimator-id-14\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(C=1, kernel=&#x27;linear&#x27;, probability=True, random_state=42)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>knn</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-15\" type=\"checkbox\" ><label for=\"sk-estimator-id-15\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsClassifier</label><div class=\"sk-toggleable__content\"><pre>KNeighborsClassifier(n_neighbors=10)</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>final_estimator</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-16\" type=\"checkbox\" ><label for=\"sk-estimator-id-16\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
            ],
            "text/plain": [
              "StackingClassifier(estimators=[('lr', LogisticRegression(max_iter=1000)),\n",
              "                               ('nb', GaussianNB()),\n",
              "                               ('dt',\n",
              "                                DecisionTreeClassifier(max_depth=3,\n",
              "                                                       random_state=42)),\n",
              "                               ('svm',\n",
              "                                SVC(C=1, kernel='linear', probability=True,\n",
              "                                    random_state=42)),\n",
              "                               ('knn', KNeighborsClassifier(n_neighbors=10))],\n",
              "                   final_estimator=LogisticRegression())"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_stacking = StackingClassifier(estimators=em_models, final_estimator=LogisticRegression())\n",
        "\n",
        "# Fit the stacking model on the training data\n",
        "model_stacking.fit(train_data, train_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_W1NIS4owGkv"
      },
      "source": [
        "### 8. Voting [Hard]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "oOBUZqnuwGkv",
        "outputId": "4624c0a6-ebae-4b42-fc48-9210c4fba693"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-11 {color: black;background-color: white;}#sk-container-id-11 pre{padding: 0;}#sk-container-id-11 div.sk-toggleable {background-color: white;}#sk-container-id-11 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-11 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-11 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-11 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-11 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-11 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-11 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-11 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-11 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-11 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-11 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-11 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-11 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-11 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-11 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-11 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-11 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-11 div.sk-item {position: relative;z-index: 1;}#sk-container-id-11 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-11 div.sk-item::before, #sk-container-id-11 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-11 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-11 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-11 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-11 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-11 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-11 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-11 div.sk-label-container {text-align: center;}#sk-container-id-11 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-11 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-11\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>VotingClassifier(estimators=[(&#x27;lr&#x27;, LogisticRegression(max_iter=1000)),\n",
              "                             (&#x27;nb&#x27;, GaussianNB()),\n",
              "                             (&#x27;dt&#x27;,\n",
              "                              DecisionTreeClassifier(max_depth=3,\n",
              "                                                     random_state=42)),\n",
              "                             (&#x27;svm&#x27;,\n",
              "                              SVC(C=1, kernel=&#x27;linear&#x27;, probability=True,\n",
              "                                  random_state=42)),\n",
              "                             (&#x27;knn&#x27;, KNeighborsClassifier(n_neighbors=10))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-17\" type=\"checkbox\" ><label for=\"sk-estimator-id-17\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">VotingClassifier</label><div class=\"sk-toggleable__content\"><pre>VotingClassifier(estimators=[(&#x27;lr&#x27;, LogisticRegression(max_iter=1000)),\n",
              "                             (&#x27;nb&#x27;, GaussianNB()),\n",
              "                             (&#x27;dt&#x27;,\n",
              "                              DecisionTreeClassifier(max_depth=3,\n",
              "                                                     random_state=42)),\n",
              "                             (&#x27;svm&#x27;,\n",
              "                              SVC(C=1, kernel=&#x27;linear&#x27;, probability=True,\n",
              "                                  random_state=42)),\n",
              "                             (&#x27;knn&#x27;, KNeighborsClassifier(n_neighbors=10))])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>lr</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-18\" type=\"checkbox\" ><label for=\"sk-estimator-id-18\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>nb</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-19\" type=\"checkbox\" ><label for=\"sk-estimator-id-19\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GaussianNB</label><div class=\"sk-toggleable__content\"><pre>GaussianNB()</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>dt</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-20\" type=\"checkbox\" ><label for=\"sk-estimator-id-20\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(max_depth=3, random_state=42)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>svm</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-21\" type=\"checkbox\" ><label for=\"sk-estimator-id-21\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(C=1, kernel=&#x27;linear&#x27;, probability=True, random_state=42)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>knn</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-22\" type=\"checkbox\" ><label for=\"sk-estimator-id-22\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsClassifier</label><div class=\"sk-toggleable__content\"><pre>KNeighborsClassifier(n_neighbors=10)</pre></div></div></div></div></div></div></div></div></div></div>"
            ],
            "text/plain": [
              "VotingClassifier(estimators=[('lr', LogisticRegression(max_iter=1000)),\n",
              "                             ('nb', GaussianNB()),\n",
              "                             ('dt',\n",
              "                              DecisionTreeClassifier(max_depth=3,\n",
              "                                                     random_state=42)),\n",
              "                             ('svm',\n",
              "                              SVC(C=1, kernel='linear', probability=True,\n",
              "                                  random_state=42)),\n",
              "                             ('knn', KNeighborsClassifier(n_neighbors=10))])"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Define the voting classifier with hard voting\n",
        "model_vh = VotingClassifier(estimators=em_models, voting='hard')\n",
        "\n",
        "# Fit the models on the training data\n",
        "model_vh.fit(train_data, train_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXyN-Y0AwGkv"
      },
      "source": [
        "### 9. Voting [Soft]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "dMnBpZzGwGkv",
        "outputId": "c397e6ff-f779-4272-c819-4f4dd86e13c3"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-12 {color: black;background-color: white;}#sk-container-id-12 pre{padding: 0;}#sk-container-id-12 div.sk-toggleable {background-color: white;}#sk-container-id-12 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-12 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-12 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-12 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-12 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-12 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-12 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-12 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-12 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-12 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-12 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-12 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-12 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-12 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-12 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-12 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-12 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-12 div.sk-item {position: relative;z-index: 1;}#sk-container-id-12 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-12 div.sk-item::before, #sk-container-id-12 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-12 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-12 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-12 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-12 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-12 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-12 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-12 div.sk-label-container {text-align: center;}#sk-container-id-12 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-12 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-12\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>VotingClassifier(estimators=[(&#x27;lr&#x27;, LogisticRegression(max_iter=1000)),\n",
              "                             (&#x27;nb&#x27;, GaussianNB()),\n",
              "                             (&#x27;dt&#x27;,\n",
              "                              DecisionTreeClassifier(max_depth=3,\n",
              "                                                     random_state=42)),\n",
              "                             (&#x27;svm&#x27;,\n",
              "                              SVC(C=1, kernel=&#x27;linear&#x27;, probability=True,\n",
              "                                  random_state=42)),\n",
              "                             (&#x27;knn&#x27;, KNeighborsClassifier(n_neighbors=10))],\n",
              "                 voting=&#x27;soft&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-23\" type=\"checkbox\" ><label for=\"sk-estimator-id-23\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">VotingClassifier</label><div class=\"sk-toggleable__content\"><pre>VotingClassifier(estimators=[(&#x27;lr&#x27;, LogisticRegression(max_iter=1000)),\n",
              "                             (&#x27;nb&#x27;, GaussianNB()),\n",
              "                             (&#x27;dt&#x27;,\n",
              "                              DecisionTreeClassifier(max_depth=3,\n",
              "                                                     random_state=42)),\n",
              "                             (&#x27;svm&#x27;,\n",
              "                              SVC(C=1, kernel=&#x27;linear&#x27;, probability=True,\n",
              "                                  random_state=42)),\n",
              "                             (&#x27;knn&#x27;, KNeighborsClassifier(n_neighbors=10))],\n",
              "                 voting=&#x27;soft&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>lr</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-24\" type=\"checkbox\" ><label for=\"sk-estimator-id-24\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>nb</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-25\" type=\"checkbox\" ><label for=\"sk-estimator-id-25\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GaussianNB</label><div class=\"sk-toggleable__content\"><pre>GaussianNB()</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>dt</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-26\" type=\"checkbox\" ><label for=\"sk-estimator-id-26\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(max_depth=3, random_state=42)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>svm</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-27\" type=\"checkbox\" ><label for=\"sk-estimator-id-27\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(C=1, kernel=&#x27;linear&#x27;, probability=True, random_state=42)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>knn</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-28\" type=\"checkbox\" ><label for=\"sk-estimator-id-28\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsClassifier</label><div class=\"sk-toggleable__content\"><pre>KNeighborsClassifier(n_neighbors=10)</pre></div></div></div></div></div></div></div></div></div></div>"
            ],
            "text/plain": [
              "VotingClassifier(estimators=[('lr', LogisticRegression(max_iter=1000)),\n",
              "                             ('nb', GaussianNB()),\n",
              "                             ('dt',\n",
              "                              DecisionTreeClassifier(max_depth=3,\n",
              "                                                     random_state=42)),\n",
              "                             ('svm',\n",
              "                              SVC(C=1, kernel='linear', probability=True,\n",
              "                                  random_state=42)),\n",
              "                             ('knn', KNeighborsClassifier(n_neighbors=10))],\n",
              "                 voting='soft')"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Define the voting classifier with soft voting\n",
        "model_vs = VotingClassifier(estimators=em_models, voting='soft')\n",
        "\n",
        "# Fit the models on the training data\n",
        "model_vs.fit(train_data, train_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXn_-ZaKwGkv"
      },
      "source": [
        "### 10. Blending"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZdTWa-DRwGkv"
      },
      "outputs": [],
      "source": [
        "# Splitting the train data into train and val set\n",
        "train_set, val_set, train_set_labels, val_set_labels = train_test_split(train_data, train_labels, test_size=0.25, random_state=0)\n",
        "\n",
        "# print(len(train_set))\n",
        "# print(len(train_set_labels))\n",
        "# print(len(val_set))\n",
        "# print(len(val_set_labels))\n",
        "# print(len(test_data))\n",
        "# print(len(test_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "5RRjbtS4wGkv",
        "outputId": "978c4af9-aa32-4baa-915f-114f81c7be65"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-13 {color: black;background-color: white;}#sk-container-id-13 pre{padding: 0;}#sk-container-id-13 div.sk-toggleable {background-color: white;}#sk-container-id-13 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-13 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-13 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-13 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-13 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-13 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-13 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-13 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-13 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-13 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-13 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-13 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-13 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-13 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-13 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-13 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-13 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-13 div.sk-item {position: relative;z-index: 1;}#sk-container-id-13 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-13 div.sk-item::before, #sk-container-id-13 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-13 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-13 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-13 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-13 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-13 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-13 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-13 div.sk-label-container {text-align: center;}#sk-container-id-13 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-13 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-13\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-29\" type=\"checkbox\" checked><label for=\"sk-estimator-id-29\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "LogisticRegression()"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "base_predictions = []\n",
        "meta_predictions = []\n",
        "\n",
        "for name, model in em_models:\n",
        "    model.fit(train_set, train_set_labels)\n",
        "    base_prediction = model.predict(val_set)\n",
        "    meta_prediction = model.predict(test_data)\n",
        "    \n",
        "    base_prediction = base_prediction.reshape(len(base_prediction), 1)\n",
        "    meta_prediction = meta_prediction.reshape(len(meta_prediction), 1)\n",
        "    \n",
        "    base_predictions.append(base_prediction)\n",
        "    meta_predictions.append(meta_prediction)\n",
        "\n",
        "base_predictions = np.hstack(base_predictions)\n",
        "meta_predictions = np.hstack(meta_predictions)\n",
        "\n",
        "model_blend = LogisticRegression()\n",
        "model_blend.fit(base_predictions, val_set_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmJBz3tIwGkv"
      },
      "source": [
        "# Deep Learning Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "N2bnmam9wGkw"
      },
      "outputs": [],
      "source": [
        "dl_names = [\"ann\", \"mlp\", \"rnn\", \"lstm\", \"gru\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TsN8M9VRwGkw"
      },
      "outputs": [],
      "source": [
        "# Reshape the features to 2D tensor shape (batch_size, num_features)\n",
        "train_data_2d = train_data.values\n",
        "test_data_2d = test_data.values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ay6hcdq7wGkw"
      },
      "source": [
        "### 1. ANN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "lmLUID2MwGkw",
        "outputId": "cdc0cec9-c8df-4be6-b819-7076d22407bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "298/298 [==============================] - 3s 5ms/step - loss: 0.5419 - accuracy: 0.7119 - val_loss: 0.2829 - val_accuracy: 0.9032\n",
            "Epoch 2/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.3029 - accuracy: 0.8898 - val_loss: 0.2191 - val_accuracy: 0.9159\n",
            "Epoch 3/500\n",
            "298/298 [==============================] - 1s 5ms/step - loss: 0.2509 - accuracy: 0.9101 - val_loss: 0.2052 - val_accuracy: 0.9195\n",
            "Epoch 4/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.2335 - accuracy: 0.9157 - val_loss: 0.1899 - val_accuracy: 0.9258\n",
            "Epoch 5/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.2190 - accuracy: 0.9218 - val_loss: 0.1853 - val_accuracy: 0.9290\n",
            "Epoch 6/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.2058 - accuracy: 0.9266 - val_loss: 0.1798 - val_accuracy: 0.9290\n",
            "Epoch 7/500\n",
            "298/298 [==============================] - 1s 5ms/step - loss: 0.2009 - accuracy: 0.9257 - val_loss: 0.1751 - val_accuracy: 0.9285\n",
            "Epoch 8/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1928 - accuracy: 0.9300 - val_loss: 0.1713 - val_accuracy: 0.9281\n",
            "Epoch 9/500\n",
            "298/298 [==============================] - 1s 5ms/step - loss: 0.1872 - accuracy: 0.9303 - val_loss: 0.1691 - val_accuracy: 0.9303\n",
            "Epoch 10/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1829 - accuracy: 0.9321 - val_loss: 0.1689 - val_accuracy: 0.9290\n",
            "Epoch 11/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1747 - accuracy: 0.9333 - val_loss: 0.1658 - val_accuracy: 0.9322\n",
            "Epoch 12/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1747 - accuracy: 0.9339 - val_loss: 0.1653 - val_accuracy: 0.9299\n",
            "Epoch 13/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1692 - accuracy: 0.9340 - val_loss: 0.1606 - val_accuracy: 0.9349\n",
            "Epoch 14/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1677 - accuracy: 0.9347 - val_loss: 0.1576 - val_accuracy: 0.9331\n",
            "Epoch 15/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1636 - accuracy: 0.9357 - val_loss: 0.1543 - val_accuracy: 0.9358\n",
            "Epoch 16/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1616 - accuracy: 0.9367 - val_loss: 0.1537 - val_accuracy: 0.9362\n",
            "Epoch 17/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1555 - accuracy: 0.9407 - val_loss: 0.1512 - val_accuracy: 0.9371\n",
            "Epoch 18/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1562 - accuracy: 0.9388 - val_loss: 0.1522 - val_accuracy: 0.9353\n",
            "Epoch 19/500\n",
            "298/298 [==============================] - 1s 5ms/step - loss: 0.1555 - accuracy: 0.9391 - val_loss: 0.1506 - val_accuracy: 0.9394\n",
            "Epoch 20/500\n",
            "298/298 [==============================] - 1s 5ms/step - loss: 0.1511 - accuracy: 0.9414 - val_loss: 0.1477 - val_accuracy: 0.9389\n",
            "Epoch 21/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1489 - accuracy: 0.9397 - val_loss: 0.1459 - val_accuracy: 0.9394\n",
            "Epoch 22/500\n",
            "298/298 [==============================] - 1s 5ms/step - loss: 0.1444 - accuracy: 0.9435 - val_loss: 0.1453 - val_accuracy: 0.9439\n",
            "Epoch 23/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1449 - accuracy: 0.9405 - val_loss: 0.1450 - val_accuracy: 0.9403\n",
            "Epoch 24/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1457 - accuracy: 0.9388 - val_loss: 0.1433 - val_accuracy: 0.9439\n",
            "Epoch 25/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1441 - accuracy: 0.9421 - val_loss: 0.1439 - val_accuracy: 0.9412\n",
            "Epoch 26/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1427 - accuracy: 0.9417 - val_loss: 0.1405 - val_accuracy: 0.9430\n",
            "Epoch 27/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1424 - accuracy: 0.9431 - val_loss: 0.1404 - val_accuracy: 0.9412\n",
            "Epoch 28/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1423 - accuracy: 0.9438 - val_loss: 0.1390 - val_accuracy: 0.9430\n",
            "Epoch 29/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1438 - accuracy: 0.9416 - val_loss: 0.1392 - val_accuracy: 0.9435\n",
            "Epoch 30/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1410 - accuracy: 0.9447 - val_loss: 0.1422 - val_accuracy: 0.9421\n",
            "Epoch 31/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1395 - accuracy: 0.9446 - val_loss: 0.1400 - val_accuracy: 0.9466\n",
            "Epoch 32/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1371 - accuracy: 0.9414 - val_loss: 0.1381 - val_accuracy: 0.9430\n",
            "Epoch 33/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1363 - accuracy: 0.9421 - val_loss: 0.1402 - val_accuracy: 0.9426\n",
            "Epoch 34/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1320 - accuracy: 0.9469 - val_loss: 0.1433 - val_accuracy: 0.9430\n",
            "Epoch 35/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1342 - accuracy: 0.9458 - val_loss: 0.1377 - val_accuracy: 0.9457\n",
            "Epoch 36/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1363 - accuracy: 0.9440 - val_loss: 0.1388 - val_accuracy: 0.9421\n",
            "Epoch 37/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1362 - accuracy: 0.9447 - val_loss: 0.1370 - val_accuracy: 0.9457\n",
            "Epoch 38/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1322 - accuracy: 0.9476 - val_loss: 0.1407 - val_accuracy: 0.9430\n",
            "Epoch 39/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1341 - accuracy: 0.9460 - val_loss: 0.1389 - val_accuracy: 0.9439\n",
            "Epoch 40/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1345 - accuracy: 0.9447 - val_loss: 0.1379 - val_accuracy: 0.9444\n",
            "Epoch 41/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1303 - accuracy: 0.9468 - val_loss: 0.1374 - val_accuracy: 0.9430\n",
            "Epoch 42/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1307 - accuracy: 0.9462 - val_loss: 0.1384 - val_accuracy: 0.9430\n",
            "Epoch 43/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1344 - accuracy: 0.9447 - val_loss: 0.1374 - val_accuracy: 0.9462\n",
            "Epoch 44/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1321 - accuracy: 0.9446 - val_loss: 0.1392 - val_accuracy: 0.9435\n",
            "Epoch 45/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1269 - accuracy: 0.9473 - val_loss: 0.1399 - val_accuracy: 0.9462\n",
            "Epoch 46/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1289 - accuracy: 0.9474 - val_loss: 0.1392 - val_accuracy: 0.9466\n",
            "Epoch 47/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1284 - accuracy: 0.9485 - val_loss: 0.1392 - val_accuracy: 0.9471\n",
            "Epoch 48/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1289 - accuracy: 0.9448 - val_loss: 0.1420 - val_accuracy: 0.9448\n",
            "Epoch 49/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1305 - accuracy: 0.9468 - val_loss: 0.1368 - val_accuracy: 0.9453\n",
            "Epoch 50/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1251 - accuracy: 0.9483 - val_loss: 0.1378 - val_accuracy: 0.9462\n",
            "Epoch 51/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1286 - accuracy: 0.9474 - val_loss: 0.1377 - val_accuracy: 0.9457\n",
            "Epoch 52/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1257 - accuracy: 0.9479 - val_loss: 0.1392 - val_accuracy: 0.9426\n",
            "Epoch 53/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1278 - accuracy: 0.9471 - val_loss: 0.1405 - val_accuracy: 0.9444\n",
            "Epoch 54/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1283 - accuracy: 0.9470 - val_loss: 0.1421 - val_accuracy: 0.9457\n",
            "Epoch 55/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1260 - accuracy: 0.9462 - val_loss: 0.1371 - val_accuracy: 0.9480\n",
            "Epoch 56/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1307 - accuracy: 0.9453 - val_loss: 0.1332 - val_accuracy: 0.9475\n",
            "Epoch 57/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1227 - accuracy: 0.9496 - val_loss: 0.1429 - val_accuracy: 0.9489\n",
            "Epoch 58/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1243 - accuracy: 0.9473 - val_loss: 0.1431 - val_accuracy: 0.9466\n",
            "Epoch 59/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1241 - accuracy: 0.9493 - val_loss: 0.1395 - val_accuracy: 0.9457\n",
            "Epoch 60/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1235 - accuracy: 0.9490 - val_loss: 0.1386 - val_accuracy: 0.9453\n",
            "Epoch 61/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1221 - accuracy: 0.9492 - val_loss: 0.1354 - val_accuracy: 0.9448\n",
            "Epoch 62/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1225 - accuracy: 0.9484 - val_loss: 0.1383 - val_accuracy: 0.9480\n",
            "Epoch 63/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1259 - accuracy: 0.9490 - val_loss: 0.1369 - val_accuracy: 0.9457\n",
            "Epoch 64/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1214 - accuracy: 0.9495 - val_loss: 0.1397 - val_accuracy: 0.9448\n",
            "Epoch 65/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1220 - accuracy: 0.9473 - val_loss: 0.1369 - val_accuracy: 0.9466\n",
            "Epoch 66/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1273 - accuracy: 0.9461 - val_loss: 0.1363 - val_accuracy: 0.9466\n",
            "Epoch 67/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1239 - accuracy: 0.9468 - val_loss: 0.1389 - val_accuracy: 0.9453\n",
            "Epoch 68/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1244 - accuracy: 0.9476 - val_loss: 0.1399 - val_accuracy: 0.9435\n",
            "Epoch 69/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1241 - accuracy: 0.9475 - val_loss: 0.1376 - val_accuracy: 0.9466\n",
            "Epoch 70/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1225 - accuracy: 0.9478 - val_loss: 0.1436 - val_accuracy: 0.9439\n",
            "Epoch 71/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1249 - accuracy: 0.9472 - val_loss: 0.1391 - val_accuracy: 0.9430\n",
            "Epoch 72/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1191 - accuracy: 0.9494 - val_loss: 0.1408 - val_accuracy: 0.9439\n",
            "Epoch 73/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1264 - accuracy: 0.9465 - val_loss: 0.1385 - val_accuracy: 0.9457\n",
            "Epoch 74/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1254 - accuracy: 0.9476 - val_loss: 0.1429 - val_accuracy: 0.9457\n",
            "Epoch 75/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1229 - accuracy: 0.9475 - val_loss: 0.1440 - val_accuracy: 0.9439\n",
            "Epoch 76/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1222 - accuracy: 0.9488 - val_loss: 0.1416 - val_accuracy: 0.9439\n",
            "Epoch 77/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1197 - accuracy: 0.9519 - val_loss: 0.1369 - val_accuracy: 0.9453\n",
            "Epoch 78/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1225 - accuracy: 0.9476 - val_loss: 0.1396 - val_accuracy: 0.9462\n",
            "Epoch 79/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1189 - accuracy: 0.9507 - val_loss: 0.1407 - val_accuracy: 0.9466\n",
            "Epoch 80/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1236 - accuracy: 0.9483 - val_loss: 0.1403 - val_accuracy: 0.9466\n",
            "Epoch 81/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1216 - accuracy: 0.9493 - val_loss: 0.1373 - val_accuracy: 0.9475\n",
            "Epoch 82/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1219 - accuracy: 0.9486 - val_loss: 0.1450 - val_accuracy: 0.9453\n",
            "Epoch 83/500\n",
            "298/298 [==============================] - 1s 5ms/step - loss: 0.1200 - accuracy: 0.9519 - val_loss: 0.1398 - val_accuracy: 0.9489\n",
            "Epoch 84/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1193 - accuracy: 0.9495 - val_loss: 0.1436 - val_accuracy: 0.9435\n",
            "Epoch 85/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1211 - accuracy: 0.9504 - val_loss: 0.1402 - val_accuracy: 0.9457\n",
            "Epoch 86/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1240 - accuracy: 0.9495 - val_loss: 0.1399 - val_accuracy: 0.9471\n",
            "Epoch 87/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1190 - accuracy: 0.9495 - val_loss: 0.1418 - val_accuracy: 0.9493\n",
            "Epoch 88/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1201 - accuracy: 0.9509 - val_loss: 0.1439 - val_accuracy: 0.9475\n",
            "Epoch 89/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1197 - accuracy: 0.9496 - val_loss: 0.1452 - val_accuracy: 0.9475\n",
            "Epoch 90/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1176 - accuracy: 0.9480 - val_loss: 0.1415 - val_accuracy: 0.9489\n",
            "Epoch 91/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1222 - accuracy: 0.9492 - val_loss: 0.1435 - val_accuracy: 0.9475\n",
            "Epoch 92/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1197 - accuracy: 0.9466 - val_loss: 0.1423 - val_accuracy: 0.9466\n",
            "Epoch 93/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1239 - accuracy: 0.9492 - val_loss: 0.1391 - val_accuracy: 0.9484\n",
            "Epoch 94/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1251 - accuracy: 0.9459 - val_loss: 0.1439 - val_accuracy: 0.9462\n",
            "Epoch 95/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1203 - accuracy: 0.9478 - val_loss: 0.1388 - val_accuracy: 0.9462\n",
            "Epoch 96/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1194 - accuracy: 0.9491 - val_loss: 0.1406 - val_accuracy: 0.9462\n",
            "Epoch 97/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1263 - accuracy: 0.9501 - val_loss: 0.1411 - val_accuracy: 0.9466\n",
            "Epoch 98/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1238 - accuracy: 0.9477 - val_loss: 0.1390 - val_accuracy: 0.9484\n",
            "Epoch 99/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1218 - accuracy: 0.9484 - val_loss: 0.1376 - val_accuracy: 0.9453\n",
            "Epoch 100/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1187 - accuracy: 0.9493 - val_loss: 0.1450 - val_accuracy: 0.9448\n",
            "Epoch 101/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1192 - accuracy: 0.9496 - val_loss: 0.1432 - val_accuracy: 0.9457\n",
            "Epoch 102/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1227 - accuracy: 0.9500 - val_loss: 0.1400 - val_accuracy: 0.9453\n",
            "Epoch 103/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1196 - accuracy: 0.9505 - val_loss: 0.1414 - val_accuracy: 0.9444\n",
            "Epoch 104/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1201 - accuracy: 0.9502 - val_loss: 0.1382 - val_accuracy: 0.9435\n",
            "Epoch 105/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1196 - accuracy: 0.9483 - val_loss: 0.1393 - val_accuracy: 0.9475\n",
            "Epoch 106/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1216 - accuracy: 0.9502 - val_loss: 0.1380 - val_accuracy: 0.9462\n",
            "Epoch 107/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1201 - accuracy: 0.9496 - val_loss: 0.1395 - val_accuracy: 0.9480\n",
            "Epoch 108/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1149 - accuracy: 0.9532 - val_loss: 0.1420 - val_accuracy: 0.9453\n",
            "Epoch 109/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1226 - accuracy: 0.9486 - val_loss: 0.1437 - val_accuracy: 0.9435\n",
            "Epoch 110/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1192 - accuracy: 0.9503 - val_loss: 0.1462 - val_accuracy: 0.9430\n",
            "Epoch 111/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1201 - accuracy: 0.9525 - val_loss: 0.1473 - val_accuracy: 0.9421\n",
            "Epoch 112/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1174 - accuracy: 0.9513 - val_loss: 0.1457 - val_accuracy: 0.9457\n",
            "Epoch 113/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1156 - accuracy: 0.9525 - val_loss: 0.1450 - val_accuracy: 0.9466\n",
            "Epoch 114/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1193 - accuracy: 0.9503 - val_loss: 0.1430 - val_accuracy: 0.9457\n",
            "Epoch 115/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1174 - accuracy: 0.9516 - val_loss: 0.1419 - val_accuracy: 0.9448\n",
            "Epoch 116/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1173 - accuracy: 0.9518 - val_loss: 0.1451 - val_accuracy: 0.9457\n",
            "Epoch 117/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1220 - accuracy: 0.9490 - val_loss: 0.1446 - val_accuracy: 0.9421\n",
            "Epoch 118/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1211 - accuracy: 0.9489 - val_loss: 0.1468 - val_accuracy: 0.9448\n",
            "Epoch 119/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1194 - accuracy: 0.9483 - val_loss: 0.1433 - val_accuracy: 0.9484\n",
            "Epoch 120/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1153 - accuracy: 0.9495 - val_loss: 0.1468 - val_accuracy: 0.9466\n",
            "Epoch 121/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1137 - accuracy: 0.9506 - val_loss: 0.1420 - val_accuracy: 0.9475\n",
            "Epoch 122/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1161 - accuracy: 0.9498 - val_loss: 0.1411 - val_accuracy: 0.9448\n",
            "Epoch 123/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1217 - accuracy: 0.9489 - val_loss: 0.1456 - val_accuracy: 0.9430\n",
            "Epoch 124/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1232 - accuracy: 0.9487 - val_loss: 0.1456 - val_accuracy: 0.9457\n",
            "Epoch 125/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1204 - accuracy: 0.9487 - val_loss: 0.1420 - val_accuracy: 0.9448\n",
            "Epoch 126/500\n",
            "298/298 [==============================] - 1s 5ms/step - loss: 0.1146 - accuracy: 0.9497 - val_loss: 0.1483 - val_accuracy: 0.9439\n",
            "Epoch 127/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1181 - accuracy: 0.9501 - val_loss: 0.1428 - val_accuracy: 0.9466\n",
            "Epoch 128/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1153 - accuracy: 0.9516 - val_loss: 0.1448 - val_accuracy: 0.9480\n",
            "Epoch 129/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1175 - accuracy: 0.9504 - val_loss: 0.1476 - val_accuracy: 0.9435\n",
            "Epoch 130/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1186 - accuracy: 0.9502 - val_loss: 0.1459 - val_accuracy: 0.9516\n",
            "Epoch 131/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1120 - accuracy: 0.9524 - val_loss: 0.1487 - val_accuracy: 0.9484\n",
            "Epoch 132/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1169 - accuracy: 0.9513 - val_loss: 0.1435 - val_accuracy: 0.9471\n",
            "Epoch 133/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1123 - accuracy: 0.9517 - val_loss: 0.1494 - val_accuracy: 0.9471\n",
            "Epoch 134/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1127 - accuracy: 0.9522 - val_loss: 0.1499 - val_accuracy: 0.9471\n",
            "Epoch 135/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1157 - accuracy: 0.9522 - val_loss: 0.1434 - val_accuracy: 0.9466\n",
            "Epoch 136/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1238 - accuracy: 0.9482 - val_loss: 0.1436 - val_accuracy: 0.9444\n",
            "Epoch 137/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1156 - accuracy: 0.9496 - val_loss: 0.1502 - val_accuracy: 0.9462\n",
            "Epoch 138/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1214 - accuracy: 0.9487 - val_loss: 0.1428 - val_accuracy: 0.9453\n",
            "Epoch 139/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1160 - accuracy: 0.9514 - val_loss: 0.1447 - val_accuracy: 0.9430\n",
            "Epoch 140/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1151 - accuracy: 0.9498 - val_loss: 0.1518 - val_accuracy: 0.9439\n",
            "Epoch 141/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1216 - accuracy: 0.9502 - val_loss: 0.1458 - val_accuracy: 0.9448\n",
            "Epoch 142/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1175 - accuracy: 0.9509 - val_loss: 0.1477 - val_accuracy: 0.9444\n",
            "Epoch 143/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1182 - accuracy: 0.9473 - val_loss: 0.1486 - val_accuracy: 0.9475\n",
            "Epoch 144/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1236 - accuracy: 0.9479 - val_loss: 0.1480 - val_accuracy: 0.9448\n",
            "Epoch 145/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1155 - accuracy: 0.9531 - val_loss: 0.1509 - val_accuracy: 0.9453\n",
            "Epoch 146/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1165 - accuracy: 0.9521 - val_loss: 0.1478 - val_accuracy: 0.9462\n",
            "Epoch 147/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1141 - accuracy: 0.9516 - val_loss: 0.1440 - val_accuracy: 0.9466\n",
            "Epoch 148/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1176 - accuracy: 0.9508 - val_loss: 0.1487 - val_accuracy: 0.9435\n",
            "Epoch 149/500\n",
            "298/298 [==============================] - 1s 5ms/step - loss: 0.1172 - accuracy: 0.9523 - val_loss: 0.1456 - val_accuracy: 0.9462\n",
            "Epoch 150/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1162 - accuracy: 0.9517 - val_loss: 0.1446 - val_accuracy: 0.9489\n",
            "Epoch 151/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1148 - accuracy: 0.9508 - val_loss: 0.1396 - val_accuracy: 0.9512\n",
            "Epoch 152/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1142 - accuracy: 0.9527 - val_loss: 0.1435 - val_accuracy: 0.9471\n",
            "Epoch 153/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1183 - accuracy: 0.9483 - val_loss: 0.1518 - val_accuracy: 0.9453\n",
            "Epoch 154/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1162 - accuracy: 0.9494 - val_loss: 0.1472 - val_accuracy: 0.9466\n",
            "Epoch 155/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1189 - accuracy: 0.9502 - val_loss: 0.1468 - val_accuracy: 0.9430\n",
            "Epoch 156/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1152 - accuracy: 0.9504 - val_loss: 0.1453 - val_accuracy: 0.9466\n",
            "Epoch 157/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1150 - accuracy: 0.9516 - val_loss: 0.1471 - val_accuracy: 0.9480\n",
            "Epoch 158/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1182 - accuracy: 0.9518 - val_loss: 0.1416 - val_accuracy: 0.9435\n",
            "Epoch 159/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1169 - accuracy: 0.9499 - val_loss: 0.1446 - val_accuracy: 0.9435\n",
            "Epoch 160/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1140 - accuracy: 0.9503 - val_loss: 0.1446 - val_accuracy: 0.9475\n",
            "Epoch 161/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1192 - accuracy: 0.9510 - val_loss: 0.1422 - val_accuracy: 0.9489\n",
            "Epoch 162/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1153 - accuracy: 0.9514 - val_loss: 0.1432 - val_accuracy: 0.9457\n",
            "Epoch 163/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1154 - accuracy: 0.9512 - val_loss: 0.1434 - val_accuracy: 0.9453\n",
            "Epoch 164/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1090 - accuracy: 0.9529 - val_loss: 0.1455 - val_accuracy: 0.9475\n",
            "Epoch 165/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1188 - accuracy: 0.9502 - val_loss: 0.1459 - val_accuracy: 0.9466\n",
            "Epoch 166/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1139 - accuracy: 0.9502 - val_loss: 0.1448 - val_accuracy: 0.9484\n",
            "Epoch 167/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1182 - accuracy: 0.9504 - val_loss: 0.1392 - val_accuracy: 0.9493\n",
            "Epoch 168/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1153 - accuracy: 0.9530 - val_loss: 0.1504 - val_accuracy: 0.9480\n",
            "Epoch 169/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1179 - accuracy: 0.9532 - val_loss: 0.1413 - val_accuracy: 0.9462\n",
            "Epoch 170/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1139 - accuracy: 0.9530 - val_loss: 0.1443 - val_accuracy: 0.9466\n",
            "Epoch 171/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1141 - accuracy: 0.9497 - val_loss: 0.1439 - val_accuracy: 0.9475\n",
            "Epoch 172/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1167 - accuracy: 0.9507 - val_loss: 0.1443 - val_accuracy: 0.9475\n",
            "Epoch 173/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1156 - accuracy: 0.9506 - val_loss: 0.1470 - val_accuracy: 0.9471\n",
            "Epoch 174/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1165 - accuracy: 0.9516 - val_loss: 0.1447 - val_accuracy: 0.9466\n",
            "Epoch 175/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1148 - accuracy: 0.9517 - val_loss: 0.1470 - val_accuracy: 0.9444\n",
            "Epoch 176/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1179 - accuracy: 0.9500 - val_loss: 0.1482 - val_accuracy: 0.9462\n",
            "Epoch 177/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1163 - accuracy: 0.9500 - val_loss: 0.1452 - val_accuracy: 0.9462\n",
            "Epoch 178/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1161 - accuracy: 0.9493 - val_loss: 0.1439 - val_accuracy: 0.9471\n",
            "Epoch 179/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1136 - accuracy: 0.9511 - val_loss: 0.1481 - val_accuracy: 0.9484\n",
            "Epoch 180/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1146 - accuracy: 0.9501 - val_loss: 0.1497 - val_accuracy: 0.9462\n",
            "Epoch 181/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1166 - accuracy: 0.9501 - val_loss: 0.1434 - val_accuracy: 0.9439\n",
            "Epoch 182/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1185 - accuracy: 0.9503 - val_loss: 0.1406 - val_accuracy: 0.9462\n",
            "Epoch 183/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1177 - accuracy: 0.9521 - val_loss: 0.1443 - val_accuracy: 0.9471\n",
            "Epoch 184/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1165 - accuracy: 0.9495 - val_loss: 0.1495 - val_accuracy: 0.9448\n",
            "Epoch 185/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1120 - accuracy: 0.9540 - val_loss: 0.1447 - val_accuracy: 0.9471\n",
            "Epoch 186/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1183 - accuracy: 0.9508 - val_loss: 0.1447 - val_accuracy: 0.9448\n",
            "Epoch 187/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1175 - accuracy: 0.9476 - val_loss: 0.1429 - val_accuracy: 0.9466\n",
            "Epoch 188/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1163 - accuracy: 0.9508 - val_loss: 0.1451 - val_accuracy: 0.9471\n",
            "Epoch 189/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1172 - accuracy: 0.9506 - val_loss: 0.1456 - val_accuracy: 0.9466\n",
            "Epoch 190/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1155 - accuracy: 0.9509 - val_loss: 0.1460 - val_accuracy: 0.9489\n",
            "Epoch 191/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1170 - accuracy: 0.9497 - val_loss: 0.1512 - val_accuracy: 0.9457\n",
            "Epoch 192/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1183 - accuracy: 0.9506 - val_loss: 0.1417 - val_accuracy: 0.9475\n",
            "Epoch 193/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1194 - accuracy: 0.9493 - val_loss: 0.1439 - val_accuracy: 0.9466\n",
            "Epoch 194/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1131 - accuracy: 0.9519 - val_loss: 0.1437 - val_accuracy: 0.9471\n",
            "Epoch 195/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1144 - accuracy: 0.9523 - val_loss: 0.1496 - val_accuracy: 0.9471\n",
            "Epoch 196/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1134 - accuracy: 0.9519 - val_loss: 0.1419 - val_accuracy: 0.9466\n",
            "Epoch 197/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1146 - accuracy: 0.9494 - val_loss: 0.1478 - val_accuracy: 0.9462\n",
            "Epoch 198/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1172 - accuracy: 0.9510 - val_loss: 0.1408 - val_accuracy: 0.9475\n",
            "Epoch 199/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1171 - accuracy: 0.9502 - val_loss: 0.1402 - val_accuracy: 0.9489\n",
            "Epoch 200/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1188 - accuracy: 0.9493 - val_loss: 0.1438 - val_accuracy: 0.9466\n",
            "Epoch 201/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1181 - accuracy: 0.9526 - val_loss: 0.1457 - val_accuracy: 0.9439\n",
            "Epoch 202/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1104 - accuracy: 0.9549 - val_loss: 0.1466 - val_accuracy: 0.9480\n",
            "Epoch 203/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1105 - accuracy: 0.9518 - val_loss: 0.1419 - val_accuracy: 0.9471\n",
            "Epoch 204/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1137 - accuracy: 0.9537 - val_loss: 0.1487 - val_accuracy: 0.9430\n",
            "Epoch 205/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1162 - accuracy: 0.9509 - val_loss: 0.1482 - val_accuracy: 0.9457\n",
            "Epoch 206/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1142 - accuracy: 0.9522 - val_loss: 0.1489 - val_accuracy: 0.9475\n",
            "Epoch 207/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1163 - accuracy: 0.9510 - val_loss: 0.1495 - val_accuracy: 0.9457\n",
            "Epoch 208/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1138 - accuracy: 0.9521 - val_loss: 0.1453 - val_accuracy: 0.9471\n",
            "Epoch 209/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1104 - accuracy: 0.9547 - val_loss: 0.1481 - val_accuracy: 0.9471\n",
            "Epoch 210/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1176 - accuracy: 0.9490 - val_loss: 0.1485 - val_accuracy: 0.9471\n",
            "Epoch 211/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1155 - accuracy: 0.9521 - val_loss: 0.1465 - val_accuracy: 0.9484\n",
            "Epoch 212/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1112 - accuracy: 0.9526 - val_loss: 0.1507 - val_accuracy: 0.9453\n",
            "Epoch 213/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1160 - accuracy: 0.9530 - val_loss: 0.1542 - val_accuracy: 0.9444\n",
            "Epoch 214/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1125 - accuracy: 0.9520 - val_loss: 0.1477 - val_accuracy: 0.9475\n",
            "Epoch 215/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1111 - accuracy: 0.9518 - val_loss: 0.1528 - val_accuracy: 0.9462\n",
            "Epoch 216/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1081 - accuracy: 0.9535 - val_loss: 0.1486 - val_accuracy: 0.9484\n",
            "Epoch 217/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1151 - accuracy: 0.9530 - val_loss: 0.1543 - val_accuracy: 0.9480\n",
            "Epoch 218/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1145 - accuracy: 0.9518 - val_loss: 0.1469 - val_accuracy: 0.9462\n",
            "Epoch 219/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1181 - accuracy: 0.9499 - val_loss: 0.1544 - val_accuracy: 0.9471\n",
            "Epoch 220/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1132 - accuracy: 0.9521 - val_loss: 0.1432 - val_accuracy: 0.9475\n",
            "Epoch 221/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1147 - accuracy: 0.9536 - val_loss: 0.1433 - val_accuracy: 0.9480\n",
            "Epoch 222/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1147 - accuracy: 0.9518 - val_loss: 0.1410 - val_accuracy: 0.9493\n",
            "Epoch 223/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1136 - accuracy: 0.9530 - val_loss: 0.1467 - val_accuracy: 0.9444\n",
            "Epoch 224/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1145 - accuracy: 0.9523 - val_loss: 0.1454 - val_accuracy: 0.9480\n",
            "Epoch 225/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1132 - accuracy: 0.9529 - val_loss: 0.1476 - val_accuracy: 0.9480\n",
            "Epoch 226/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1150 - accuracy: 0.9514 - val_loss: 0.1595 - val_accuracy: 0.9444\n",
            "Epoch 227/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1121 - accuracy: 0.9519 - val_loss: 0.1397 - val_accuracy: 0.9480\n",
            "Epoch 228/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1179 - accuracy: 0.9497 - val_loss: 0.1453 - val_accuracy: 0.9475\n",
            "Epoch 229/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1138 - accuracy: 0.9535 - val_loss: 0.1455 - val_accuracy: 0.9462\n",
            "Epoch 230/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1186 - accuracy: 0.9504 - val_loss: 0.1509 - val_accuracy: 0.9462\n",
            "Epoch 231/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1107 - accuracy: 0.9521 - val_loss: 0.1522 - val_accuracy: 0.9435\n",
            "Epoch 232/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1126 - accuracy: 0.9505 - val_loss: 0.1465 - val_accuracy: 0.9439\n",
            "Epoch 233/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1150 - accuracy: 0.9532 - val_loss: 0.1480 - val_accuracy: 0.9462\n",
            "Epoch 234/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1127 - accuracy: 0.9516 - val_loss: 0.1505 - val_accuracy: 0.9439\n",
            "Epoch 235/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1121 - accuracy: 0.9530 - val_loss: 0.1531 - val_accuracy: 0.9457\n",
            "Epoch 236/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1125 - accuracy: 0.9527 - val_loss: 0.1467 - val_accuracy: 0.9480\n",
            "Epoch 237/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1131 - accuracy: 0.9529 - val_loss: 0.1461 - val_accuracy: 0.9502\n",
            "Epoch 238/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1138 - accuracy: 0.9543 - val_loss: 0.1430 - val_accuracy: 0.9457\n",
            "Epoch 239/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1146 - accuracy: 0.9520 - val_loss: 0.1472 - val_accuracy: 0.9484\n",
            "Epoch 240/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1136 - accuracy: 0.9516 - val_loss: 0.1526 - val_accuracy: 0.9493\n",
            "Epoch 241/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1136 - accuracy: 0.9518 - val_loss: 0.1441 - val_accuracy: 0.9489\n",
            "Epoch 242/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1124 - accuracy: 0.9538 - val_loss: 0.1479 - val_accuracy: 0.9507\n",
            "Epoch 243/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1150 - accuracy: 0.9500 - val_loss: 0.1490 - val_accuracy: 0.9480\n",
            "Epoch 244/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1157 - accuracy: 0.9505 - val_loss: 0.1482 - val_accuracy: 0.9507\n",
            "Epoch 245/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1151 - accuracy: 0.9489 - val_loss: 0.1565 - val_accuracy: 0.9475\n",
            "Epoch 246/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1180 - accuracy: 0.9498 - val_loss: 0.1490 - val_accuracy: 0.9507\n",
            "Epoch 247/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1111 - accuracy: 0.9542 - val_loss: 0.1472 - val_accuracy: 0.9502\n",
            "Epoch 248/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1131 - accuracy: 0.9508 - val_loss: 0.1652 - val_accuracy: 0.9453\n",
            "Epoch 249/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1148 - accuracy: 0.9523 - val_loss: 0.1584 - val_accuracy: 0.9480\n",
            "Epoch 250/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1117 - accuracy: 0.9529 - val_loss: 0.1519 - val_accuracy: 0.9484\n",
            "Epoch 251/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1146 - accuracy: 0.9507 - val_loss: 0.1520 - val_accuracy: 0.9484\n",
            "Epoch 252/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1103 - accuracy: 0.9533 - val_loss: 0.1489 - val_accuracy: 0.9489\n",
            "Epoch 253/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1116 - accuracy: 0.9513 - val_loss: 0.1506 - val_accuracy: 0.9489\n",
            "Epoch 254/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1123 - accuracy: 0.9508 - val_loss: 0.1480 - val_accuracy: 0.9471\n",
            "Epoch 255/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1145 - accuracy: 0.9518 - val_loss: 0.1506 - val_accuracy: 0.9502\n",
            "Epoch 256/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1143 - accuracy: 0.9523 - val_loss: 0.1502 - val_accuracy: 0.9489\n",
            "Epoch 257/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1139 - accuracy: 0.9503 - val_loss: 0.1448 - val_accuracy: 0.9480\n",
            "Epoch 258/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1151 - accuracy: 0.9502 - val_loss: 0.1561 - val_accuracy: 0.9457\n",
            "Epoch 259/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1131 - accuracy: 0.9518 - val_loss: 0.1500 - val_accuracy: 0.9484\n",
            "Epoch 260/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1126 - accuracy: 0.9519 - val_loss: 0.1506 - val_accuracy: 0.9462\n",
            "Epoch 261/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1127 - accuracy: 0.9520 - val_loss: 0.1484 - val_accuracy: 0.9475\n",
            "Epoch 262/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1125 - accuracy: 0.9533 - val_loss: 0.1485 - val_accuracy: 0.9480\n",
            "Epoch 263/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1189 - accuracy: 0.9508 - val_loss: 0.1437 - val_accuracy: 0.9475\n",
            "Epoch 264/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1093 - accuracy: 0.9526 - val_loss: 0.1542 - val_accuracy: 0.9462\n",
            "Epoch 265/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1160 - accuracy: 0.9510 - val_loss: 0.1476 - val_accuracy: 0.9489\n",
            "Epoch 266/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1151 - accuracy: 0.9524 - val_loss: 0.1619 - val_accuracy: 0.9466\n",
            "Epoch 267/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1122 - accuracy: 0.9536 - val_loss: 0.1543 - val_accuracy: 0.9502\n",
            "Epoch 268/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1148 - accuracy: 0.9518 - val_loss: 0.1460 - val_accuracy: 0.9466\n",
            "Epoch 269/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1123 - accuracy: 0.9534 - val_loss: 0.1482 - val_accuracy: 0.9489\n",
            "Epoch 270/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1110 - accuracy: 0.9508 - val_loss: 0.1501 - val_accuracy: 0.9471\n",
            "Epoch 271/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1142 - accuracy: 0.9526 - val_loss: 0.1449 - val_accuracy: 0.9457\n",
            "Epoch 272/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1105 - accuracy: 0.9532 - val_loss: 0.1509 - val_accuracy: 0.9480\n",
            "Epoch 273/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1125 - accuracy: 0.9523 - val_loss: 0.1496 - val_accuracy: 0.9466\n",
            "Epoch 274/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1125 - accuracy: 0.9537 - val_loss: 0.1480 - val_accuracy: 0.9480\n",
            "Epoch 275/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1106 - accuracy: 0.9527 - val_loss: 0.1475 - val_accuracy: 0.9507\n",
            "Epoch 276/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1107 - accuracy: 0.9522 - val_loss: 0.1418 - val_accuracy: 0.9498\n",
            "Epoch 277/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1096 - accuracy: 0.9538 - val_loss: 0.1492 - val_accuracy: 0.9462\n",
            "Epoch 278/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1150 - accuracy: 0.9532 - val_loss: 0.1469 - val_accuracy: 0.9484\n",
            "Epoch 279/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1121 - accuracy: 0.9535 - val_loss: 0.1483 - val_accuracy: 0.9457\n",
            "Epoch 280/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1096 - accuracy: 0.9535 - val_loss: 0.1594 - val_accuracy: 0.9435\n",
            "Epoch 281/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1143 - accuracy: 0.9520 - val_loss: 0.1524 - val_accuracy: 0.9448\n",
            "Epoch 282/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1135 - accuracy: 0.9541 - val_loss: 0.1587 - val_accuracy: 0.9457\n",
            "Epoch 283/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1092 - accuracy: 0.9531 - val_loss: 0.1489 - val_accuracy: 0.9480\n",
            "Epoch 284/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1077 - accuracy: 0.9532 - val_loss: 0.1607 - val_accuracy: 0.9448\n",
            "Epoch 285/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1127 - accuracy: 0.9524 - val_loss: 0.1505 - val_accuracy: 0.9462\n",
            "Epoch 286/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1150 - accuracy: 0.9508 - val_loss: 0.1499 - val_accuracy: 0.9471\n",
            "Epoch 287/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1148 - accuracy: 0.9525 - val_loss: 0.1596 - val_accuracy: 0.9430\n",
            "Epoch 288/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1067 - accuracy: 0.9555 - val_loss: 0.1479 - val_accuracy: 0.9475\n",
            "Epoch 289/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1131 - accuracy: 0.9507 - val_loss: 0.1533 - val_accuracy: 0.9480\n",
            "Epoch 290/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1112 - accuracy: 0.9540 - val_loss: 0.1492 - val_accuracy: 0.9516\n",
            "Epoch 291/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.1103 - accuracy: 0.9546 - val_loss: 0.1469 - val_accuracy: 0.9475\n",
            "Epoch 292/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1147 - accuracy: 0.9527 - val_loss: 0.1407 - val_accuracy: 0.9457\n",
            "Epoch 293/500\n",
            "298/298 [==============================] - 1s 5ms/step - loss: 0.1102 - accuracy: 0.9534 - val_loss: 0.1434 - val_accuracy: 0.9471\n",
            "Epoch 294/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1155 - accuracy: 0.9506 - val_loss: 0.1461 - val_accuracy: 0.9471\n",
            "Epoch 295/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1154 - accuracy: 0.9487 - val_loss: 0.1514 - val_accuracy: 0.9493\n",
            "Epoch 296/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1101 - accuracy: 0.9539 - val_loss: 0.1559 - val_accuracy: 0.9462\n",
            "Epoch 297/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1143 - accuracy: 0.9528 - val_loss: 0.1495 - val_accuracy: 0.9475\n",
            "Epoch 298/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1155 - accuracy: 0.9512 - val_loss: 0.1490 - val_accuracy: 0.9462\n",
            "Epoch 299/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.1131 - accuracy: 0.9513 - val_loss: 0.1527 - val_accuracy: 0.9457\n",
            "Epoch 300/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1097 - accuracy: 0.9536 - val_loss: 0.1498 - val_accuracy: 0.9471\n",
            "Epoch 301/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1105 - accuracy: 0.9531 - val_loss: 0.1473 - val_accuracy: 0.9457\n",
            "Epoch 302/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1150 - accuracy: 0.9497 - val_loss: 0.1490 - val_accuracy: 0.9493\n",
            "Epoch 303/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1125 - accuracy: 0.9514 - val_loss: 0.1525 - val_accuracy: 0.9484\n",
            "Epoch 304/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1115 - accuracy: 0.9526 - val_loss: 0.1569 - val_accuracy: 0.9430\n",
            "Epoch 305/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1088 - accuracy: 0.9520 - val_loss: 0.1516 - val_accuracy: 0.9471\n",
            "Epoch 306/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1116 - accuracy: 0.9537 - val_loss: 0.1551 - val_accuracy: 0.9475\n",
            "Epoch 307/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1109 - accuracy: 0.9518 - val_loss: 0.1548 - val_accuracy: 0.9471\n",
            "Epoch 308/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1125 - accuracy: 0.9519 - val_loss: 0.1555 - val_accuracy: 0.9475\n",
            "Epoch 309/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1084 - accuracy: 0.9549 - val_loss: 0.1490 - val_accuracy: 0.9480\n",
            "Epoch 310/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1136 - accuracy: 0.9506 - val_loss: 0.1514 - val_accuracy: 0.9466\n",
            "Epoch 311/500\n",
            "298/298 [==============================] - 1s 5ms/step - loss: 0.1128 - accuracy: 0.9516 - val_loss: 0.1502 - val_accuracy: 0.9466\n",
            "Epoch 312/500\n",
            "298/298 [==============================] - 1s 5ms/step - loss: 0.1141 - accuracy: 0.9507 - val_loss: 0.1546 - val_accuracy: 0.9462\n",
            "Epoch 313/500\n",
            "298/298 [==============================] - 1s 5ms/step - loss: 0.1127 - accuracy: 0.9534 - val_loss: 0.1463 - val_accuracy: 0.9480\n",
            "Epoch 314/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1078 - accuracy: 0.9524 - val_loss: 0.1569 - val_accuracy: 0.9484\n",
            "Epoch 315/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1114 - accuracy: 0.9538 - val_loss: 0.1461 - val_accuracy: 0.9475\n",
            "Epoch 316/500\n",
            "298/298 [==============================] - 1s 5ms/step - loss: 0.1089 - accuracy: 0.9545 - val_loss: 0.1560 - val_accuracy: 0.9475\n",
            "Epoch 317/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1133 - accuracy: 0.9490 - val_loss: 0.1523 - val_accuracy: 0.9493\n",
            "Epoch 318/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1138 - accuracy: 0.9510 - val_loss: 0.1561 - val_accuracy: 0.9498\n",
            "Epoch 319/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1080 - accuracy: 0.9545 - val_loss: 0.1468 - val_accuracy: 0.9471\n",
            "Epoch 320/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1121 - accuracy: 0.9525 - val_loss: 0.1490 - val_accuracy: 0.9489\n",
            "Epoch 321/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1080 - accuracy: 0.9517 - val_loss: 0.1502 - val_accuracy: 0.9480\n",
            "Epoch 322/500\n",
            "298/298 [==============================] - 1s 5ms/step - loss: 0.1119 - accuracy: 0.9523 - val_loss: 0.1550 - val_accuracy: 0.9484\n",
            "Epoch 323/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1123 - accuracy: 0.9535 - val_loss: 0.1581 - val_accuracy: 0.9475\n",
            "Epoch 324/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1101 - accuracy: 0.9519 - val_loss: 0.1550 - val_accuracy: 0.9489\n",
            "Epoch 325/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1135 - accuracy: 0.9497 - val_loss: 0.1619 - val_accuracy: 0.9484\n",
            "Epoch 326/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1098 - accuracy: 0.9546 - val_loss: 0.1502 - val_accuracy: 0.9471\n",
            "Epoch 327/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1101 - accuracy: 0.9520 - val_loss: 0.1575 - val_accuracy: 0.9466\n",
            "Epoch 328/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1131 - accuracy: 0.9521 - val_loss: 0.1449 - val_accuracy: 0.9484\n",
            "Epoch 329/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1090 - accuracy: 0.9549 - val_loss: 0.1520 - val_accuracy: 0.9475\n",
            "Epoch 330/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1105 - accuracy: 0.9508 - val_loss: 0.1506 - val_accuracy: 0.9480\n",
            "Epoch 331/500\n",
            "298/298 [==============================] - 1s 5ms/step - loss: 0.1125 - accuracy: 0.9549 - val_loss: 0.1533 - val_accuracy: 0.9466\n",
            "Epoch 332/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1099 - accuracy: 0.9529 - val_loss: 0.1518 - val_accuracy: 0.9489\n",
            "Epoch 333/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1135 - accuracy: 0.9527 - val_loss: 0.1580 - val_accuracy: 0.9475\n",
            "Epoch 334/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1106 - accuracy: 0.9510 - val_loss: 0.1557 - val_accuracy: 0.9480\n",
            "Epoch 335/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1132 - accuracy: 0.9528 - val_loss: 0.1628 - val_accuracy: 0.9471\n",
            "Epoch 336/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1125 - accuracy: 0.9539 - val_loss: 0.1518 - val_accuracy: 0.9489\n",
            "Epoch 337/500\n",
            "298/298 [==============================] - 1s 5ms/step - loss: 0.1112 - accuracy: 0.9534 - val_loss: 0.1562 - val_accuracy: 0.9489\n",
            "Epoch 338/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1092 - accuracy: 0.9544 - val_loss: 0.1630 - val_accuracy: 0.9489\n",
            "Epoch 339/500\n",
            "298/298 [==============================] - 1s 5ms/step - loss: 0.1133 - accuracy: 0.9514 - val_loss: 0.1601 - val_accuracy: 0.9466\n",
            "Epoch 340/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1129 - accuracy: 0.9521 - val_loss: 0.1658 - val_accuracy: 0.9471\n",
            "Epoch 341/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1154 - accuracy: 0.9527 - val_loss: 0.1604 - val_accuracy: 0.9493\n",
            "Epoch 342/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1136 - accuracy: 0.9501 - val_loss: 0.1506 - val_accuracy: 0.9502\n",
            "Epoch 343/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1073 - accuracy: 0.9559 - val_loss: 0.1540 - val_accuracy: 0.9484\n",
            "Epoch 344/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1125 - accuracy: 0.9501 - val_loss: 0.1558 - val_accuracy: 0.9502\n",
            "Epoch 345/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1098 - accuracy: 0.9541 - val_loss: 0.1643 - val_accuracy: 0.9462\n",
            "Epoch 346/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1086 - accuracy: 0.9516 - val_loss: 0.1541 - val_accuracy: 0.9484\n",
            "Epoch 347/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1111 - accuracy: 0.9518 - val_loss: 0.1601 - val_accuracy: 0.9498\n",
            "Epoch 348/500\n",
            "298/298 [==============================] - 1s 5ms/step - loss: 0.1098 - accuracy: 0.9519 - val_loss: 0.1529 - val_accuracy: 0.9489\n",
            "Epoch 349/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1104 - accuracy: 0.9537 - val_loss: 0.1587 - val_accuracy: 0.9471\n",
            "Epoch 350/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1101 - accuracy: 0.9540 - val_loss: 0.1564 - val_accuracy: 0.9471\n",
            "Epoch 351/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1070 - accuracy: 0.9541 - val_loss: 0.1615 - val_accuracy: 0.9462\n",
            "Epoch 352/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1102 - accuracy: 0.9539 - val_loss: 0.1674 - val_accuracy: 0.9484\n",
            "Epoch 353/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1122 - accuracy: 0.9525 - val_loss: 0.1498 - val_accuracy: 0.9489\n",
            "Epoch 354/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1120 - accuracy: 0.9539 - val_loss: 0.1534 - val_accuracy: 0.9471\n",
            "Epoch 355/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1139 - accuracy: 0.9533 - val_loss: 0.1525 - val_accuracy: 0.9484\n",
            "Epoch 356/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1117 - accuracy: 0.9529 - val_loss: 0.1539 - val_accuracy: 0.9484\n",
            "Epoch 357/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1114 - accuracy: 0.9528 - val_loss: 0.1594 - val_accuracy: 0.9466\n",
            "Epoch 358/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1122 - accuracy: 0.9548 - val_loss: 0.1515 - val_accuracy: 0.9484\n",
            "Epoch 359/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1117 - accuracy: 0.9524 - val_loss: 0.1577 - val_accuracy: 0.9480\n",
            "Epoch 360/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1093 - accuracy: 0.9527 - val_loss: 0.1605 - val_accuracy: 0.9489\n",
            "Epoch 361/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1113 - accuracy: 0.9528 - val_loss: 0.1601 - val_accuracy: 0.9475\n",
            "Epoch 362/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1151 - accuracy: 0.9509 - val_loss: 0.1537 - val_accuracy: 0.9489\n",
            "Epoch 363/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1115 - accuracy: 0.9524 - val_loss: 0.1615 - val_accuracy: 0.9480\n",
            "Epoch 364/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1106 - accuracy: 0.9533 - val_loss: 0.1605 - val_accuracy: 0.9489\n",
            "Epoch 365/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1094 - accuracy: 0.9531 - val_loss: 0.1533 - val_accuracy: 0.9512\n",
            "Epoch 366/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1080 - accuracy: 0.9528 - val_loss: 0.1574 - val_accuracy: 0.9489\n",
            "Epoch 367/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1109 - accuracy: 0.9518 - val_loss: 0.1602 - val_accuracy: 0.9493\n",
            "Epoch 368/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1120 - accuracy: 0.9512 - val_loss: 0.1628 - val_accuracy: 0.9471\n",
            "Epoch 369/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1073 - accuracy: 0.9540 - val_loss: 0.1519 - val_accuracy: 0.9484\n",
            "Epoch 370/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1093 - accuracy: 0.9524 - val_loss: 0.1536 - val_accuracy: 0.9475\n",
            "Epoch 371/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1134 - accuracy: 0.9527 - val_loss: 0.1573 - val_accuracy: 0.9502\n",
            "Epoch 372/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1079 - accuracy: 0.9560 - val_loss: 0.1575 - val_accuracy: 0.9475\n",
            "Epoch 373/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1104 - accuracy: 0.9545 - val_loss: 0.1573 - val_accuracy: 0.9480\n",
            "Epoch 374/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1102 - accuracy: 0.9531 - val_loss: 0.1544 - val_accuracy: 0.9462\n",
            "Epoch 375/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1086 - accuracy: 0.9526 - val_loss: 0.1628 - val_accuracy: 0.9489\n",
            "Epoch 376/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1127 - accuracy: 0.9525 - val_loss: 0.1589 - val_accuracy: 0.9480\n",
            "Epoch 377/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1085 - accuracy: 0.9538 - val_loss: 0.1601 - val_accuracy: 0.9484\n",
            "Epoch 378/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1125 - accuracy: 0.9509 - val_loss: 0.1563 - val_accuracy: 0.9502\n",
            "Epoch 379/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1118 - accuracy: 0.9532 - val_loss: 0.1473 - val_accuracy: 0.9498\n",
            "Epoch 380/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1063 - accuracy: 0.9542 - val_loss: 0.1511 - val_accuracy: 0.9489\n",
            "Epoch 381/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1081 - accuracy: 0.9533 - val_loss: 0.1564 - val_accuracy: 0.9493\n",
            "Epoch 382/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1095 - accuracy: 0.9545 - val_loss: 0.1499 - val_accuracy: 0.9484\n",
            "Epoch 383/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1125 - accuracy: 0.9526 - val_loss: 0.1464 - val_accuracy: 0.9471\n",
            "Epoch 384/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1135 - accuracy: 0.9526 - val_loss: 0.1573 - val_accuracy: 0.9489\n",
            "Epoch 385/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1106 - accuracy: 0.9512 - val_loss: 0.1598 - val_accuracy: 0.9475\n",
            "Epoch 386/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1102 - accuracy: 0.9537 - val_loss: 0.1564 - val_accuracy: 0.9498\n",
            "Epoch 387/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1108 - accuracy: 0.9539 - val_loss: 0.1578 - val_accuracy: 0.9493\n",
            "Epoch 388/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1119 - accuracy: 0.9519 - val_loss: 0.1539 - val_accuracy: 0.9493\n",
            "Epoch 389/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1078 - accuracy: 0.9544 - val_loss: 0.1690 - val_accuracy: 0.9489\n",
            "Epoch 390/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1087 - accuracy: 0.9534 - val_loss: 0.1598 - val_accuracy: 0.9498\n",
            "Epoch 391/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1112 - accuracy: 0.9511 - val_loss: 0.1535 - val_accuracy: 0.9471\n",
            "Epoch 392/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1123 - accuracy: 0.9521 - val_loss: 0.1603 - val_accuracy: 0.9484\n",
            "Epoch 393/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1100 - accuracy: 0.9541 - val_loss: 0.1579 - val_accuracy: 0.9462\n",
            "Epoch 394/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1115 - accuracy: 0.9530 - val_loss: 0.1569 - val_accuracy: 0.9471\n",
            "Epoch 395/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1113 - accuracy: 0.9513 - val_loss: 0.1644 - val_accuracy: 0.9475\n",
            "Epoch 396/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1117 - accuracy: 0.9538 - val_loss: 0.1680 - val_accuracy: 0.9489\n",
            "Epoch 397/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1089 - accuracy: 0.9532 - val_loss: 0.1625 - val_accuracy: 0.9489\n",
            "Epoch 398/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1111 - accuracy: 0.9516 - val_loss: 0.1642 - val_accuracy: 0.9484\n",
            "Epoch 399/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1117 - accuracy: 0.9524 - val_loss: 0.1669 - val_accuracy: 0.9484\n",
            "Epoch 400/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1090 - accuracy: 0.9543 - val_loss: 0.1662 - val_accuracy: 0.9502\n",
            "Epoch 401/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1095 - accuracy: 0.9534 - val_loss: 0.1689 - val_accuracy: 0.9493\n",
            "Epoch 402/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1075 - accuracy: 0.9536 - val_loss: 0.1614 - val_accuracy: 0.9480\n",
            "Epoch 403/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1123 - accuracy: 0.9527 - val_loss: 0.1620 - val_accuracy: 0.9475\n",
            "Epoch 404/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1107 - accuracy: 0.9546 - val_loss: 0.1690 - val_accuracy: 0.9507\n",
            "Epoch 405/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1105 - accuracy: 0.9547 - val_loss: 0.1597 - val_accuracy: 0.9466\n",
            "Epoch 406/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1072 - accuracy: 0.9559 - val_loss: 0.1674 - val_accuracy: 0.9457\n",
            "Epoch 407/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1107 - accuracy: 0.9528 - val_loss: 0.1595 - val_accuracy: 0.9475\n",
            "Epoch 408/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1068 - accuracy: 0.9523 - val_loss: 0.1746 - val_accuracy: 0.9471\n",
            "Epoch 409/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1086 - accuracy: 0.9533 - val_loss: 0.1767 - val_accuracy: 0.9448\n",
            "Epoch 410/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1105 - accuracy: 0.9547 - val_loss: 0.1589 - val_accuracy: 0.9462\n",
            "Epoch 411/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1143 - accuracy: 0.9518 - val_loss: 0.1622 - val_accuracy: 0.9480\n",
            "Epoch 412/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1089 - accuracy: 0.9532 - val_loss: 0.1622 - val_accuracy: 0.9471\n",
            "Epoch 413/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1146 - accuracy: 0.9507 - val_loss: 0.1595 - val_accuracy: 0.9466\n",
            "Epoch 414/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1094 - accuracy: 0.9541 - val_loss: 0.1564 - val_accuracy: 0.9489\n",
            "Epoch 415/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1099 - accuracy: 0.9544 - val_loss: 0.1615 - val_accuracy: 0.9484\n",
            "Epoch 416/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1078 - accuracy: 0.9529 - val_loss: 0.1647 - val_accuracy: 0.9471\n",
            "Epoch 417/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1096 - accuracy: 0.9522 - val_loss: 0.1616 - val_accuracy: 0.9507\n",
            "Epoch 418/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1105 - accuracy: 0.9514 - val_loss: 0.1657 - val_accuracy: 0.9489\n",
            "Epoch 419/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1112 - accuracy: 0.9536 - val_loss: 0.1566 - val_accuracy: 0.9493\n",
            "Epoch 420/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1143 - accuracy: 0.9516 - val_loss: 0.1639 - val_accuracy: 0.9493\n",
            "Epoch 421/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1115 - accuracy: 0.9531 - val_loss: 0.1581 - val_accuracy: 0.9475\n",
            "Epoch 422/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1119 - accuracy: 0.9518 - val_loss: 0.1634 - val_accuracy: 0.9502\n",
            "Epoch 423/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1098 - accuracy: 0.9535 - val_loss: 0.1529 - val_accuracy: 0.9498\n",
            "Epoch 424/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1110 - accuracy: 0.9509 - val_loss: 0.1639 - val_accuracy: 0.9489\n",
            "Epoch 425/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1077 - accuracy: 0.9542 - val_loss: 0.1687 - val_accuracy: 0.9439\n",
            "Epoch 426/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1053 - accuracy: 0.9539 - val_loss: 0.1651 - val_accuracy: 0.9512\n",
            "Epoch 427/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1129 - accuracy: 0.9527 - val_loss: 0.1576 - val_accuracy: 0.9462\n",
            "Epoch 428/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1088 - accuracy: 0.9547 - val_loss: 0.1592 - val_accuracy: 0.9475\n",
            "Epoch 429/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1105 - accuracy: 0.9534 - val_loss: 0.1617 - val_accuracy: 0.9457\n",
            "Epoch 430/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1090 - accuracy: 0.9535 - val_loss: 0.1613 - val_accuracy: 0.9484\n",
            "Epoch 431/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1078 - accuracy: 0.9541 - val_loss: 0.1517 - val_accuracy: 0.9484\n",
            "Epoch 432/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1130 - accuracy: 0.9505 - val_loss: 0.1600 - val_accuracy: 0.9480\n",
            "Epoch 433/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1069 - accuracy: 0.9550 - val_loss: 0.1573 - val_accuracy: 0.9502\n",
            "Epoch 434/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1098 - accuracy: 0.9554 - val_loss: 0.1534 - val_accuracy: 0.9507\n",
            "Epoch 435/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1081 - accuracy: 0.9543 - val_loss: 0.1672 - val_accuracy: 0.9493\n",
            "Epoch 436/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1096 - accuracy: 0.9517 - val_loss: 0.1607 - val_accuracy: 0.9475\n",
            "Epoch 437/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1078 - accuracy: 0.9541 - val_loss: 0.1644 - val_accuracy: 0.9462\n",
            "Epoch 438/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1078 - accuracy: 0.9544 - val_loss: 0.1680 - val_accuracy: 0.9489\n",
            "Epoch 439/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1071 - accuracy: 0.9546 - val_loss: 0.1640 - val_accuracy: 0.9493\n",
            "Epoch 440/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1092 - accuracy: 0.9508 - val_loss: 0.1641 - val_accuracy: 0.9498\n",
            "Epoch 441/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1045 - accuracy: 0.9554 - val_loss: 0.1589 - val_accuracy: 0.9502\n",
            "Epoch 442/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1111 - accuracy: 0.9539 - val_loss: 0.1498 - val_accuracy: 0.9457\n",
            "Epoch 443/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1089 - accuracy: 0.9528 - val_loss: 0.1530 - val_accuracy: 0.9493\n",
            "Epoch 444/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1136 - accuracy: 0.9526 - val_loss: 0.1523 - val_accuracy: 0.9493\n",
            "Epoch 445/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1069 - accuracy: 0.9545 - val_loss: 0.1539 - val_accuracy: 0.9462\n",
            "Epoch 446/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1097 - accuracy: 0.9546 - val_loss: 0.1607 - val_accuracy: 0.9489\n",
            "Epoch 447/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1108 - accuracy: 0.9542 - val_loss: 0.1617 - val_accuracy: 0.9480\n",
            "Epoch 448/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1058 - accuracy: 0.9564 - val_loss: 0.1605 - val_accuracy: 0.9475\n",
            "Epoch 449/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1114 - accuracy: 0.9539 - val_loss: 0.1593 - val_accuracy: 0.9475\n",
            "Epoch 450/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1082 - accuracy: 0.9529 - val_loss: 0.1447 - val_accuracy: 0.9502\n",
            "Epoch 451/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1085 - accuracy: 0.9528 - val_loss: 0.1576 - val_accuracy: 0.9493\n",
            "Epoch 452/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1087 - accuracy: 0.9513 - val_loss: 0.1561 - val_accuracy: 0.9493\n",
            "Epoch 453/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1133 - accuracy: 0.9536 - val_loss: 0.1585 - val_accuracy: 0.9480\n",
            "Epoch 454/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1091 - accuracy: 0.9520 - val_loss: 0.1530 - val_accuracy: 0.9480\n",
            "Epoch 455/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1072 - accuracy: 0.9571 - val_loss: 0.1600 - val_accuracy: 0.9493\n",
            "Epoch 456/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1090 - accuracy: 0.9516 - val_loss: 0.1624 - val_accuracy: 0.9507\n",
            "Epoch 457/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1080 - accuracy: 0.9534 - val_loss: 0.1626 - val_accuracy: 0.9484\n",
            "Epoch 458/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1081 - accuracy: 0.9536 - val_loss: 0.1609 - val_accuracy: 0.9480\n",
            "Epoch 459/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1116 - accuracy: 0.9519 - val_loss: 0.1564 - val_accuracy: 0.9493\n",
            "Epoch 460/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1132 - accuracy: 0.9531 - val_loss: 0.1573 - val_accuracy: 0.9493\n",
            "Epoch 461/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1133 - accuracy: 0.9513 - val_loss: 0.1603 - val_accuracy: 0.9493\n",
            "Epoch 462/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1056 - accuracy: 0.9547 - val_loss: 0.1591 - val_accuracy: 0.9471\n",
            "Epoch 463/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1096 - accuracy: 0.9548 - val_loss: 0.1654 - val_accuracy: 0.9475\n",
            "Epoch 464/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1107 - accuracy: 0.9517 - val_loss: 0.1574 - val_accuracy: 0.9453\n",
            "Epoch 465/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1081 - accuracy: 0.9532 - val_loss: 0.1581 - val_accuracy: 0.9489\n",
            "Epoch 466/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1057 - accuracy: 0.9555 - val_loss: 0.1647 - val_accuracy: 0.9471\n",
            "Epoch 467/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1112 - accuracy: 0.9542 - val_loss: 0.1583 - val_accuracy: 0.9502\n",
            "Epoch 468/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1086 - accuracy: 0.9526 - val_loss: 0.1627 - val_accuracy: 0.9512\n",
            "Epoch 469/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1134 - accuracy: 0.9514 - val_loss: 0.1684 - val_accuracy: 0.9489\n",
            "Epoch 470/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1093 - accuracy: 0.9531 - val_loss: 0.1634 - val_accuracy: 0.9484\n",
            "Epoch 471/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1105 - accuracy: 0.9514 - val_loss: 0.1663 - val_accuracy: 0.9475\n",
            "Epoch 472/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1112 - accuracy: 0.9517 - val_loss: 0.1601 - val_accuracy: 0.9480\n",
            "Epoch 473/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1068 - accuracy: 0.9552 - val_loss: 0.1573 - val_accuracy: 0.9512\n",
            "Epoch 474/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1095 - accuracy: 0.9531 - val_loss: 0.1504 - val_accuracy: 0.9498\n",
            "Epoch 475/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1119 - accuracy: 0.9537 - val_loss: 0.1479 - val_accuracy: 0.9502\n",
            "Epoch 476/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1104 - accuracy: 0.9507 - val_loss: 0.1523 - val_accuracy: 0.9530\n",
            "Epoch 477/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1069 - accuracy: 0.9520 - val_loss: 0.1530 - val_accuracy: 0.9484\n",
            "Epoch 478/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1088 - accuracy: 0.9544 - val_loss: 0.1638 - val_accuracy: 0.9502\n",
            "Epoch 479/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1109 - accuracy: 0.9549 - val_loss: 0.1557 - val_accuracy: 0.9530\n",
            "Epoch 480/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1100 - accuracy: 0.9531 - val_loss: 0.1610 - val_accuracy: 0.9507\n",
            "Epoch 481/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1069 - accuracy: 0.9534 - val_loss: 0.1550 - val_accuracy: 0.9493\n",
            "Epoch 482/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1060 - accuracy: 0.9533 - val_loss: 0.1568 - val_accuracy: 0.9512\n",
            "Epoch 483/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1094 - accuracy: 0.9516 - val_loss: 0.1604 - val_accuracy: 0.9502\n",
            "Epoch 484/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1097 - accuracy: 0.9531 - val_loss: 0.1540 - val_accuracy: 0.9512\n",
            "Epoch 485/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1066 - accuracy: 0.9551 - val_loss: 0.1568 - val_accuracy: 0.9475\n",
            "Epoch 486/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1118 - accuracy: 0.9534 - val_loss: 0.1543 - val_accuracy: 0.9489\n",
            "Epoch 487/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1097 - accuracy: 0.9531 - val_loss: 0.1659 - val_accuracy: 0.9489\n",
            "Epoch 488/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1096 - accuracy: 0.9544 - val_loss: 0.1617 - val_accuracy: 0.9475\n",
            "Epoch 489/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1088 - accuracy: 0.9544 - val_loss: 0.1581 - val_accuracy: 0.9502\n",
            "Epoch 490/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1084 - accuracy: 0.9527 - val_loss: 0.1606 - val_accuracy: 0.9493\n",
            "Epoch 491/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1082 - accuracy: 0.9549 - val_loss: 0.1593 - val_accuracy: 0.9498\n",
            "Epoch 492/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1094 - accuracy: 0.9523 - val_loss: 0.1672 - val_accuracy: 0.9493\n",
            "Epoch 493/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1106 - accuracy: 0.9526 - val_loss: 0.1587 - val_accuracy: 0.9480\n",
            "Epoch 494/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1105 - accuracy: 0.9546 - val_loss: 0.1567 - val_accuracy: 0.9457\n",
            "Epoch 495/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1091 - accuracy: 0.9524 - val_loss: 0.1562 - val_accuracy: 0.9512\n",
            "Epoch 496/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1104 - accuracy: 0.9537 - val_loss: 0.1554 - val_accuracy: 0.9493\n",
            "Epoch 497/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1096 - accuracy: 0.9535 - val_loss: 0.1616 - val_accuracy: 0.9453\n",
            "Epoch 498/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1083 - accuracy: 0.9556 - val_loss: 0.1564 - val_accuracy: 0.9489\n",
            "Epoch 499/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1061 - accuracy: 0.9558 - val_loss: 0.1572 - val_accuracy: 0.9493\n",
            "Epoch 500/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.1092 - accuracy: 0.9537 - val_loss: 0.1574 - val_accuracy: 0.9516\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_6 (Dense)             (None, 15)                465       \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 15)                0         \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 15)                240       \n",
            "                                                                 \n",
            " dropout_5 (Dropout)         (None, 15)                0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 1)                 16        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 721\n",
            "Trainable params: 721\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Define the model architecture\n",
        "model_ann = Sequential()\n",
        "model_ann.add(Dense(15, input_dim=train_data_2d.shape[1], activation='relu'))\n",
        "model_ann.add(Dropout(0.2))\n",
        "model_ann.add(Dense(15, activation='relu'))\n",
        "model_ann.add(Dropout(0.4))\n",
        "model_ann.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "model_ann.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model_ann.fit(train_data_2d, train_labels, validation_data=(test_data_2d, test_labels), epochs=500, batch_size=33, verbose=1)\n",
        "model_ann.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pG5_VkydwGkw"
      },
      "source": [
        "### 2. MLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "xppHxCZbwGkw",
        "outputId": "765f4244-e6c4-49c4-b8d1-0a7bf20c306e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-14 {color: black;background-color: white;}#sk-container-id-14 pre{padding: 0;}#sk-container-id-14 div.sk-toggleable {background-color: white;}#sk-container-id-14 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-14 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-14 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-14 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-14 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-14 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-14 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-14 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-14 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-14 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-14 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-14 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-14 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-14 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-14 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-14 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-14 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-14 div.sk-item {position: relative;z-index: 1;}#sk-container-id-14 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-14 div.sk-item::before, #sk-container-id-14 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-14 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-14 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-14 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-14 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-14 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-14 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-14 div.sk-label-container {text-align: center;}#sk-container-id-14 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-14 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-14\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(hidden_layer_sizes=(50, 50), max_iter=1000, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-30\" type=\"checkbox\" checked><label for=\"sk-estimator-id-30\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(hidden_layer_sizes=(50, 50), max_iter=1000, random_state=42)</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "MLPClassifier(hidden_layer_sizes=(50, 50), max_iter=1000, random_state=42)"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create an MLP classifier with two hidden layers of 50 neurons each\n",
        "model_mlp = MLPClassifier(hidden_layer_sizes=(50, 50), max_iter=1000, random_state=42)\n",
        "\n",
        "# Train the MLP classifier on the training set\n",
        "model_mlp.fit(train_data_2d, train_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "IhKhS0aHwGkw"
      },
      "outputs": [],
      "source": [
        "# Reshape the features to 3D tensor shape (batch_size, time_steps, num_features)\n",
        "train_data_3d = train_data_2d.reshape(-1, 1, train_data_2d.shape[1])\n",
        "test_data_3d = test_data_2d.reshape(-1, 1, test_data_2d.shape[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCMlH0nMwGkw"
      },
      "source": [
        "### 3. Simple RNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R5U8PxfjwGkx",
        "outputId": "4516b706-abd5-48ec-dc8a-49b94d106654"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "298/298 [==============================] - 5s 7ms/step - loss: 0.2701 - accuracy: 0.8910 - val_loss: 0.1943 - val_accuracy: 0.9222\n",
            "Epoch 2/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.1942 - accuracy: 0.9229 - val_loss: 0.1881 - val_accuracy: 0.9290\n",
            "Epoch 3/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.1857 - accuracy: 0.9247 - val_loss: 0.1821 - val_accuracy: 0.9281\n",
            "Epoch 4/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.1823 - accuracy: 0.9273 - val_loss: 0.1759 - val_accuracy: 0.9299\n",
            "Epoch 5/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.1788 - accuracy: 0.9281 - val_loss: 0.1750 - val_accuracy: 0.9290\n",
            "Epoch 6/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.1735 - accuracy: 0.9343 - val_loss: 0.1825 - val_accuracy: 0.9245\n",
            "Epoch 7/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.1703 - accuracy: 0.9302 - val_loss: 0.1677 - val_accuracy: 0.9326\n",
            "Epoch 8/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.1685 - accuracy: 0.9298 - val_loss: 0.1640 - val_accuracy: 0.9340\n",
            "Epoch 9/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.1636 - accuracy: 0.9321 - val_loss: 0.1689 - val_accuracy: 0.9308\n",
            "Epoch 10/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.1615 - accuracy: 0.9347 - val_loss: 0.1625 - val_accuracy: 0.9331\n",
            "Epoch 11/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.1557 - accuracy: 0.9383 - val_loss: 0.1543 - val_accuracy: 0.9385\n",
            "Epoch 12/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.1562 - accuracy: 0.9348 - val_loss: 0.1532 - val_accuracy: 0.9403\n",
            "Epoch 13/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.1514 - accuracy: 0.9395 - val_loss: 0.1540 - val_accuracy: 0.9376\n",
            "Epoch 14/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.1501 - accuracy: 0.9376 - val_loss: 0.1486 - val_accuracy: 0.9421\n",
            "Epoch 15/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.1509 - accuracy: 0.9380 - val_loss: 0.1462 - val_accuracy: 0.9417\n",
            "Epoch 16/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.1442 - accuracy: 0.9406 - val_loss: 0.1497 - val_accuracy: 0.9403\n",
            "Epoch 17/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.1395 - accuracy: 0.9424 - val_loss: 0.1446 - val_accuracy: 0.9408\n",
            "Epoch 18/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.1432 - accuracy: 0.9405 - val_loss: 0.1413 - val_accuracy: 0.9444\n",
            "Epoch 19/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.1420 - accuracy: 0.9415 - val_loss: 0.1375 - val_accuracy: 0.9430\n",
            "Epoch 20/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.1357 - accuracy: 0.9418 - val_loss: 0.1393 - val_accuracy: 0.9439\n",
            "Epoch 21/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.1388 - accuracy: 0.9406 - val_loss: 0.1362 - val_accuracy: 0.9453\n",
            "Epoch 22/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.1320 - accuracy: 0.9440 - val_loss: 0.1336 - val_accuracy: 0.9453\n",
            "Epoch 23/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.1310 - accuracy: 0.9447 - val_loss: 0.1335 - val_accuracy: 0.9444\n",
            "Epoch 24/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.1271 - accuracy: 0.9463 - val_loss: 0.1337 - val_accuracy: 0.9462\n",
            "Epoch 25/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.1271 - accuracy: 0.9472 - val_loss: 0.1311 - val_accuracy: 0.9471\n",
            "Epoch 26/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.1249 - accuracy: 0.9467 - val_loss: 0.1287 - val_accuracy: 0.9484\n",
            "Epoch 27/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.1240 - accuracy: 0.9475 - val_loss: 0.1275 - val_accuracy: 0.9484\n",
            "Epoch 28/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.1273 - accuracy: 0.9467 - val_loss: 0.1264 - val_accuracy: 0.9493\n",
            "Epoch 29/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.1200 - accuracy: 0.9488 - val_loss: 0.1259 - val_accuracy: 0.9484\n",
            "Epoch 30/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.1226 - accuracy: 0.9489 - val_loss: 0.1246 - val_accuracy: 0.9489\n",
            "Epoch 31/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.1208 - accuracy: 0.9493 - val_loss: 0.1232 - val_accuracy: 0.9502\n",
            "Epoch 32/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.1169 - accuracy: 0.9491 - val_loss: 0.1216 - val_accuracy: 0.9489\n",
            "Epoch 33/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.1194 - accuracy: 0.9494 - val_loss: 0.1199 - val_accuracy: 0.9530\n",
            "Epoch 34/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.1157 - accuracy: 0.9511 - val_loss: 0.1214 - val_accuracy: 0.9512\n",
            "Epoch 35/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.1155 - accuracy: 0.9529 - val_loss: 0.1206 - val_accuracy: 0.9498\n",
            "Epoch 36/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.1191 - accuracy: 0.9517 - val_loss: 0.1194 - val_accuracy: 0.9530\n",
            "Epoch 37/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.1140 - accuracy: 0.9546 - val_loss: 0.1167 - val_accuracy: 0.9548\n",
            "Epoch 38/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.1146 - accuracy: 0.9534 - val_loss: 0.1174 - val_accuracy: 0.9561\n",
            "Epoch 39/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.1098 - accuracy: 0.9527 - val_loss: 0.1180 - val_accuracy: 0.9534\n",
            "Epoch 40/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.1089 - accuracy: 0.9523 - val_loss: 0.1246 - val_accuracy: 0.9457\n",
            "Epoch 41/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.1135 - accuracy: 0.9536 - val_loss: 0.1169 - val_accuracy: 0.9530\n",
            "Epoch 42/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.1069 - accuracy: 0.9540 - val_loss: 0.1157 - val_accuracy: 0.9489\n",
            "Epoch 43/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.1080 - accuracy: 0.9553 - val_loss: 0.1146 - val_accuracy: 0.9543\n",
            "Epoch 44/500\n",
            "298/298 [==============================] - 2s 7ms/step - loss: 0.1077 - accuracy: 0.9540 - val_loss: 0.1176 - val_accuracy: 0.9561\n",
            "Epoch 45/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.1111 - accuracy: 0.9533 - val_loss: 0.1169 - val_accuracy: 0.9579\n",
            "Epoch 46/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.1071 - accuracy: 0.9548 - val_loss: 0.1120 - val_accuracy: 0.9575\n",
            "Epoch 47/500\n",
            "298/298 [==============================] - 1s 5ms/step - loss: 0.1018 - accuracy: 0.9579 - val_loss: 0.1182 - val_accuracy: 0.9516\n",
            "Epoch 48/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.1081 - accuracy: 0.9546 - val_loss: 0.1148 - val_accuracy: 0.9566\n",
            "Epoch 49/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.1043 - accuracy: 0.9556 - val_loss: 0.1141 - val_accuracy: 0.9552\n",
            "Epoch 50/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.1003 - accuracy: 0.9579 - val_loss: 0.1125 - val_accuracy: 0.9561\n",
            "Epoch 51/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.1059 - accuracy: 0.9537 - val_loss: 0.1149 - val_accuracy: 0.9566\n",
            "Epoch 52/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.1026 - accuracy: 0.9560 - val_loss: 0.1207 - val_accuracy: 0.9552\n",
            "Epoch 53/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.1015 - accuracy: 0.9587 - val_loss: 0.1101 - val_accuracy: 0.9602\n",
            "Epoch 54/500\n",
            "298/298 [==============================] - 1s 5ms/step - loss: 0.0983 - accuracy: 0.9588 - val_loss: 0.1127 - val_accuracy: 0.9579\n",
            "Epoch 55/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.1021 - accuracy: 0.9576 - val_loss: 0.1111 - val_accuracy: 0.9575\n",
            "Epoch 56/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.1004 - accuracy: 0.9593 - val_loss: 0.1086 - val_accuracy: 0.9584\n",
            "Epoch 57/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.1038 - accuracy: 0.9583 - val_loss: 0.1077 - val_accuracy: 0.9593\n",
            "Epoch 58/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0994 - accuracy: 0.9585 - val_loss: 0.1083 - val_accuracy: 0.9584\n",
            "Epoch 59/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0975 - accuracy: 0.9618 - val_loss: 0.1069 - val_accuracy: 0.9584\n",
            "Epoch 60/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0985 - accuracy: 0.9589 - val_loss: 0.1069 - val_accuracy: 0.9607\n",
            "Epoch 61/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0998 - accuracy: 0.9595 - val_loss: 0.1066 - val_accuracy: 0.9597\n",
            "Epoch 62/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0977 - accuracy: 0.9604 - val_loss: 0.1061 - val_accuracy: 0.9597\n",
            "Epoch 63/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0971 - accuracy: 0.9600 - val_loss: 0.1148 - val_accuracy: 0.9588\n",
            "Epoch 64/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0937 - accuracy: 0.9620 - val_loss: 0.1086 - val_accuracy: 0.9607\n",
            "Epoch 65/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0954 - accuracy: 0.9588 - val_loss: 0.1081 - val_accuracy: 0.9588\n",
            "Epoch 66/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.0947 - accuracy: 0.9605 - val_loss: 0.1073 - val_accuracy: 0.9629\n",
            "Epoch 67/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.0959 - accuracy: 0.9603 - val_loss: 0.1123 - val_accuracy: 0.9566\n",
            "Epoch 68/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.0965 - accuracy: 0.9609 - val_loss: 0.1078 - val_accuracy: 0.9593\n",
            "Epoch 69/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0946 - accuracy: 0.9608 - val_loss: 0.1049 - val_accuracy: 0.9616\n",
            "Epoch 70/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0913 - accuracy: 0.9646 - val_loss: 0.1082 - val_accuracy: 0.9611\n",
            "Epoch 71/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0928 - accuracy: 0.9615 - val_loss: 0.1035 - val_accuracy: 0.9629\n",
            "Epoch 72/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0913 - accuracy: 0.9622 - val_loss: 0.1061 - val_accuracy: 0.9634\n",
            "Epoch 73/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0927 - accuracy: 0.9628 - val_loss: 0.1023 - val_accuracy: 0.9616\n",
            "Epoch 74/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0885 - accuracy: 0.9636 - val_loss: 0.1043 - val_accuracy: 0.9593\n",
            "Epoch 75/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0901 - accuracy: 0.9641 - val_loss: 0.1056 - val_accuracy: 0.9579\n",
            "Epoch 76/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0952 - accuracy: 0.9606 - val_loss: 0.1074 - val_accuracy: 0.9602\n",
            "Epoch 77/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0946 - accuracy: 0.9586 - val_loss: 0.1033 - val_accuracy: 0.9607\n",
            "Epoch 78/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0911 - accuracy: 0.9621 - val_loss: 0.1075 - val_accuracy: 0.9629\n",
            "Epoch 79/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0849 - accuracy: 0.9651 - val_loss: 0.1051 - val_accuracy: 0.9629\n",
            "Epoch 80/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0881 - accuracy: 0.9648 - val_loss: 0.1076 - val_accuracy: 0.9643\n",
            "Epoch 81/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0883 - accuracy: 0.9625 - val_loss: 0.1036 - val_accuracy: 0.9638\n",
            "Epoch 82/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.0868 - accuracy: 0.9642 - val_loss: 0.1071 - val_accuracy: 0.9620\n",
            "Epoch 83/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.0903 - accuracy: 0.9613 - val_loss: 0.1029 - val_accuracy: 0.9634\n",
            "Epoch 84/500\n",
            "298/298 [==============================] - 2s 7ms/step - loss: 0.0861 - accuracy: 0.9635 - val_loss: 0.1019 - val_accuracy: 0.9629\n",
            "Epoch 85/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.0873 - accuracy: 0.9645 - val_loss: 0.1020 - val_accuracy: 0.9647\n",
            "Epoch 86/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0877 - accuracy: 0.9650 - val_loss: 0.1031 - val_accuracy: 0.9638\n",
            "Epoch 87/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.0855 - accuracy: 0.9641 - val_loss: 0.1016 - val_accuracy: 0.9629\n",
            "Epoch 88/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.0868 - accuracy: 0.9639 - val_loss: 0.1032 - val_accuracy: 0.9647\n",
            "Epoch 89/500\n",
            "298/298 [==============================] - 2s 7ms/step - loss: 0.0873 - accuracy: 0.9660 - val_loss: 0.1033 - val_accuracy: 0.9638\n",
            "Epoch 90/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.0855 - accuracy: 0.9649 - val_loss: 0.1027 - val_accuracy: 0.9607\n",
            "Epoch 91/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0858 - accuracy: 0.9671 - val_loss: 0.1035 - val_accuracy: 0.9643\n",
            "Epoch 92/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0867 - accuracy: 0.9649 - val_loss: 0.1013 - val_accuracy: 0.9647\n",
            "Epoch 93/500\n",
            "298/298 [==============================] - 2s 7ms/step - loss: 0.0861 - accuracy: 0.9649 - val_loss: 0.1046 - val_accuracy: 0.9597\n",
            "Epoch 94/500\n",
            "298/298 [==============================] - 2s 7ms/step - loss: 0.0880 - accuracy: 0.9643 - val_loss: 0.1011 - val_accuracy: 0.9643\n",
            "Epoch 95/500\n",
            "298/298 [==============================] - 2s 7ms/step - loss: 0.0851 - accuracy: 0.9647 - val_loss: 0.1030 - val_accuracy: 0.9620\n",
            "Epoch 96/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.0845 - accuracy: 0.9655 - val_loss: 0.0990 - val_accuracy: 0.9629\n",
            "Epoch 97/500\n",
            "298/298 [==============================] - 2s 7ms/step - loss: 0.0832 - accuracy: 0.9683 - val_loss: 0.1024 - val_accuracy: 0.9620\n",
            "Epoch 98/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0830 - accuracy: 0.9655 - val_loss: 0.1076 - val_accuracy: 0.9647\n",
            "Epoch 99/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0853 - accuracy: 0.9650 - val_loss: 0.1004 - val_accuracy: 0.9652\n",
            "Epoch 100/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.0884 - accuracy: 0.9640 - val_loss: 0.0993 - val_accuracy: 0.9656\n",
            "Epoch 101/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.0829 - accuracy: 0.9675 - val_loss: 0.1000 - val_accuracy: 0.9652\n",
            "Epoch 102/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0839 - accuracy: 0.9647 - val_loss: 0.1001 - val_accuracy: 0.9629\n",
            "Epoch 103/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.0826 - accuracy: 0.9658 - val_loss: 0.0980 - val_accuracy: 0.9652\n",
            "Epoch 104/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.0820 - accuracy: 0.9657 - val_loss: 0.0985 - val_accuracy: 0.9643\n",
            "Epoch 105/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0830 - accuracy: 0.9675 - val_loss: 0.1001 - val_accuracy: 0.9688\n",
            "Epoch 106/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0794 - accuracy: 0.9686 - val_loss: 0.0987 - val_accuracy: 0.9692\n",
            "Epoch 107/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0828 - accuracy: 0.9656 - val_loss: 0.1018 - val_accuracy: 0.9665\n",
            "Epoch 108/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0829 - accuracy: 0.9656 - val_loss: 0.1000 - val_accuracy: 0.9679\n",
            "Epoch 109/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0799 - accuracy: 0.9669 - val_loss: 0.1004 - val_accuracy: 0.9629\n",
            "Epoch 110/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.0844 - accuracy: 0.9657 - val_loss: 0.0994 - val_accuracy: 0.9647\n",
            "Epoch 111/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0792 - accuracy: 0.9674 - val_loss: 0.1013 - val_accuracy: 0.9625\n",
            "Epoch 112/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.0745 - accuracy: 0.9705 - val_loss: 0.0990 - val_accuracy: 0.9643\n",
            "Epoch 113/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0779 - accuracy: 0.9678 - val_loss: 0.0991 - val_accuracy: 0.9674\n",
            "Epoch 114/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0795 - accuracy: 0.9672 - val_loss: 0.1055 - val_accuracy: 0.9638\n",
            "Epoch 115/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0804 - accuracy: 0.9659 - val_loss: 0.1031 - val_accuracy: 0.9652\n",
            "Epoch 116/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0791 - accuracy: 0.9675 - val_loss: 0.1006 - val_accuracy: 0.9656\n",
            "Epoch 117/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.0782 - accuracy: 0.9662 - val_loss: 0.1002 - val_accuracy: 0.9625\n",
            "Epoch 118/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0803 - accuracy: 0.9671 - val_loss: 0.0996 - val_accuracy: 0.9643\n",
            "Epoch 119/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0772 - accuracy: 0.9693 - val_loss: 0.1006 - val_accuracy: 0.9638\n",
            "Epoch 120/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0780 - accuracy: 0.9701 - val_loss: 0.1006 - val_accuracy: 0.9665\n",
            "Epoch 121/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0787 - accuracy: 0.9672 - val_loss: 0.0993 - val_accuracy: 0.9629\n",
            "Epoch 122/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0812 - accuracy: 0.9689 - val_loss: 0.1004 - val_accuracy: 0.9620\n",
            "Epoch 123/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0755 - accuracy: 0.9704 - val_loss: 0.1072 - val_accuracy: 0.9584\n",
            "Epoch 124/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0738 - accuracy: 0.9683 - val_loss: 0.1013 - val_accuracy: 0.9647\n",
            "Epoch 125/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0784 - accuracy: 0.9695 - val_loss: 0.1020 - val_accuracy: 0.9616\n",
            "Epoch 126/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0827 - accuracy: 0.9641 - val_loss: 0.1005 - val_accuracy: 0.9670\n",
            "Epoch 127/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0777 - accuracy: 0.9664 - val_loss: 0.1011 - val_accuracy: 0.9665\n",
            "Epoch 128/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0790 - accuracy: 0.9683 - val_loss: 0.0972 - val_accuracy: 0.9674\n",
            "Epoch 129/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0777 - accuracy: 0.9684 - val_loss: 0.0982 - val_accuracy: 0.9665\n",
            "Epoch 130/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0772 - accuracy: 0.9699 - val_loss: 0.0996 - val_accuracy: 0.9625\n",
            "Epoch 131/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0793 - accuracy: 0.9670 - val_loss: 0.0975 - val_accuracy: 0.9656\n",
            "Epoch 132/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0821 - accuracy: 0.9671 - val_loss: 0.0990 - val_accuracy: 0.9638\n",
            "Epoch 133/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0766 - accuracy: 0.9700 - val_loss: 0.0991 - val_accuracy: 0.9643\n",
            "Epoch 134/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.0783 - accuracy: 0.9691 - val_loss: 0.0992 - val_accuracy: 0.9634\n",
            "Epoch 135/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.0780 - accuracy: 0.9691 - val_loss: 0.0975 - val_accuracy: 0.9701\n",
            "Epoch 136/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.0770 - accuracy: 0.9684 - val_loss: 0.1008 - val_accuracy: 0.9670\n",
            "Epoch 137/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.0760 - accuracy: 0.9701 - val_loss: 0.0990 - val_accuracy: 0.9683\n",
            "Epoch 138/500\n",
            "298/298 [==============================] - 2s 7ms/step - loss: 0.0745 - accuracy: 0.9699 - val_loss: 0.1019 - val_accuracy: 0.9643\n",
            "Epoch 139/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0771 - accuracy: 0.9684 - val_loss: 0.1000 - val_accuracy: 0.9647\n",
            "Epoch 140/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.0767 - accuracy: 0.9687 - val_loss: 0.0975 - val_accuracy: 0.9616\n",
            "Epoch 141/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.0742 - accuracy: 0.9696 - val_loss: 0.0985 - val_accuracy: 0.9611\n",
            "Epoch 142/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.0747 - accuracy: 0.9694 - val_loss: 0.0967 - val_accuracy: 0.9634\n",
            "Epoch 143/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.0738 - accuracy: 0.9693 - val_loss: 0.1005 - val_accuracy: 0.9674\n",
            "Epoch 144/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.0750 - accuracy: 0.9691 - val_loss: 0.0991 - val_accuracy: 0.9643\n",
            "Epoch 145/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.0755 - accuracy: 0.9689 - val_loss: 0.0993 - val_accuracy: 0.9661\n",
            "Epoch 146/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.0732 - accuracy: 0.9701 - val_loss: 0.1021 - val_accuracy: 0.9661\n",
            "Epoch 147/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.0770 - accuracy: 0.9706 - val_loss: 0.0976 - val_accuracy: 0.9683\n",
            "Epoch 148/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.0773 - accuracy: 0.9705 - val_loss: 0.0966 - val_accuracy: 0.9652\n",
            "Epoch 149/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.0706 - accuracy: 0.9718 - val_loss: 0.0982 - val_accuracy: 0.9638\n",
            "Epoch 150/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.0733 - accuracy: 0.9701 - val_loss: 0.0976 - val_accuracy: 0.9656\n",
            "Epoch 151/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.0753 - accuracy: 0.9699 - val_loss: 0.1000 - val_accuracy: 0.9638\n",
            "Epoch 152/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.0754 - accuracy: 0.9712 - val_loss: 0.0965 - val_accuracy: 0.9638\n",
            "Epoch 153/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.0756 - accuracy: 0.9691 - val_loss: 0.0956 - val_accuracy: 0.9634\n",
            "Epoch 154/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.0736 - accuracy: 0.9726 - val_loss: 0.0991 - val_accuracy: 0.9683\n",
            "Epoch 155/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.0684 - accuracy: 0.9727 - val_loss: 0.0938 - val_accuracy: 0.9625\n",
            "Epoch 156/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.0739 - accuracy: 0.9702 - val_loss: 0.0957 - val_accuracy: 0.9652\n",
            "Epoch 157/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.0760 - accuracy: 0.9695 - val_loss: 0.0974 - val_accuracy: 0.9656\n",
            "Epoch 158/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.0742 - accuracy: 0.9708 - val_loss: 0.0973 - val_accuracy: 0.9629\n",
            "Epoch 159/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.0727 - accuracy: 0.9712 - val_loss: 0.1007 - val_accuracy: 0.9652\n",
            "Epoch 160/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.0738 - accuracy: 0.9697 - val_loss: 0.1014 - val_accuracy: 0.9652\n",
            "Epoch 161/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.0725 - accuracy: 0.9720 - val_loss: 0.0951 - val_accuracy: 0.9661\n",
            "Epoch 162/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.0736 - accuracy: 0.9702 - val_loss: 0.1062 - val_accuracy: 0.9652\n",
            "Epoch 163/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0715 - accuracy: 0.9715 - val_loss: 0.0991 - val_accuracy: 0.9656\n",
            "Epoch 164/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.0737 - accuracy: 0.9704 - val_loss: 0.0983 - val_accuracy: 0.9661\n",
            "Epoch 165/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.0714 - accuracy: 0.9725 - val_loss: 0.1002 - val_accuracy: 0.9643\n",
            "Epoch 166/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.0701 - accuracy: 0.9715 - val_loss: 0.0985 - val_accuracy: 0.9638\n",
            "Epoch 167/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.0696 - accuracy: 0.9740 - val_loss: 0.0990 - val_accuracy: 0.9661\n",
            "Epoch 168/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0687 - accuracy: 0.9717 - val_loss: 0.0984 - val_accuracy: 0.9638\n",
            "Epoch 169/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0703 - accuracy: 0.9709 - val_loss: 0.1007 - val_accuracy: 0.9625\n",
            "Epoch 170/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0736 - accuracy: 0.9692 - val_loss: 0.1008 - val_accuracy: 0.9634\n",
            "Epoch 171/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0730 - accuracy: 0.9704 - val_loss: 0.0988 - val_accuracy: 0.9665\n",
            "Epoch 172/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.0706 - accuracy: 0.9704 - val_loss: 0.0978 - val_accuracy: 0.9634\n",
            "Epoch 173/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.0680 - accuracy: 0.9728 - val_loss: 0.0998 - val_accuracy: 0.9629\n",
            "Epoch 174/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0686 - accuracy: 0.9730 - val_loss: 0.0981 - val_accuracy: 0.9701\n",
            "Epoch 175/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0710 - accuracy: 0.9720 - val_loss: 0.0991 - val_accuracy: 0.9679\n",
            "Epoch 176/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0732 - accuracy: 0.9709 - val_loss: 0.0981 - val_accuracy: 0.9652\n",
            "Epoch 177/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0699 - accuracy: 0.9728 - val_loss: 0.1005 - val_accuracy: 0.9661\n",
            "Epoch 178/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0722 - accuracy: 0.9714 - val_loss: 0.1023 - val_accuracy: 0.9638\n",
            "Epoch 179/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.0698 - accuracy: 0.9709 - val_loss: 0.0995 - val_accuracy: 0.9661\n",
            "Epoch 180/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0712 - accuracy: 0.9709 - val_loss: 0.0996 - val_accuracy: 0.9634\n",
            "Epoch 181/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.0672 - accuracy: 0.9728 - val_loss: 0.1005 - val_accuracy: 0.9625\n",
            "Epoch 182/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.0698 - accuracy: 0.9730 - val_loss: 0.1035 - val_accuracy: 0.9643\n",
            "Epoch 183/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0711 - accuracy: 0.9714 - val_loss: 0.1005 - val_accuracy: 0.9652\n",
            "Epoch 184/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0680 - accuracy: 0.9735 - val_loss: 0.0996 - val_accuracy: 0.9652\n",
            "Epoch 185/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0704 - accuracy: 0.9721 - val_loss: 0.0979 - val_accuracy: 0.9629\n",
            "Epoch 186/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0676 - accuracy: 0.9709 - val_loss: 0.1000 - val_accuracy: 0.9656\n",
            "Epoch 187/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0690 - accuracy: 0.9704 - val_loss: 0.0969 - val_accuracy: 0.9620\n",
            "Epoch 188/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.0702 - accuracy: 0.9712 - val_loss: 0.0957 - val_accuracy: 0.9629\n",
            "Epoch 189/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.0676 - accuracy: 0.9727 - val_loss: 0.0971 - val_accuracy: 0.9647\n",
            "Epoch 190/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.0666 - accuracy: 0.9736 - val_loss: 0.0959 - val_accuracy: 0.9670\n",
            "Epoch 191/500\n",
            "298/298 [==============================] - 2s 7ms/step - loss: 0.0678 - accuracy: 0.9731 - val_loss: 0.0999 - val_accuracy: 0.9625\n",
            "Epoch 192/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.0653 - accuracy: 0.9741 - val_loss: 0.1009 - val_accuracy: 0.9656\n",
            "Epoch 193/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.0671 - accuracy: 0.9746 - val_loss: 0.0958 - val_accuracy: 0.9647\n",
            "Epoch 194/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.0669 - accuracy: 0.9743 - val_loss: 0.0979 - val_accuracy: 0.9652\n",
            "Epoch 195/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.0698 - accuracy: 0.9728 - val_loss: 0.0986 - val_accuracy: 0.9638\n",
            "Epoch 196/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.0678 - accuracy: 0.9740 - val_loss: 0.1028 - val_accuracy: 0.9620\n",
            "Epoch 197/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.0659 - accuracy: 0.9721 - val_loss: 0.1034 - val_accuracy: 0.9670\n",
            "Epoch 198/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.0688 - accuracy: 0.9739 - val_loss: 0.1008 - val_accuracy: 0.9674\n",
            "Epoch 199/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.0694 - accuracy: 0.9726 - val_loss: 0.0961 - val_accuracy: 0.9647\n",
            "Epoch 200/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.0696 - accuracy: 0.9716 - val_loss: 0.1025 - val_accuracy: 0.9656\n",
            "Epoch 201/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.0699 - accuracy: 0.9715 - val_loss: 0.0952 - val_accuracy: 0.9665\n",
            "Epoch 202/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.0692 - accuracy: 0.9722 - val_loss: 0.0980 - val_accuracy: 0.9647\n",
            "Epoch 203/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.0610 - accuracy: 0.9765 - val_loss: 0.0984 - val_accuracy: 0.9634\n",
            "Epoch 204/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.0689 - accuracy: 0.9737 - val_loss: 0.0988 - val_accuracy: 0.9656\n",
            "Epoch 205/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.0671 - accuracy: 0.9726 - val_loss: 0.0979 - val_accuracy: 0.9670\n",
            "Epoch 206/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0679 - accuracy: 0.9731 - val_loss: 0.0988 - val_accuracy: 0.9670\n",
            "Epoch 207/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0653 - accuracy: 0.9736 - val_loss: 0.1038 - val_accuracy: 0.9652\n",
            "Epoch 208/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0692 - accuracy: 0.9717 - val_loss: 0.0972 - val_accuracy: 0.9661\n",
            "Epoch 209/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0699 - accuracy: 0.9733 - val_loss: 0.0990 - val_accuracy: 0.9625\n",
            "Epoch 210/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0661 - accuracy: 0.9742 - val_loss: 0.0989 - val_accuracy: 0.9643\n",
            "Epoch 211/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0655 - accuracy: 0.9751 - val_loss: 0.1009 - val_accuracy: 0.9661\n",
            "Epoch 212/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0666 - accuracy: 0.9735 - val_loss: 0.1005 - val_accuracy: 0.9643\n",
            "Epoch 213/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0650 - accuracy: 0.9753 - val_loss: 0.0998 - val_accuracy: 0.9647\n",
            "Epoch 214/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0687 - accuracy: 0.9725 - val_loss: 0.0980 - val_accuracy: 0.9643\n",
            "Epoch 215/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0643 - accuracy: 0.9737 - val_loss: 0.0991 - val_accuracy: 0.9647\n",
            "Epoch 216/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.0642 - accuracy: 0.9738 - val_loss: 0.0980 - val_accuracy: 0.9643\n",
            "Epoch 217/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.0660 - accuracy: 0.9738 - val_loss: 0.0967 - val_accuracy: 0.9643\n",
            "Epoch 218/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.0658 - accuracy: 0.9726 - val_loss: 0.1001 - val_accuracy: 0.9647\n",
            "Epoch 219/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.0632 - accuracy: 0.9736 - val_loss: 0.0992 - val_accuracy: 0.9688\n",
            "Epoch 220/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.0681 - accuracy: 0.9736 - val_loss: 0.1004 - val_accuracy: 0.9683\n",
            "Epoch 221/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.0685 - accuracy: 0.9725 - val_loss: 0.0968 - val_accuracy: 0.9643\n",
            "Epoch 222/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.0645 - accuracy: 0.9728 - val_loss: 0.0990 - val_accuracy: 0.9665\n",
            "Epoch 223/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.0668 - accuracy: 0.9726 - val_loss: 0.0959 - val_accuracy: 0.9652\n",
            "Epoch 224/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.0617 - accuracy: 0.9742 - val_loss: 0.0974 - val_accuracy: 0.9661\n",
            "Epoch 225/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.0691 - accuracy: 0.9737 - val_loss: 0.0986 - val_accuracy: 0.9670\n",
            "Epoch 226/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.0655 - accuracy: 0.9730 - val_loss: 0.1007 - val_accuracy: 0.9670\n",
            "Epoch 227/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.0664 - accuracy: 0.9722 - val_loss: 0.0993 - val_accuracy: 0.9670\n",
            "Epoch 228/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.0626 - accuracy: 0.9760 - val_loss: 0.0999 - val_accuracy: 0.9647\n",
            "Epoch 229/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.0602 - accuracy: 0.9747 - val_loss: 0.1030 - val_accuracy: 0.9629\n",
            "Epoch 230/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.0685 - accuracy: 0.9732 - val_loss: 0.0996 - val_accuracy: 0.9647\n",
            "Epoch 231/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.0665 - accuracy: 0.9745 - val_loss: 0.1010 - val_accuracy: 0.9656\n",
            "Epoch 232/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.0675 - accuracy: 0.9735 - val_loss: 0.1004 - val_accuracy: 0.9656\n",
            "Epoch 233/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.0673 - accuracy: 0.9732 - val_loss: 0.1049 - val_accuracy: 0.9674\n",
            "Epoch 234/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.0653 - accuracy: 0.9737 - val_loss: 0.1002 - val_accuracy: 0.9661\n",
            "Epoch 235/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.0610 - accuracy: 0.9774 - val_loss: 0.1012 - val_accuracy: 0.9643\n",
            "Epoch 236/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.0630 - accuracy: 0.9750 - val_loss: 0.0971 - val_accuracy: 0.9661\n",
            "Epoch 237/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.0661 - accuracy: 0.9736 - val_loss: 0.0986 - val_accuracy: 0.9674\n",
            "Epoch 238/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0585 - accuracy: 0.9763 - val_loss: 0.1004 - val_accuracy: 0.9643\n",
            "Epoch 239/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0637 - accuracy: 0.9723 - val_loss: 0.1030 - val_accuracy: 0.9656\n",
            "Epoch 240/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0610 - accuracy: 0.9761 - val_loss: 0.1022 - val_accuracy: 0.9661\n",
            "Epoch 241/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0627 - accuracy: 0.9745 - val_loss: 0.1011 - val_accuracy: 0.9638\n",
            "Epoch 242/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0642 - accuracy: 0.9739 - val_loss: 0.0977 - val_accuracy: 0.9665\n",
            "Epoch 243/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0661 - accuracy: 0.9749 - val_loss: 0.0988 - val_accuracy: 0.9688\n",
            "Epoch 244/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.0613 - accuracy: 0.9776 - val_loss: 0.0984 - val_accuracy: 0.9665\n",
            "Epoch 245/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.0660 - accuracy: 0.9745 - val_loss: 0.1001 - val_accuracy: 0.9647\n",
            "Epoch 246/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0632 - accuracy: 0.9749 - val_loss: 0.1007 - val_accuracy: 0.9656\n",
            "Epoch 247/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0649 - accuracy: 0.9735 - val_loss: 0.1032 - val_accuracy: 0.9647\n",
            "Epoch 248/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0647 - accuracy: 0.9739 - val_loss: 0.0998 - val_accuracy: 0.9656\n",
            "Epoch 249/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0629 - accuracy: 0.9753 - val_loss: 0.0996 - val_accuracy: 0.9652\n",
            "Epoch 250/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0618 - accuracy: 0.9750 - val_loss: 0.0944 - val_accuracy: 0.9656\n",
            "Epoch 251/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0650 - accuracy: 0.9745 - val_loss: 0.0988 - val_accuracy: 0.9643\n",
            "Epoch 252/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0642 - accuracy: 0.9744 - val_loss: 0.1012 - val_accuracy: 0.9661\n",
            "Epoch 253/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0648 - accuracy: 0.9730 - val_loss: 0.0995 - val_accuracy: 0.9634\n",
            "Epoch 254/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0649 - accuracy: 0.9741 - val_loss: 0.0999 - val_accuracy: 0.9670\n",
            "Epoch 255/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0638 - accuracy: 0.9737 - val_loss: 0.0978 - val_accuracy: 0.9647\n",
            "Epoch 256/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0623 - accuracy: 0.9756 - val_loss: 0.0981 - val_accuracy: 0.9647\n",
            "Epoch 257/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0623 - accuracy: 0.9740 - val_loss: 0.0982 - val_accuracy: 0.9697\n",
            "Epoch 258/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0616 - accuracy: 0.9740 - val_loss: 0.0985 - val_accuracy: 0.9638\n",
            "Epoch 259/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0621 - accuracy: 0.9752 - val_loss: 0.0983 - val_accuracy: 0.9652\n",
            "Epoch 260/500\n",
            "298/298 [==============================] - 1s 5ms/step - loss: 0.0620 - accuracy: 0.9748 - val_loss: 0.1048 - val_accuracy: 0.9679\n",
            "Epoch 261/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0657 - accuracy: 0.9732 - val_loss: 0.0962 - val_accuracy: 0.9652\n",
            "Epoch 262/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0621 - accuracy: 0.9749 - val_loss: 0.1012 - val_accuracy: 0.9674\n",
            "Epoch 263/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0594 - accuracy: 0.9773 - val_loss: 0.1007 - val_accuracy: 0.9661\n",
            "Epoch 264/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0586 - accuracy: 0.9759 - val_loss: 0.0984 - val_accuracy: 0.9656\n",
            "Epoch 265/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0621 - accuracy: 0.9740 - val_loss: 0.1014 - val_accuracy: 0.9665\n",
            "Epoch 266/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0646 - accuracy: 0.9729 - val_loss: 0.0984 - val_accuracy: 0.9647\n",
            "Epoch 267/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0611 - accuracy: 0.9747 - val_loss: 0.0967 - val_accuracy: 0.9674\n",
            "Epoch 268/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0633 - accuracy: 0.9746 - val_loss: 0.1016 - val_accuracy: 0.9652\n",
            "Epoch 269/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0590 - accuracy: 0.9774 - val_loss: 0.0982 - val_accuracy: 0.9638\n",
            "Epoch 270/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0610 - accuracy: 0.9761 - val_loss: 0.0989 - val_accuracy: 0.9674\n",
            "Epoch 271/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0640 - accuracy: 0.9755 - val_loss: 0.0994 - val_accuracy: 0.9670\n",
            "Epoch 272/500\n",
            "298/298 [==============================] - 1s 5ms/step - loss: 0.0614 - accuracy: 0.9754 - val_loss: 0.1000 - val_accuracy: 0.9665\n",
            "Epoch 273/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0616 - accuracy: 0.9758 - val_loss: 0.1003 - val_accuracy: 0.9656\n",
            "Epoch 274/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0614 - accuracy: 0.9752 - val_loss: 0.1036 - val_accuracy: 0.9665\n",
            "Epoch 275/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0623 - accuracy: 0.9769 - val_loss: 0.0989 - val_accuracy: 0.9665\n",
            "Epoch 276/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0631 - accuracy: 0.9751 - val_loss: 0.1022 - val_accuracy: 0.9647\n",
            "Epoch 277/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0618 - accuracy: 0.9751 - val_loss: 0.0990 - val_accuracy: 0.9643\n",
            "Epoch 278/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.0592 - accuracy: 0.9751 - val_loss: 0.1017 - val_accuracy: 0.9643\n",
            "Epoch 279/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.0599 - accuracy: 0.9755 - val_loss: 0.1005 - val_accuracy: 0.9643\n",
            "Epoch 280/500\n",
            "298/298 [==============================] - 2s 7ms/step - loss: 0.0645 - accuracy: 0.9733 - val_loss: 0.1002 - val_accuracy: 0.9652\n",
            "Epoch 281/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.0649 - accuracy: 0.9734 - val_loss: 0.0994 - val_accuracy: 0.9652\n",
            "Epoch 282/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0620 - accuracy: 0.9748 - val_loss: 0.0988 - val_accuracy: 0.9665\n",
            "Epoch 283/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0603 - accuracy: 0.9757 - val_loss: 0.1008 - val_accuracy: 0.9661\n",
            "Epoch 284/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0598 - accuracy: 0.9762 - val_loss: 0.1008 - val_accuracy: 0.9665\n",
            "Epoch 285/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0598 - accuracy: 0.9768 - val_loss: 0.0985 - val_accuracy: 0.9656\n",
            "Epoch 286/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0623 - accuracy: 0.9752 - val_loss: 0.1003 - val_accuracy: 0.9643\n",
            "Epoch 287/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0644 - accuracy: 0.9741 - val_loss: 0.0981 - val_accuracy: 0.9647\n",
            "Epoch 288/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0587 - accuracy: 0.9769 - val_loss: 0.0985 - val_accuracy: 0.9643\n",
            "Epoch 289/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0632 - accuracy: 0.9736 - val_loss: 0.1017 - val_accuracy: 0.9634\n",
            "Epoch 290/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0593 - accuracy: 0.9757 - val_loss: 0.0990 - val_accuracy: 0.9647\n",
            "Epoch 291/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0620 - accuracy: 0.9746 - val_loss: 0.0995 - val_accuracy: 0.9652\n",
            "Epoch 292/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0599 - accuracy: 0.9752 - val_loss: 0.0963 - val_accuracy: 0.9665\n",
            "Epoch 293/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0599 - accuracy: 0.9768 - val_loss: 0.0991 - val_accuracy: 0.9661\n",
            "Epoch 294/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0609 - accuracy: 0.9751 - val_loss: 0.0993 - val_accuracy: 0.9652\n",
            "Epoch 295/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0647 - accuracy: 0.9735 - val_loss: 0.1031 - val_accuracy: 0.9683\n",
            "Epoch 296/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0604 - accuracy: 0.9766 - val_loss: 0.0974 - val_accuracy: 0.9647\n",
            "Epoch 297/500\n",
            "298/298 [==============================] - 1s 5ms/step - loss: 0.0596 - accuracy: 0.9749 - val_loss: 0.0993 - val_accuracy: 0.9661\n",
            "Epoch 298/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0588 - accuracy: 0.9751 - val_loss: 0.0995 - val_accuracy: 0.9625\n",
            "Epoch 299/500\n",
            "298/298 [==============================] - 1s 5ms/step - loss: 0.0604 - accuracy: 0.9760 - val_loss: 0.1016 - val_accuracy: 0.9661\n",
            "Epoch 300/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0610 - accuracy: 0.9753 - val_loss: 0.1002 - val_accuracy: 0.9670\n",
            "Epoch 301/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0661 - accuracy: 0.9738 - val_loss: 0.0998 - val_accuracy: 0.9661\n",
            "Epoch 302/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0613 - accuracy: 0.9751 - val_loss: 0.0991 - val_accuracy: 0.9656\n",
            "Epoch 303/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0602 - accuracy: 0.9756 - val_loss: 0.0985 - val_accuracy: 0.9652\n",
            "Epoch 304/500\n",
            "298/298 [==============================] - 1s 5ms/step - loss: 0.0569 - accuracy: 0.9772 - val_loss: 0.0987 - val_accuracy: 0.9643\n",
            "Epoch 305/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0591 - accuracy: 0.9759 - val_loss: 0.0986 - val_accuracy: 0.9661\n",
            "Epoch 306/500\n",
            "298/298 [==============================] - 1s 5ms/step - loss: 0.0577 - accuracy: 0.9766 - val_loss: 0.1005 - val_accuracy: 0.9643\n",
            "Epoch 307/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0583 - accuracy: 0.9767 - val_loss: 0.0998 - val_accuracy: 0.9665\n",
            "Epoch 308/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0601 - accuracy: 0.9748 - val_loss: 0.1021 - val_accuracy: 0.9647\n",
            "Epoch 309/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0622 - accuracy: 0.9759 - val_loss: 0.1041 - val_accuracy: 0.9638\n",
            "Epoch 310/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0632 - accuracy: 0.9749 - val_loss: 0.1024 - val_accuracy: 0.9670\n",
            "Epoch 311/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0583 - accuracy: 0.9765 - val_loss: 0.1011 - val_accuracy: 0.9652\n",
            "Epoch 312/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0588 - accuracy: 0.9767 - val_loss: 0.1027 - val_accuracy: 0.9679\n",
            "Epoch 313/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0598 - accuracy: 0.9752 - val_loss: 0.0993 - val_accuracy: 0.9674\n",
            "Epoch 314/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0602 - accuracy: 0.9763 - val_loss: 0.1017 - val_accuracy: 0.9652\n",
            "Epoch 315/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0579 - accuracy: 0.9774 - val_loss: 0.0995 - val_accuracy: 0.9670\n",
            "Epoch 316/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0564 - accuracy: 0.9792 - val_loss: 0.0997 - val_accuracy: 0.9643\n",
            "Epoch 317/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.0631 - accuracy: 0.9762 - val_loss: 0.1003 - val_accuracy: 0.9661\n",
            "Epoch 318/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0578 - accuracy: 0.9758 - val_loss: 0.0979 - val_accuracy: 0.9656\n",
            "Epoch 319/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0558 - accuracy: 0.9771 - val_loss: 0.1034 - val_accuracy: 0.9679\n",
            "Epoch 320/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.0603 - accuracy: 0.9755 - val_loss: 0.1024 - val_accuracy: 0.9647\n",
            "Epoch 321/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0577 - accuracy: 0.9776 - val_loss: 0.1006 - val_accuracy: 0.9634\n",
            "Epoch 322/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0574 - accuracy: 0.9776 - val_loss: 0.1045 - val_accuracy: 0.9638\n",
            "Epoch 323/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0591 - accuracy: 0.9761 - val_loss: 0.1020 - val_accuracy: 0.9638\n",
            "Epoch 324/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0583 - accuracy: 0.9760 - val_loss: 0.1041 - val_accuracy: 0.9629\n",
            "Epoch 325/500\n",
            "298/298 [==============================] - 1s 5ms/step - loss: 0.0587 - accuracy: 0.9769 - val_loss: 0.1053 - val_accuracy: 0.9638\n",
            "Epoch 326/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0560 - accuracy: 0.9768 - val_loss: 0.1030 - val_accuracy: 0.9670\n",
            "Epoch 327/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0635 - accuracy: 0.9767 - val_loss: 0.1024 - val_accuracy: 0.9629\n",
            "Epoch 328/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0628 - accuracy: 0.9742 - val_loss: 0.1007 - val_accuracy: 0.9625\n",
            "Epoch 329/500\n",
            "298/298 [==============================] - 1s 5ms/step - loss: 0.0585 - accuracy: 0.9771 - val_loss: 0.1015 - val_accuracy: 0.9629\n",
            "Epoch 330/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0605 - accuracy: 0.9753 - val_loss: 0.0998 - val_accuracy: 0.9647\n",
            "Epoch 331/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0631 - accuracy: 0.9755 - val_loss: 0.1026 - val_accuracy: 0.9638\n",
            "Epoch 332/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0572 - accuracy: 0.9781 - val_loss: 0.1007 - val_accuracy: 0.9629\n",
            "Epoch 333/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0584 - accuracy: 0.9771 - val_loss: 0.1065 - val_accuracy: 0.9643\n",
            "Epoch 334/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0606 - accuracy: 0.9764 - val_loss: 0.1052 - val_accuracy: 0.9652\n",
            "Epoch 335/500\n",
            "298/298 [==============================] - 1s 5ms/step - loss: 0.0605 - accuracy: 0.9773 - val_loss: 0.1014 - val_accuracy: 0.9643\n",
            "Epoch 336/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0590 - accuracy: 0.9755 - val_loss: 0.1017 - val_accuracy: 0.9647\n",
            "Epoch 337/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0585 - accuracy: 0.9772 - val_loss: 0.1016 - val_accuracy: 0.9634\n",
            "Epoch 338/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0587 - accuracy: 0.9769 - val_loss: 0.0971 - val_accuracy: 0.9656\n",
            "Epoch 339/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0551 - accuracy: 0.9777 - val_loss: 0.0997 - val_accuracy: 0.9638\n",
            "Epoch 340/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0580 - accuracy: 0.9773 - val_loss: 0.1042 - val_accuracy: 0.9647\n",
            "Epoch 341/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0630 - accuracy: 0.9763 - val_loss: 0.1012 - val_accuracy: 0.9647\n",
            "Epoch 342/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.0605 - accuracy: 0.9757 - val_loss: 0.1026 - val_accuracy: 0.9647\n",
            "Epoch 343/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.0600 - accuracy: 0.9753 - val_loss: 0.1014 - val_accuracy: 0.9647\n",
            "Epoch 344/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0571 - accuracy: 0.9773 - val_loss: 0.1021 - val_accuracy: 0.9638\n",
            "Epoch 345/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0564 - accuracy: 0.9772 - val_loss: 0.1060 - val_accuracy: 0.9661\n",
            "Epoch 346/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.0624 - accuracy: 0.9754 - val_loss: 0.1041 - val_accuracy: 0.9652\n",
            "Epoch 347/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0589 - accuracy: 0.9741 - val_loss: 0.1063 - val_accuracy: 0.9656\n",
            "Epoch 348/500\n",
            "298/298 [==============================] - 1s 5ms/step - loss: 0.0607 - accuracy: 0.9769 - val_loss: 0.1019 - val_accuracy: 0.9647\n",
            "Epoch 349/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0548 - accuracy: 0.9766 - val_loss: 0.1038 - val_accuracy: 0.9647\n",
            "Epoch 350/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0572 - accuracy: 0.9772 - val_loss: 0.0996 - val_accuracy: 0.9670\n",
            "Epoch 351/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0543 - accuracy: 0.9776 - val_loss: 0.1022 - val_accuracy: 0.9661\n",
            "Epoch 352/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0565 - accuracy: 0.9785 - val_loss: 0.1036 - val_accuracy: 0.9670\n",
            "Epoch 353/500\n",
            "298/298 [==============================] - 1s 5ms/step - loss: 0.0560 - accuracy: 0.9775 - val_loss: 0.1033 - val_accuracy: 0.9665\n",
            "Epoch 354/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0583 - accuracy: 0.9763 - val_loss: 0.1025 - val_accuracy: 0.9665\n",
            "Epoch 355/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0581 - accuracy: 0.9769 - val_loss: 0.1017 - val_accuracy: 0.9661\n",
            "Epoch 356/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0563 - accuracy: 0.9754 - val_loss: 0.1054 - val_accuracy: 0.9670\n",
            "Epoch 357/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0581 - accuracy: 0.9774 - val_loss: 0.1057 - val_accuracy: 0.9652\n",
            "Epoch 358/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0595 - accuracy: 0.9758 - val_loss: 0.0984 - val_accuracy: 0.9661\n",
            "Epoch 359/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0574 - accuracy: 0.9765 - val_loss: 0.1038 - val_accuracy: 0.9652\n",
            "Epoch 360/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0600 - accuracy: 0.9772 - val_loss: 0.1005 - val_accuracy: 0.9665\n",
            "Epoch 361/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0535 - accuracy: 0.9780 - val_loss: 0.1008 - val_accuracy: 0.9670\n",
            "Epoch 362/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0624 - accuracy: 0.9765 - val_loss: 0.1021 - val_accuracy: 0.9643\n",
            "Epoch 363/500\n",
            "298/298 [==============================] - 1s 5ms/step - loss: 0.0549 - accuracy: 0.9777 - val_loss: 0.1022 - val_accuracy: 0.9670\n",
            "Epoch 364/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0596 - accuracy: 0.9778 - val_loss: 0.1038 - val_accuracy: 0.9665\n",
            "Epoch 365/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0576 - accuracy: 0.9772 - val_loss: 0.1041 - val_accuracy: 0.9661\n",
            "Epoch 366/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0572 - accuracy: 0.9774 - val_loss: 0.1038 - val_accuracy: 0.9656\n",
            "Epoch 367/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0565 - accuracy: 0.9767 - val_loss: 0.1029 - val_accuracy: 0.9661\n",
            "Epoch 368/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0586 - accuracy: 0.9776 - val_loss: 0.0994 - val_accuracy: 0.9670\n",
            "Epoch 369/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0568 - accuracy: 0.9758 - val_loss: 0.1007 - val_accuracy: 0.9647\n",
            "Epoch 370/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0592 - accuracy: 0.9757 - val_loss: 0.1003 - val_accuracy: 0.9665\n",
            "Epoch 371/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0567 - accuracy: 0.9778 - val_loss: 0.1041 - val_accuracy: 0.9692\n",
            "Epoch 372/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0566 - accuracy: 0.9763 - val_loss: 0.1042 - val_accuracy: 0.9638\n",
            "Epoch 373/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0562 - accuracy: 0.9778 - val_loss: 0.1026 - val_accuracy: 0.9661\n",
            "Epoch 374/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0564 - accuracy: 0.9777 - val_loss: 0.1018 - val_accuracy: 0.9665\n",
            "Epoch 375/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0570 - accuracy: 0.9778 - val_loss: 0.1024 - val_accuracy: 0.9643\n",
            "Epoch 376/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0564 - accuracy: 0.9768 - val_loss: 0.1045 - val_accuracy: 0.9674\n",
            "Epoch 377/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0546 - accuracy: 0.9788 - val_loss: 0.1043 - val_accuracy: 0.9679\n",
            "Epoch 378/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0567 - accuracy: 0.9766 - val_loss: 0.1024 - val_accuracy: 0.9661\n",
            "Epoch 379/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0547 - accuracy: 0.9788 - val_loss: 0.1050 - val_accuracy: 0.9688\n",
            "Epoch 380/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0566 - accuracy: 0.9776 - val_loss: 0.1039 - val_accuracy: 0.9652\n",
            "Epoch 381/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0566 - accuracy: 0.9765 - val_loss: 0.1024 - val_accuracy: 0.9652\n",
            "Epoch 382/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0560 - accuracy: 0.9778 - val_loss: 0.1042 - val_accuracy: 0.9674\n",
            "Epoch 383/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0585 - accuracy: 0.9773 - val_loss: 0.1036 - val_accuracy: 0.9643\n",
            "Epoch 384/500\n",
            "298/298 [==============================] - 1s 5ms/step - loss: 0.0585 - accuracy: 0.9774 - val_loss: 0.1032 - val_accuracy: 0.9665\n",
            "Epoch 385/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0574 - accuracy: 0.9771 - val_loss: 0.1038 - val_accuracy: 0.9661\n",
            "Epoch 386/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0570 - accuracy: 0.9775 - val_loss: 0.1059 - val_accuracy: 0.9661\n",
            "Epoch 387/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0565 - accuracy: 0.9761 - val_loss: 0.1050 - val_accuracy: 0.9665\n",
            "Epoch 388/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0576 - accuracy: 0.9759 - val_loss: 0.1042 - val_accuracy: 0.9656\n",
            "Epoch 389/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0560 - accuracy: 0.9780 - val_loss: 0.1025 - val_accuracy: 0.9661\n",
            "Epoch 390/500\n",
            "298/298 [==============================] - 1s 5ms/step - loss: 0.0567 - accuracy: 0.9771 - val_loss: 0.1043 - val_accuracy: 0.9643\n",
            "Epoch 391/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0542 - accuracy: 0.9783 - val_loss: 0.1075 - val_accuracy: 0.9661\n",
            "Epoch 392/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0531 - accuracy: 0.9806 - val_loss: 0.1044 - val_accuracy: 0.9661\n",
            "Epoch 393/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0594 - accuracy: 0.9764 - val_loss: 0.1025 - val_accuracy: 0.9656\n",
            "Epoch 394/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0512 - accuracy: 0.9795 - val_loss: 0.1028 - val_accuracy: 0.9656\n",
            "Epoch 395/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0600 - accuracy: 0.9745 - val_loss: 0.1001 - val_accuracy: 0.9665\n",
            "Epoch 396/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0551 - accuracy: 0.9776 - val_loss: 0.1014 - val_accuracy: 0.9643\n",
            "Epoch 397/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0558 - accuracy: 0.9759 - val_loss: 0.1039 - val_accuracy: 0.9656\n",
            "Epoch 398/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0503 - accuracy: 0.9815 - val_loss: 0.1038 - val_accuracy: 0.9656\n",
            "Epoch 399/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0585 - accuracy: 0.9761 - val_loss: 0.1001 - val_accuracy: 0.9656\n",
            "Epoch 400/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0559 - accuracy: 0.9774 - val_loss: 0.1038 - val_accuracy: 0.9665\n",
            "Epoch 401/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0557 - accuracy: 0.9796 - val_loss: 0.1014 - val_accuracy: 0.9656\n",
            "Epoch 402/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0570 - accuracy: 0.9775 - val_loss: 0.0995 - val_accuracy: 0.9647\n",
            "Epoch 403/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0561 - accuracy: 0.9782 - val_loss: 0.1034 - val_accuracy: 0.9643\n",
            "Epoch 404/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0573 - accuracy: 0.9764 - val_loss: 0.1018 - val_accuracy: 0.9656\n",
            "Epoch 405/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0544 - accuracy: 0.9782 - val_loss: 0.1012 - val_accuracy: 0.9652\n",
            "Epoch 406/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0532 - accuracy: 0.9784 - val_loss: 0.1014 - val_accuracy: 0.9665\n",
            "Epoch 407/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0554 - accuracy: 0.9780 - val_loss: 0.1027 - val_accuracy: 0.9661\n",
            "Epoch 408/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0567 - accuracy: 0.9760 - val_loss: 0.1005 - val_accuracy: 0.9656\n",
            "Epoch 409/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0560 - accuracy: 0.9765 - val_loss: 0.1037 - val_accuracy: 0.9652\n",
            "Epoch 410/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0596 - accuracy: 0.9752 - val_loss: 0.1012 - val_accuracy: 0.9665\n",
            "Epoch 411/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0509 - accuracy: 0.9808 - val_loss: 0.1023 - val_accuracy: 0.9647\n",
            "Epoch 412/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0540 - accuracy: 0.9785 - val_loss: 0.1033 - val_accuracy: 0.9679\n",
            "Epoch 413/500\n",
            "298/298 [==============================] - 1s 5ms/step - loss: 0.0540 - accuracy: 0.9790 - val_loss: 0.1053 - val_accuracy: 0.9665\n",
            "Epoch 414/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0518 - accuracy: 0.9794 - val_loss: 0.1034 - val_accuracy: 0.9665\n",
            "Epoch 415/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0558 - accuracy: 0.9751 - val_loss: 0.1056 - val_accuracy: 0.9656\n",
            "Epoch 416/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0549 - accuracy: 0.9786 - val_loss: 0.1046 - val_accuracy: 0.9661\n",
            "Epoch 417/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0585 - accuracy: 0.9765 - val_loss: 0.1067 - val_accuracy: 0.9652\n",
            "Epoch 418/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0551 - accuracy: 0.9769 - val_loss: 0.1069 - val_accuracy: 0.9665\n",
            "Epoch 419/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0586 - accuracy: 0.9766 - val_loss: 0.1057 - val_accuracy: 0.9656\n",
            "Epoch 420/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0568 - accuracy: 0.9761 - val_loss: 0.1056 - val_accuracy: 0.9656\n",
            "Epoch 421/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0530 - accuracy: 0.9781 - val_loss: 0.1097 - val_accuracy: 0.9665\n",
            "Epoch 422/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0537 - accuracy: 0.9778 - val_loss: 0.1073 - val_accuracy: 0.9652\n",
            "Epoch 423/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0525 - accuracy: 0.9787 - val_loss: 0.1013 - val_accuracy: 0.9661\n",
            "Epoch 424/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0580 - accuracy: 0.9757 - val_loss: 0.1051 - val_accuracy: 0.9656\n",
            "Epoch 425/500\n",
            "298/298 [==============================] - 1s 5ms/step - loss: 0.0568 - accuracy: 0.9766 - val_loss: 0.1038 - val_accuracy: 0.9629\n",
            "Epoch 426/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0540 - accuracy: 0.9798 - val_loss: 0.1042 - val_accuracy: 0.9674\n",
            "Epoch 427/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0520 - accuracy: 0.9789 - val_loss: 0.1082 - val_accuracy: 0.9679\n",
            "Epoch 428/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0555 - accuracy: 0.9783 - val_loss: 0.1074 - val_accuracy: 0.9656\n",
            "Epoch 429/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0589 - accuracy: 0.9764 - val_loss: 0.1020 - val_accuracy: 0.9665\n",
            "Epoch 430/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0514 - accuracy: 0.9789 - val_loss: 0.1044 - val_accuracy: 0.9665\n",
            "Epoch 431/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0549 - accuracy: 0.9771 - val_loss: 0.1032 - val_accuracy: 0.9661\n",
            "Epoch 432/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0580 - accuracy: 0.9773 - val_loss: 0.1013 - val_accuracy: 0.9679\n",
            "Epoch 433/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0533 - accuracy: 0.9784 - val_loss: 0.0968 - val_accuracy: 0.9647\n",
            "Epoch 434/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0540 - accuracy: 0.9798 - val_loss: 0.1052 - val_accuracy: 0.9683\n",
            "Epoch 435/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0546 - accuracy: 0.9787 - val_loss: 0.1004 - val_accuracy: 0.9674\n",
            "Epoch 436/500\n",
            "298/298 [==============================] - 1s 5ms/step - loss: 0.0546 - accuracy: 0.9776 - val_loss: 0.0999 - val_accuracy: 0.9661\n",
            "Epoch 437/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0555 - accuracy: 0.9791 - val_loss: 0.1001 - val_accuracy: 0.9670\n",
            "Epoch 438/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0519 - accuracy: 0.9804 - val_loss: 0.1031 - val_accuracy: 0.9670\n",
            "Epoch 439/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0533 - accuracy: 0.9787 - val_loss: 0.1016 - val_accuracy: 0.9674\n",
            "Epoch 440/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0563 - accuracy: 0.9773 - val_loss: 0.1008 - val_accuracy: 0.9643\n",
            "Epoch 441/500\n",
            "298/298 [==============================] - 1s 5ms/step - loss: 0.0559 - accuracy: 0.9783 - val_loss: 0.1040 - val_accuracy: 0.9661\n",
            "Epoch 442/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0567 - accuracy: 0.9777 - val_loss: 0.1025 - val_accuracy: 0.9665\n",
            "Epoch 443/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0548 - accuracy: 0.9780 - val_loss: 0.0991 - val_accuracy: 0.9629\n",
            "Epoch 444/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0534 - accuracy: 0.9784 - val_loss: 0.1050 - val_accuracy: 0.9665\n",
            "Epoch 445/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0495 - accuracy: 0.9802 - val_loss: 0.1015 - val_accuracy: 0.9643\n",
            "Epoch 446/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0579 - accuracy: 0.9771 - val_loss: 0.1031 - val_accuracy: 0.9656\n",
            "Epoch 447/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0520 - accuracy: 0.9801 - val_loss: 0.1035 - val_accuracy: 0.9652\n",
            "Epoch 448/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0535 - accuracy: 0.9793 - val_loss: 0.1070 - val_accuracy: 0.9652\n",
            "Epoch 449/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0538 - accuracy: 0.9782 - val_loss: 0.1042 - val_accuracy: 0.9661\n",
            "Epoch 450/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0512 - accuracy: 0.9798 - val_loss: 0.1032 - val_accuracy: 0.9652\n",
            "Epoch 451/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0513 - accuracy: 0.9796 - val_loss: 0.1045 - val_accuracy: 0.9652\n",
            "Epoch 452/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0525 - accuracy: 0.9776 - val_loss: 0.1043 - val_accuracy: 0.9652\n",
            "Epoch 453/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0520 - accuracy: 0.9785 - val_loss: 0.1029 - val_accuracy: 0.9665\n",
            "Epoch 454/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0543 - accuracy: 0.9790 - val_loss: 0.1035 - val_accuracy: 0.9683\n",
            "Epoch 455/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0529 - accuracy: 0.9788 - val_loss: 0.1042 - val_accuracy: 0.9679\n",
            "Epoch 456/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0560 - accuracy: 0.9769 - val_loss: 0.1079 - val_accuracy: 0.9647\n",
            "Epoch 457/500\n",
            "298/298 [==============================] - 1s 5ms/step - loss: 0.0510 - accuracy: 0.9795 - val_loss: 0.1053 - val_accuracy: 0.9656\n",
            "Epoch 458/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0536 - accuracy: 0.9779 - val_loss: 0.1023 - val_accuracy: 0.9643\n",
            "Epoch 459/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0564 - accuracy: 0.9782 - val_loss: 0.1023 - val_accuracy: 0.9652\n",
            "Epoch 460/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0516 - accuracy: 0.9808 - val_loss: 0.0995 - val_accuracy: 0.9652\n",
            "Epoch 461/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0509 - accuracy: 0.9784 - val_loss: 0.1044 - val_accuracy: 0.9656\n",
            "Epoch 462/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0520 - accuracy: 0.9784 - val_loss: 0.1005 - val_accuracy: 0.9643\n",
            "Epoch 463/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0507 - accuracy: 0.9797 - val_loss: 0.1024 - val_accuracy: 0.9674\n",
            "Epoch 464/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0568 - accuracy: 0.9779 - val_loss: 0.1040 - val_accuracy: 0.9656\n",
            "Epoch 465/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0519 - accuracy: 0.9782 - val_loss: 0.1010 - val_accuracy: 0.9647\n",
            "Epoch 466/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0532 - accuracy: 0.9797 - val_loss: 0.1028 - val_accuracy: 0.9665\n",
            "Epoch 467/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0567 - accuracy: 0.9783 - val_loss: 0.1021 - val_accuracy: 0.9647\n",
            "Epoch 468/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0542 - accuracy: 0.9797 - val_loss: 0.1010 - val_accuracy: 0.9643\n",
            "Epoch 469/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0556 - accuracy: 0.9780 - val_loss: 0.1034 - val_accuracy: 0.9661\n",
            "Epoch 470/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0527 - accuracy: 0.9789 - val_loss: 0.1026 - val_accuracy: 0.9634\n",
            "Epoch 471/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.0538 - accuracy: 0.9783 - val_loss: 0.1047 - val_accuracy: 0.9638\n",
            "Epoch 472/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0509 - accuracy: 0.9795 - val_loss: 0.1013 - val_accuracy: 0.9629\n",
            "Epoch 473/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0553 - accuracy: 0.9784 - val_loss: 0.0998 - val_accuracy: 0.9652\n",
            "Epoch 474/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0553 - accuracy: 0.9790 - val_loss: 0.0987 - val_accuracy: 0.9656\n",
            "Epoch 475/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0552 - accuracy: 0.9778 - val_loss: 0.1026 - val_accuracy: 0.9661\n",
            "Epoch 476/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0542 - accuracy: 0.9784 - val_loss: 0.0998 - val_accuracy: 0.9670\n",
            "Epoch 477/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0494 - accuracy: 0.9803 - val_loss: 0.1025 - val_accuracy: 0.9661\n",
            "Epoch 478/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0495 - accuracy: 0.9798 - val_loss: 0.1067 - val_accuracy: 0.9656\n",
            "Epoch 479/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0498 - accuracy: 0.9800 - val_loss: 0.1042 - val_accuracy: 0.9656\n",
            "Epoch 480/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0529 - accuracy: 0.9789 - val_loss: 0.0997 - val_accuracy: 0.9656\n",
            "Epoch 481/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0539 - accuracy: 0.9784 - val_loss: 0.1010 - val_accuracy: 0.9652\n",
            "Epoch 482/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0572 - accuracy: 0.9769 - val_loss: 0.1003 - val_accuracy: 0.9679\n",
            "Epoch 483/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0507 - accuracy: 0.9805 - val_loss: 0.0990 - val_accuracy: 0.9665\n",
            "Epoch 484/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0545 - accuracy: 0.9772 - val_loss: 0.1048 - val_accuracy: 0.9674\n",
            "Epoch 485/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0535 - accuracy: 0.9794 - val_loss: 0.1029 - val_accuracy: 0.9674\n",
            "Epoch 486/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.0511 - accuracy: 0.9799 - val_loss: 0.1038 - val_accuracy: 0.9674\n",
            "Epoch 487/500\n",
            "298/298 [==============================] - 1s 4ms/step - loss: 0.0517 - accuracy: 0.9786 - val_loss: 0.1019 - val_accuracy: 0.9665\n",
            "Epoch 488/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.0534 - accuracy: 0.9798 - val_loss: 0.1000 - val_accuracy: 0.9656\n",
            "Epoch 489/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0478 - accuracy: 0.9802 - val_loss: 0.1038 - val_accuracy: 0.9683\n",
            "Epoch 490/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0561 - accuracy: 0.9781 - val_loss: 0.1025 - val_accuracy: 0.9679\n",
            "Epoch 491/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0531 - accuracy: 0.9776 - val_loss: 0.1067 - val_accuracy: 0.9701\n",
            "Epoch 492/500\n",
            "298/298 [==============================] - 2s 6ms/step - loss: 0.0561 - accuracy: 0.9768 - val_loss: 0.1018 - val_accuracy: 0.9656\n",
            "Epoch 493/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0512 - accuracy: 0.9788 - val_loss: 0.0991 - val_accuracy: 0.9683\n",
            "Epoch 494/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0551 - accuracy: 0.9788 - val_loss: 0.1051 - val_accuracy: 0.9674\n",
            "Epoch 495/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0511 - accuracy: 0.9800 - val_loss: 0.1048 - val_accuracy: 0.9683\n",
            "Epoch 496/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0520 - accuracy: 0.9794 - val_loss: 0.1076 - val_accuracy: 0.9670\n",
            "Epoch 497/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0543 - accuracy: 0.9779 - val_loss: 0.1018 - val_accuracy: 0.9674\n",
            "Epoch 498/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0543 - accuracy: 0.9787 - val_loss: 0.1016 - val_accuracy: 0.9661\n",
            "Epoch 499/500\n",
            "298/298 [==============================] - 2s 5ms/step - loss: 0.0519 - accuracy: 0.9808 - val_loss: 0.1037 - val_accuracy: 0.9661\n",
            "Epoch 500/500\n",
            "298/298 [==============================] - 1s 5ms/step - loss: 0.0474 - accuracy: 0.9805 - val_loss: 0.1042 - val_accuracy: 0.9670\n",
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " simple_rnn_2 (SimpleRNN)    (None, 1, 64)             6080      \n",
            "                                                                 \n",
            " dropout_12 (Dropout)        (None, 1, 64)             0         \n",
            "                                                                 \n",
            " simple_rnn_3 (SimpleRNN)    (None, 32)                3104      \n",
            "                                                                 \n",
            " dropout_13 (Dropout)        (None, 32)                0         \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 9,217\n",
            "Trainable params: 9,217\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Define the model architecture\n",
        "model_rnn = Sequential()\n",
        "model_rnn.add(SimpleRNN(64, input_shape=(train_data_3d.shape[1], train_data_3d.shape[2]), return_sequences=True))\n",
        "model_rnn.add(Dropout(0.2))\n",
        "model_rnn.add(SimpleRNN(32))\n",
        "model_rnn.add(Dropout(0.2))\n",
        "model_rnn.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "model_rnn.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model_rnn.fit(train_data_3d, train_labels, validation_data=(test_data_3d, test_labels), epochs=500, batch_size=33)\n",
        "model_rnn.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0V-vHRawGkx"
      },
      "source": [
        "### 4. LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2S96rNT2wGkx",
        "outputId": "c33504e4-e270-4edb-a5c9-bf99f688f648"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "298/298 [==============================] - 10s 12ms/step - loss: 0.3019 - accuracy: 0.8958 - val_loss: 0.1997 - val_accuracy: 0.9236\n",
            "Epoch 2/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.1885 - accuracy: 0.9298 - val_loss: 0.1858 - val_accuracy: 0.9281\n",
            "Epoch 3/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.1706 - accuracy: 0.9329 - val_loss: 0.1809 - val_accuracy: 0.9231\n",
            "Epoch 4/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.1616 - accuracy: 0.9344 - val_loss: 0.1684 - val_accuracy: 0.9331\n",
            "Epoch 5/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.1527 - accuracy: 0.9379 - val_loss: 0.1709 - val_accuracy: 0.9263\n",
            "Epoch 6/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.1442 - accuracy: 0.9406 - val_loss: 0.1508 - val_accuracy: 0.9349\n",
            "Epoch 7/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.1351 - accuracy: 0.9457 - val_loss: 0.1462 - val_accuracy: 0.9358\n",
            "Epoch 8/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.1275 - accuracy: 0.9472 - val_loss: 0.1386 - val_accuracy: 0.9435\n",
            "Epoch 9/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.1217 - accuracy: 0.9487 - val_loss: 0.1448 - val_accuracy: 0.9444\n",
            "Epoch 10/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.1196 - accuracy: 0.9502 - val_loss: 0.1282 - val_accuracy: 0.9480\n",
            "Epoch 11/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.1147 - accuracy: 0.9497 - val_loss: 0.1226 - val_accuracy: 0.9521\n",
            "Epoch 12/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.1075 - accuracy: 0.9544 - val_loss: 0.1237 - val_accuracy: 0.9498\n",
            "Epoch 13/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.1031 - accuracy: 0.9568 - val_loss: 0.1224 - val_accuracy: 0.9521\n",
            "Epoch 14/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.1018 - accuracy: 0.9554 - val_loss: 0.1218 - val_accuracy: 0.9525\n",
            "Epoch 15/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0977 - accuracy: 0.9575 - val_loss: 0.1198 - val_accuracy: 0.9566\n",
            "Epoch 16/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0938 - accuracy: 0.9585 - val_loss: 0.1173 - val_accuracy: 0.9570\n",
            "Epoch 17/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0916 - accuracy: 0.9597 - val_loss: 0.1187 - val_accuracy: 0.9552\n",
            "Epoch 18/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0893 - accuracy: 0.9613 - val_loss: 0.1168 - val_accuracy: 0.9539\n",
            "Epoch 19/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0867 - accuracy: 0.9629 - val_loss: 0.1111 - val_accuracy: 0.9570\n",
            "Epoch 20/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0835 - accuracy: 0.9648 - val_loss: 0.1086 - val_accuracy: 0.9593\n",
            "Epoch 21/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0832 - accuracy: 0.9647 - val_loss: 0.1095 - val_accuracy: 0.9575\n",
            "Epoch 22/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0816 - accuracy: 0.9655 - val_loss: 0.1084 - val_accuracy: 0.9579\n",
            "Epoch 23/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0813 - accuracy: 0.9656 - val_loss: 0.1057 - val_accuracy: 0.9607\n",
            "Epoch 24/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0777 - accuracy: 0.9666 - val_loss: 0.1104 - val_accuracy: 0.9602\n",
            "Epoch 25/500\n",
            "298/298 [==============================] - 3s 8ms/step - loss: 0.0778 - accuracy: 0.9659 - val_loss: 0.1057 - val_accuracy: 0.9625\n",
            "Epoch 26/500\n",
            "298/298 [==============================] - 3s 8ms/step - loss: 0.0739 - accuracy: 0.9687 - val_loss: 0.1064 - val_accuracy: 0.9625\n",
            "Epoch 27/500\n",
            "298/298 [==============================] - 3s 8ms/step - loss: 0.0747 - accuracy: 0.9687 - val_loss: 0.1052 - val_accuracy: 0.9575\n",
            "Epoch 28/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0763 - accuracy: 0.9674 - val_loss: 0.1039 - val_accuracy: 0.9638\n",
            "Epoch 29/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0710 - accuracy: 0.9705 - val_loss: 0.1090 - val_accuracy: 0.9602\n",
            "Epoch 30/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0711 - accuracy: 0.9694 - val_loss: 0.1056 - val_accuracy: 0.9629\n",
            "Epoch 31/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0683 - accuracy: 0.9701 - val_loss: 0.1019 - val_accuracy: 0.9602\n",
            "Epoch 32/500\n",
            "298/298 [==============================] - 3s 8ms/step - loss: 0.0697 - accuracy: 0.9697 - val_loss: 0.1049 - val_accuracy: 0.9593\n",
            "Epoch 33/500\n",
            "298/298 [==============================] - 3s 9ms/step - loss: 0.0678 - accuracy: 0.9697 - val_loss: 0.1064 - val_accuracy: 0.9602\n",
            "Epoch 34/500\n",
            "298/298 [==============================] - 3s 8ms/step - loss: 0.0645 - accuracy: 0.9725 - val_loss: 0.1053 - val_accuracy: 0.9607\n",
            "Epoch 35/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0677 - accuracy: 0.9706 - val_loss: 0.1052 - val_accuracy: 0.9607\n",
            "Epoch 36/500\n",
            "298/298 [==============================] - 2s 7ms/step - loss: 0.0627 - accuracy: 0.9734 - val_loss: 0.1127 - val_accuracy: 0.9588\n",
            "Epoch 37/500\n",
            "298/298 [==============================] - 3s 11ms/step - loss: 0.0626 - accuracy: 0.9724 - val_loss: 0.1035 - val_accuracy: 0.9629\n",
            "Epoch 38/500\n",
            "298/298 [==============================] - 3s 10ms/step - loss: 0.0633 - accuracy: 0.9729 - val_loss: 0.1121 - val_accuracy: 0.9611\n",
            "Epoch 39/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0625 - accuracy: 0.9714 - val_loss: 0.1035 - val_accuracy: 0.9634\n",
            "Epoch 40/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0632 - accuracy: 0.9715 - val_loss: 0.1047 - val_accuracy: 0.9616\n",
            "Epoch 41/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0613 - accuracy: 0.9738 - val_loss: 0.1090 - val_accuracy: 0.9602\n",
            "Epoch 42/500\n",
            "298/298 [==============================] - 3s 9ms/step - loss: 0.0596 - accuracy: 0.9750 - val_loss: 0.1061 - val_accuracy: 0.9616\n",
            "Epoch 43/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0593 - accuracy: 0.9742 - val_loss: 0.1083 - val_accuracy: 0.9620\n",
            "Epoch 44/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0572 - accuracy: 0.9764 - val_loss: 0.1105 - val_accuracy: 0.9557\n",
            "Epoch 45/500\n",
            "298/298 [==============================] - 2s 7ms/step - loss: 0.0593 - accuracy: 0.9730 - val_loss: 0.1028 - val_accuracy: 0.9643\n",
            "Epoch 46/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0574 - accuracy: 0.9755 - val_loss: 0.1107 - val_accuracy: 0.9602\n",
            "Epoch 47/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0565 - accuracy: 0.9751 - val_loss: 0.1068 - val_accuracy: 0.9625\n",
            "Epoch 48/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0590 - accuracy: 0.9746 - val_loss: 0.1060 - val_accuracy: 0.9652\n",
            "Epoch 49/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0567 - accuracy: 0.9760 - val_loss: 0.1072 - val_accuracy: 0.9625\n",
            "Epoch 50/500\n",
            "298/298 [==============================] - 2s 7ms/step - loss: 0.0529 - accuracy: 0.9764 - val_loss: 0.1071 - val_accuracy: 0.9629\n",
            "Epoch 51/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0543 - accuracy: 0.9759 - val_loss: 0.1089 - val_accuracy: 0.9611\n",
            "Epoch 52/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0543 - accuracy: 0.9755 - val_loss: 0.1116 - val_accuracy: 0.9625\n",
            "Epoch 53/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0520 - accuracy: 0.9767 - val_loss: 0.1055 - val_accuracy: 0.9647\n",
            "Epoch 54/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0559 - accuracy: 0.9744 - val_loss: 0.1056 - val_accuracy: 0.9679\n",
            "Epoch 55/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0531 - accuracy: 0.9768 - val_loss: 0.1064 - val_accuracy: 0.9643\n",
            "Epoch 56/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0529 - accuracy: 0.9771 - val_loss: 0.1048 - val_accuracy: 0.9652\n",
            "Epoch 57/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0518 - accuracy: 0.9768 - val_loss: 0.1054 - val_accuracy: 0.9652\n",
            "Epoch 58/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0527 - accuracy: 0.9768 - val_loss: 0.1096 - val_accuracy: 0.9620\n",
            "Epoch 59/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0530 - accuracy: 0.9760 - val_loss: 0.1115 - val_accuracy: 0.9629\n",
            "Epoch 60/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0503 - accuracy: 0.9784 - val_loss: 0.1146 - val_accuracy: 0.9629\n",
            "Epoch 61/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0506 - accuracy: 0.9783 - val_loss: 0.1101 - val_accuracy: 0.9665\n",
            "Epoch 62/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0521 - accuracy: 0.9769 - val_loss: 0.1059 - val_accuracy: 0.9616\n",
            "Epoch 63/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0496 - accuracy: 0.9777 - val_loss: 0.1072 - val_accuracy: 0.9661\n",
            "Epoch 64/500\n",
            "298/298 [==============================] - 2s 7ms/step - loss: 0.0499 - accuracy: 0.9780 - val_loss: 0.1006 - val_accuracy: 0.9652\n",
            "Epoch 65/500\n",
            "298/298 [==============================] - 2s 7ms/step - loss: 0.0481 - accuracy: 0.9778 - val_loss: 0.1096 - val_accuracy: 0.9661\n",
            "Epoch 66/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0500 - accuracy: 0.9773 - val_loss: 0.1010 - val_accuracy: 0.9674\n",
            "Epoch 67/500\n",
            "298/298 [==============================] - 2s 7ms/step - loss: 0.0484 - accuracy: 0.9803 - val_loss: 0.1105 - val_accuracy: 0.9652\n",
            "Epoch 68/500\n",
            "298/298 [==============================] - 2s 7ms/step - loss: 0.0498 - accuracy: 0.9778 - val_loss: 0.1044 - val_accuracy: 0.9665\n",
            "Epoch 69/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0477 - accuracy: 0.9798 - val_loss: 0.1069 - val_accuracy: 0.9683\n",
            "Epoch 70/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0502 - accuracy: 0.9772 - val_loss: 0.1076 - val_accuracy: 0.9683\n",
            "Epoch 71/500\n",
            "298/298 [==============================] - 2s 7ms/step - loss: 0.0482 - accuracy: 0.9793 - val_loss: 0.1051 - val_accuracy: 0.9643\n",
            "Epoch 72/500\n",
            "298/298 [==============================] - 2s 7ms/step - loss: 0.0477 - accuracy: 0.9785 - val_loss: 0.1078 - val_accuracy: 0.9665\n",
            "Epoch 73/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0467 - accuracy: 0.9794 - val_loss: 0.1124 - val_accuracy: 0.9665\n",
            "Epoch 74/500\n",
            "298/298 [==============================] - 2s 7ms/step - loss: 0.0455 - accuracy: 0.9796 - val_loss: 0.1117 - val_accuracy: 0.9643\n",
            "Epoch 75/500\n",
            "298/298 [==============================] - 2s 7ms/step - loss: 0.0467 - accuracy: 0.9798 - val_loss: 0.1146 - val_accuracy: 0.9647\n",
            "Epoch 76/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0448 - accuracy: 0.9807 - val_loss: 0.1241 - val_accuracy: 0.9629\n",
            "Epoch 77/500\n",
            "298/298 [==============================] - 3s 9ms/step - loss: 0.0449 - accuracy: 0.9801 - val_loss: 0.1134 - val_accuracy: 0.9674\n",
            "Epoch 78/500\n",
            "298/298 [==============================] - 3s 9ms/step - loss: 0.0446 - accuracy: 0.9802 - val_loss: 0.1164 - val_accuracy: 0.9652\n",
            "Epoch 79/500\n",
            "298/298 [==============================] - 3s 8ms/step - loss: 0.0478 - accuracy: 0.9778 - val_loss: 0.1087 - val_accuracy: 0.9670\n",
            "Epoch 80/500\n",
            "298/298 [==============================] - 3s 9ms/step - loss: 0.0469 - accuracy: 0.9800 - val_loss: 0.1141 - val_accuracy: 0.9697\n",
            "Epoch 81/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0468 - accuracy: 0.9803 - val_loss: 0.1074 - val_accuracy: 0.9679\n",
            "Epoch 82/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0471 - accuracy: 0.9787 - val_loss: 0.1054 - val_accuracy: 0.9688\n",
            "Epoch 83/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0429 - accuracy: 0.9796 - val_loss: 0.1103 - val_accuracy: 0.9674\n",
            "Epoch 84/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0454 - accuracy: 0.9809 - val_loss: 0.1184 - val_accuracy: 0.9670\n",
            "Epoch 85/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0436 - accuracy: 0.9805 - val_loss: 0.1144 - val_accuracy: 0.9652\n",
            "Epoch 86/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0436 - accuracy: 0.9804 - val_loss: 0.1095 - val_accuracy: 0.9674\n",
            "Epoch 87/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0433 - accuracy: 0.9806 - val_loss: 0.1130 - val_accuracy: 0.9692\n",
            "Epoch 88/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0426 - accuracy: 0.9807 - val_loss: 0.1115 - val_accuracy: 0.9661\n",
            "Epoch 89/500\n",
            "298/298 [==============================] - 3s 8ms/step - loss: 0.0432 - accuracy: 0.9804 - val_loss: 0.1099 - val_accuracy: 0.9683\n",
            "Epoch 90/500\n",
            "298/298 [==============================] - 3s 9ms/step - loss: 0.0461 - accuracy: 0.9802 - val_loss: 0.1061 - val_accuracy: 0.9674\n",
            "Epoch 91/500\n",
            "298/298 [==============================] - 3s 9ms/step - loss: 0.0430 - accuracy: 0.9812 - val_loss: 0.1120 - val_accuracy: 0.9670\n",
            "Epoch 92/500\n",
            "298/298 [==============================] - 3s 9ms/step - loss: 0.0422 - accuracy: 0.9812 - val_loss: 0.1157 - val_accuracy: 0.9688\n",
            "Epoch 93/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0446 - accuracy: 0.9812 - val_loss: 0.1065 - val_accuracy: 0.9674\n",
            "Epoch 94/500\n",
            "298/298 [==============================] - 3s 9ms/step - loss: 0.0447 - accuracy: 0.9794 - val_loss: 0.1090 - val_accuracy: 0.9697\n",
            "Epoch 95/500\n",
            "298/298 [==============================] - 3s 9ms/step - loss: 0.0434 - accuracy: 0.9808 - val_loss: 0.1034 - val_accuracy: 0.9711\n",
            "Epoch 96/500\n",
            "298/298 [==============================] - 3s 9ms/step - loss: 0.0384 - accuracy: 0.9830 - val_loss: 0.1149 - val_accuracy: 0.9661\n",
            "Epoch 97/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0421 - accuracy: 0.9820 - val_loss: 0.1117 - val_accuracy: 0.9692\n",
            "Epoch 98/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0387 - accuracy: 0.9823 - val_loss: 0.1164 - val_accuracy: 0.9724\n",
            "Epoch 99/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0456 - accuracy: 0.9795 - val_loss: 0.1126 - val_accuracy: 0.9670\n",
            "Epoch 100/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0388 - accuracy: 0.9823 - val_loss: 0.1130 - val_accuracy: 0.9688\n",
            "Epoch 101/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0411 - accuracy: 0.9809 - val_loss: 0.1092 - val_accuracy: 0.9720\n",
            "Epoch 102/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0433 - accuracy: 0.9810 - val_loss: 0.1131 - val_accuracy: 0.9652\n",
            "Epoch 103/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0376 - accuracy: 0.9830 - val_loss: 0.1115 - val_accuracy: 0.9692\n",
            "Epoch 104/500\n",
            "298/298 [==============================] - 3s 9ms/step - loss: 0.0424 - accuracy: 0.9806 - val_loss: 0.1066 - val_accuracy: 0.9720\n",
            "Epoch 105/500\n",
            "298/298 [==============================] - 3s 9ms/step - loss: 0.0396 - accuracy: 0.9826 - val_loss: 0.1027 - val_accuracy: 0.9724\n",
            "Epoch 106/500\n",
            "298/298 [==============================] - 3s 9ms/step - loss: 0.0411 - accuracy: 0.9809 - val_loss: 0.1200 - val_accuracy: 0.9674\n",
            "Epoch 107/500\n",
            "298/298 [==============================] - 3s 9ms/step - loss: 0.0402 - accuracy: 0.9819 - val_loss: 0.1168 - val_accuracy: 0.9679\n",
            "Epoch 108/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0403 - accuracy: 0.9817 - val_loss: 0.1068 - val_accuracy: 0.9729\n",
            "Epoch 109/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0402 - accuracy: 0.9808 - val_loss: 0.1116 - val_accuracy: 0.9674\n",
            "Epoch 110/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0361 - accuracy: 0.9834 - val_loss: 0.1118 - val_accuracy: 0.9701\n",
            "Epoch 111/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0382 - accuracy: 0.9824 - val_loss: 0.1140 - val_accuracy: 0.9688\n",
            "Epoch 112/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0389 - accuracy: 0.9820 - val_loss: 0.1186 - val_accuracy: 0.9679\n",
            "Epoch 113/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0380 - accuracy: 0.9835 - val_loss: 0.1223 - val_accuracy: 0.9679\n",
            "Epoch 114/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0376 - accuracy: 0.9834 - val_loss: 0.1299 - val_accuracy: 0.9652\n",
            "Epoch 115/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0374 - accuracy: 0.9820 - val_loss: 0.1151 - val_accuracy: 0.9665\n",
            "Epoch 116/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0393 - accuracy: 0.9810 - val_loss: 0.1202 - val_accuracy: 0.9701\n",
            "Epoch 117/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0382 - accuracy: 0.9824 - val_loss: 0.1167 - val_accuracy: 0.9692\n",
            "Epoch 118/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0392 - accuracy: 0.9818 - val_loss: 0.1088 - val_accuracy: 0.9706\n",
            "Epoch 119/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0385 - accuracy: 0.9823 - val_loss: 0.1144 - val_accuracy: 0.9661\n",
            "Epoch 120/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0392 - accuracy: 0.9815 - val_loss: 0.1196 - val_accuracy: 0.9688\n",
            "Epoch 121/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0393 - accuracy: 0.9813 - val_loss: 0.1224 - val_accuracy: 0.9665\n",
            "Epoch 122/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0387 - accuracy: 0.9835 - val_loss: 0.1167 - val_accuracy: 0.9701\n",
            "Epoch 123/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0366 - accuracy: 0.9827 - val_loss: 0.1115 - val_accuracy: 0.9720\n",
            "Epoch 124/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0381 - accuracy: 0.9825 - val_loss: 0.1133 - val_accuracy: 0.9724\n",
            "Epoch 125/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0357 - accuracy: 0.9840 - val_loss: 0.1133 - val_accuracy: 0.9683\n",
            "Epoch 126/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0363 - accuracy: 0.9831 - val_loss: 0.1176 - val_accuracy: 0.9661\n",
            "Epoch 127/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0386 - accuracy: 0.9819 - val_loss: 0.1188 - val_accuracy: 0.9688\n",
            "Epoch 128/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0373 - accuracy: 0.9829 - val_loss: 0.1154 - val_accuracy: 0.9701\n",
            "Epoch 129/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0373 - accuracy: 0.9830 - val_loss: 0.1146 - val_accuracy: 0.9701\n",
            "Epoch 130/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0380 - accuracy: 0.9822 - val_loss: 0.1192 - val_accuracy: 0.9697\n",
            "Epoch 131/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0360 - accuracy: 0.9835 - val_loss: 0.1208 - val_accuracy: 0.9683\n",
            "Epoch 132/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0368 - accuracy: 0.9829 - val_loss: 0.1176 - val_accuracy: 0.9697\n",
            "Epoch 133/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0369 - accuracy: 0.9814 - val_loss: 0.1256 - val_accuracy: 0.9706\n",
            "Epoch 134/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0373 - accuracy: 0.9818 - val_loss: 0.1240 - val_accuracy: 0.9706\n",
            "Epoch 135/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0380 - accuracy: 0.9824 - val_loss: 0.1145 - val_accuracy: 0.9688\n",
            "Epoch 136/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0346 - accuracy: 0.9839 - val_loss: 0.1170 - val_accuracy: 0.9701\n",
            "Epoch 137/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0361 - accuracy: 0.9847 - val_loss: 0.1192 - val_accuracy: 0.9715\n",
            "Epoch 138/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0378 - accuracy: 0.9833 - val_loss: 0.1154 - val_accuracy: 0.9720\n",
            "Epoch 139/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0355 - accuracy: 0.9827 - val_loss: 0.1185 - val_accuracy: 0.9692\n",
            "Epoch 140/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0347 - accuracy: 0.9843 - val_loss: 0.1245 - val_accuracy: 0.9701\n",
            "Epoch 141/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0392 - accuracy: 0.9817 - val_loss: 0.1169 - val_accuracy: 0.9670\n",
            "Epoch 142/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0363 - accuracy: 0.9837 - val_loss: 0.1233 - val_accuracy: 0.9697\n",
            "Epoch 143/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0344 - accuracy: 0.9845 - val_loss: 0.1183 - val_accuracy: 0.9733\n",
            "Epoch 144/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0344 - accuracy: 0.9842 - val_loss: 0.1109 - val_accuracy: 0.9729\n",
            "Epoch 145/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0352 - accuracy: 0.9837 - val_loss: 0.1198 - val_accuracy: 0.9683\n",
            "Epoch 146/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0331 - accuracy: 0.9843 - val_loss: 0.1222 - val_accuracy: 0.9706\n",
            "Epoch 147/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0358 - accuracy: 0.9833 - val_loss: 0.1148 - val_accuracy: 0.9729\n",
            "Epoch 148/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0338 - accuracy: 0.9843 - val_loss: 0.1261 - val_accuracy: 0.9706\n",
            "Epoch 149/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0360 - accuracy: 0.9832 - val_loss: 0.1160 - val_accuracy: 0.9683\n",
            "Epoch 150/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0328 - accuracy: 0.9841 - val_loss: 0.1172 - val_accuracy: 0.9729\n",
            "Epoch 151/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0340 - accuracy: 0.9842 - val_loss: 0.1198 - val_accuracy: 0.9701\n",
            "Epoch 152/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0344 - accuracy: 0.9843 - val_loss: 0.1302 - val_accuracy: 0.9692\n",
            "Epoch 153/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0330 - accuracy: 0.9840 - val_loss: 0.1265 - val_accuracy: 0.9733\n",
            "Epoch 154/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0337 - accuracy: 0.9844 - val_loss: 0.1317 - val_accuracy: 0.9692\n",
            "Epoch 155/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0347 - accuracy: 0.9832 - val_loss: 0.1242 - val_accuracy: 0.9692\n",
            "Epoch 156/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0375 - accuracy: 0.9825 - val_loss: 0.1216 - val_accuracy: 0.9706\n",
            "Epoch 157/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0342 - accuracy: 0.9848 - val_loss: 0.1246 - val_accuracy: 0.9688\n",
            "Epoch 158/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0346 - accuracy: 0.9841 - val_loss: 0.1242 - val_accuracy: 0.9634\n",
            "Epoch 159/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0334 - accuracy: 0.9834 - val_loss: 0.1233 - val_accuracy: 0.9701\n",
            "Epoch 160/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0355 - accuracy: 0.9826 - val_loss: 0.1216 - val_accuracy: 0.9715\n",
            "Epoch 161/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0323 - accuracy: 0.9840 - val_loss: 0.1251 - val_accuracy: 0.9729\n",
            "Epoch 162/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0325 - accuracy: 0.9859 - val_loss: 0.1267 - val_accuracy: 0.9701\n",
            "Epoch 163/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0347 - accuracy: 0.9841 - val_loss: 0.1270 - val_accuracy: 0.9711\n",
            "Epoch 164/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0333 - accuracy: 0.9846 - val_loss: 0.1302 - val_accuracy: 0.9720\n",
            "Epoch 165/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0319 - accuracy: 0.9851 - val_loss: 0.1375 - val_accuracy: 0.9738\n",
            "Epoch 166/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0326 - accuracy: 0.9847 - val_loss: 0.1297 - val_accuracy: 0.9738\n",
            "Epoch 167/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0326 - accuracy: 0.9850 - val_loss: 0.1369 - val_accuracy: 0.9706\n",
            "Epoch 168/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0324 - accuracy: 0.9850 - val_loss: 0.1347 - val_accuracy: 0.9697\n",
            "Epoch 169/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0331 - accuracy: 0.9844 - val_loss: 0.1347 - val_accuracy: 0.9706\n",
            "Epoch 170/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0337 - accuracy: 0.9847 - val_loss: 0.1312 - val_accuracy: 0.9679\n",
            "Epoch 171/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0331 - accuracy: 0.9845 - val_loss: 0.1237 - val_accuracy: 0.9701\n",
            "Epoch 172/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0304 - accuracy: 0.9863 - val_loss: 0.1363 - val_accuracy: 0.9656\n",
            "Epoch 173/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0333 - accuracy: 0.9826 - val_loss: 0.1319 - val_accuracy: 0.9674\n",
            "Epoch 174/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0339 - accuracy: 0.9846 - val_loss: 0.1276 - val_accuracy: 0.9688\n",
            "Epoch 175/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0318 - accuracy: 0.9848 - val_loss: 0.1334 - val_accuracy: 0.9692\n",
            "Epoch 176/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0328 - accuracy: 0.9842 - val_loss: 0.1231 - val_accuracy: 0.9697\n",
            "Epoch 177/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0330 - accuracy: 0.9838 - val_loss: 0.1331 - val_accuracy: 0.9688\n",
            "Epoch 178/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0308 - accuracy: 0.9854 - val_loss: 0.1260 - val_accuracy: 0.9665\n",
            "Epoch 179/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0317 - accuracy: 0.9841 - val_loss: 0.1294 - val_accuracy: 0.9715\n",
            "Epoch 180/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0303 - accuracy: 0.9851 - val_loss: 0.1295 - val_accuracy: 0.9692\n",
            "Epoch 181/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0315 - accuracy: 0.9858 - val_loss: 0.1412 - val_accuracy: 0.9701\n",
            "Epoch 182/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0310 - accuracy: 0.9856 - val_loss: 0.1304 - val_accuracy: 0.9697\n",
            "Epoch 183/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0320 - accuracy: 0.9857 - val_loss: 0.1289 - val_accuracy: 0.9720\n",
            "Epoch 184/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0300 - accuracy: 0.9843 - val_loss: 0.1318 - val_accuracy: 0.9697\n",
            "Epoch 185/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0311 - accuracy: 0.9860 - val_loss: 0.1337 - val_accuracy: 0.9697\n",
            "Epoch 186/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0315 - accuracy: 0.9851 - val_loss: 0.1307 - val_accuracy: 0.9683\n",
            "Epoch 187/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0321 - accuracy: 0.9848 - val_loss: 0.1290 - val_accuracy: 0.9724\n",
            "Epoch 188/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0310 - accuracy: 0.9854 - val_loss: 0.1242 - val_accuracy: 0.9670\n",
            "Epoch 189/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0296 - accuracy: 0.9854 - val_loss: 0.1284 - val_accuracy: 0.9701\n",
            "Epoch 190/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0313 - accuracy: 0.9852 - val_loss: 0.1316 - val_accuracy: 0.9697\n",
            "Epoch 191/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0301 - accuracy: 0.9854 - val_loss: 0.1288 - val_accuracy: 0.9665\n",
            "Epoch 192/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0283 - accuracy: 0.9868 - val_loss: 0.1315 - val_accuracy: 0.9688\n",
            "Epoch 193/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0328 - accuracy: 0.9844 - val_loss: 0.1211 - val_accuracy: 0.9706\n",
            "Epoch 194/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0312 - accuracy: 0.9849 - val_loss: 0.1304 - val_accuracy: 0.9729\n",
            "Epoch 195/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0282 - accuracy: 0.9866 - val_loss: 0.1313 - val_accuracy: 0.9715\n",
            "Epoch 196/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0340 - accuracy: 0.9856 - val_loss: 0.1304 - val_accuracy: 0.9711\n",
            "Epoch 197/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0309 - accuracy: 0.9855 - val_loss: 0.1285 - val_accuracy: 0.9638\n",
            "Epoch 198/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0317 - accuracy: 0.9851 - val_loss: 0.1226 - val_accuracy: 0.9683\n",
            "Epoch 199/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0313 - accuracy: 0.9843 - val_loss: 0.1214 - val_accuracy: 0.9683\n",
            "Epoch 200/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0305 - accuracy: 0.9854 - val_loss: 0.1217 - val_accuracy: 0.9697\n",
            "Epoch 201/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0299 - accuracy: 0.9851 - val_loss: 0.1307 - val_accuracy: 0.9724\n",
            "Epoch 202/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0283 - accuracy: 0.9853 - val_loss: 0.1322 - val_accuracy: 0.9674\n",
            "Epoch 203/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0288 - accuracy: 0.9845 - val_loss: 0.1361 - val_accuracy: 0.9697\n",
            "Epoch 204/500\n",
            "298/298 [==============================] - 3s 9ms/step - loss: 0.0289 - accuracy: 0.9860 - val_loss: 0.1343 - val_accuracy: 0.9697\n",
            "Epoch 205/500\n",
            "298/298 [==============================] - 3s 9ms/step - loss: 0.0296 - accuracy: 0.9858 - val_loss: 0.1330 - val_accuracy: 0.9670\n",
            "Epoch 206/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0327 - accuracy: 0.9843 - val_loss: 0.1280 - val_accuracy: 0.9692\n",
            "Epoch 207/500\n",
            "298/298 [==============================] - 3s 9ms/step - loss: 0.0304 - accuracy: 0.9857 - val_loss: 0.1377 - val_accuracy: 0.9674\n",
            "Epoch 208/500\n",
            "298/298 [==============================] - 3s 8ms/step - loss: 0.0314 - accuracy: 0.9852 - val_loss: 0.1465 - val_accuracy: 0.9706\n",
            "Epoch 209/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0316 - accuracy: 0.9850 - val_loss: 0.1266 - val_accuracy: 0.9701\n",
            "Epoch 210/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0280 - accuracy: 0.9869 - val_loss: 0.1328 - val_accuracy: 0.9724\n",
            "Epoch 211/500\n",
            "298/298 [==============================] - 3s 9ms/step - loss: 0.0312 - accuracy: 0.9864 - val_loss: 0.1354 - val_accuracy: 0.9706\n",
            "Epoch 212/500\n",
            "298/298 [==============================] - 3s 9ms/step - loss: 0.0288 - accuracy: 0.9853 - val_loss: 0.1361 - val_accuracy: 0.9665\n",
            "Epoch 213/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0297 - accuracy: 0.9858 - val_loss: 0.1408 - val_accuracy: 0.9679\n",
            "Epoch 214/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0302 - accuracy: 0.9867 - val_loss: 0.1368 - val_accuracy: 0.9679\n",
            "Epoch 215/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0303 - accuracy: 0.9853 - val_loss: 0.1453 - val_accuracy: 0.9697\n",
            "Epoch 216/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0301 - accuracy: 0.9852 - val_loss: 0.1365 - val_accuracy: 0.9706\n",
            "Epoch 217/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0294 - accuracy: 0.9856 - val_loss: 0.1349 - val_accuracy: 0.9706\n",
            "Epoch 218/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0327 - accuracy: 0.9839 - val_loss: 0.1293 - val_accuracy: 0.9688\n",
            "Epoch 219/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0287 - accuracy: 0.9850 - val_loss: 0.1266 - val_accuracy: 0.9711\n",
            "Epoch 220/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0278 - accuracy: 0.9860 - val_loss: 0.1281 - val_accuracy: 0.9692\n",
            "Epoch 221/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0323 - accuracy: 0.9844 - val_loss: 0.1236 - val_accuracy: 0.9742\n",
            "Epoch 222/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0286 - accuracy: 0.9862 - val_loss: 0.1299 - val_accuracy: 0.9733\n",
            "Epoch 223/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0316 - accuracy: 0.9842 - val_loss: 0.1324 - val_accuracy: 0.9688\n",
            "Epoch 224/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0301 - accuracy: 0.9848 - val_loss: 0.1246 - val_accuracy: 0.9656\n",
            "Epoch 225/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0288 - accuracy: 0.9851 - val_loss: 0.1340 - val_accuracy: 0.9692\n",
            "Epoch 226/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0268 - accuracy: 0.9856 - val_loss: 0.1351 - val_accuracy: 0.9706\n",
            "Epoch 227/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0280 - accuracy: 0.9863 - val_loss: 0.1387 - val_accuracy: 0.9692\n",
            "Epoch 228/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0323 - accuracy: 0.9857 - val_loss: 0.1405 - val_accuracy: 0.9697\n",
            "Epoch 229/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0293 - accuracy: 0.9863 - val_loss: 0.1393 - val_accuracy: 0.9665\n",
            "Epoch 230/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0287 - accuracy: 0.9856 - val_loss: 0.1395 - val_accuracy: 0.9683\n",
            "Epoch 231/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0295 - accuracy: 0.9867 - val_loss: 0.1377 - val_accuracy: 0.9679\n",
            "Epoch 232/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0293 - accuracy: 0.9859 - val_loss: 0.1330 - val_accuracy: 0.9701\n",
            "Epoch 233/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0283 - accuracy: 0.9851 - val_loss: 0.1359 - val_accuracy: 0.9715\n",
            "Epoch 234/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0269 - accuracy: 0.9863 - val_loss: 0.1327 - val_accuracy: 0.9706\n",
            "Epoch 235/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0308 - accuracy: 0.9855 - val_loss: 0.1335 - val_accuracy: 0.9715\n",
            "Epoch 236/500\n",
            "298/298 [==============================] - 3s 9ms/step - loss: 0.0277 - accuracy: 0.9861 - val_loss: 0.1368 - val_accuracy: 0.9674\n",
            "Epoch 237/500\n",
            "298/298 [==============================] - 3s 9ms/step - loss: 0.0298 - accuracy: 0.9862 - val_loss: 0.1384 - val_accuracy: 0.9711\n",
            "Epoch 238/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0278 - accuracy: 0.9873 - val_loss: 0.1292 - val_accuracy: 0.9701\n",
            "Epoch 239/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0306 - accuracy: 0.9855 - val_loss: 0.1385 - val_accuracy: 0.9701\n",
            "Epoch 240/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0296 - accuracy: 0.9857 - val_loss: 0.1381 - val_accuracy: 0.9738\n",
            "Epoch 241/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0275 - accuracy: 0.9864 - val_loss: 0.1324 - val_accuracy: 0.9701\n",
            "Epoch 242/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0281 - accuracy: 0.9862 - val_loss: 0.1339 - val_accuracy: 0.9701\n",
            "Epoch 243/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0287 - accuracy: 0.9865 - val_loss: 0.1306 - val_accuracy: 0.9679\n",
            "Epoch 244/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0280 - accuracy: 0.9865 - val_loss: 0.1371 - val_accuracy: 0.9720\n",
            "Epoch 245/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0274 - accuracy: 0.9854 - val_loss: 0.1393 - val_accuracy: 0.9679\n",
            "Epoch 246/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0268 - accuracy: 0.9873 - val_loss: 0.1464 - val_accuracy: 0.9692\n",
            "Epoch 247/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0294 - accuracy: 0.9857 - val_loss: 0.1391 - val_accuracy: 0.9688\n",
            "Epoch 248/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0273 - accuracy: 0.9873 - val_loss: 0.1431 - val_accuracy: 0.9670\n",
            "Epoch 249/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0276 - accuracy: 0.9868 - val_loss: 0.1460 - val_accuracy: 0.9688\n",
            "Epoch 250/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0279 - accuracy: 0.9860 - val_loss: 0.1502 - val_accuracy: 0.9688\n",
            "Epoch 251/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0268 - accuracy: 0.9875 - val_loss: 0.1332 - val_accuracy: 0.9683\n",
            "Epoch 252/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0286 - accuracy: 0.9866 - val_loss: 0.1483 - val_accuracy: 0.9679\n",
            "Epoch 253/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0283 - accuracy: 0.9858 - val_loss: 0.1421 - val_accuracy: 0.9715\n",
            "Epoch 254/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0288 - accuracy: 0.9850 - val_loss: 0.1350 - val_accuracy: 0.9688\n",
            "Epoch 255/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0266 - accuracy: 0.9854 - val_loss: 0.1391 - val_accuracy: 0.9697\n",
            "Epoch 256/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0262 - accuracy: 0.9870 - val_loss: 0.1428 - val_accuracy: 0.9692\n",
            "Epoch 257/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0285 - accuracy: 0.9865 - val_loss: 0.1412 - val_accuracy: 0.9697\n",
            "Epoch 258/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0308 - accuracy: 0.9865 - val_loss: 0.1433 - val_accuracy: 0.9711\n",
            "Epoch 259/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0278 - accuracy: 0.9869 - val_loss: 0.1368 - val_accuracy: 0.9733\n",
            "Epoch 260/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0268 - accuracy: 0.9865 - val_loss: 0.1489 - val_accuracy: 0.9720\n",
            "Epoch 261/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0270 - accuracy: 0.9871 - val_loss: 0.1374 - val_accuracy: 0.9706\n",
            "Epoch 262/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0262 - accuracy: 0.9877 - val_loss: 0.1423 - val_accuracy: 0.9706\n",
            "Epoch 263/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0272 - accuracy: 0.9866 - val_loss: 0.1372 - val_accuracy: 0.9720\n",
            "Epoch 264/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0306 - accuracy: 0.9862 - val_loss: 0.1511 - val_accuracy: 0.9674\n",
            "Epoch 265/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0280 - accuracy: 0.9859 - val_loss: 0.1459 - val_accuracy: 0.9697\n",
            "Epoch 266/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0291 - accuracy: 0.9862 - val_loss: 0.1471 - val_accuracy: 0.9679\n",
            "Epoch 267/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0289 - accuracy: 0.9859 - val_loss: 0.1424 - val_accuracy: 0.9683\n",
            "Epoch 268/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0264 - accuracy: 0.9876 - val_loss: 0.1433 - val_accuracy: 0.9674\n",
            "Epoch 269/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0274 - accuracy: 0.9869 - val_loss: 0.1498 - val_accuracy: 0.9688\n",
            "Epoch 270/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0270 - accuracy: 0.9867 - val_loss: 0.1365 - val_accuracy: 0.9692\n",
            "Epoch 271/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0277 - accuracy: 0.9868 - val_loss: 0.1406 - val_accuracy: 0.9670\n",
            "Epoch 272/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0254 - accuracy: 0.9871 - val_loss: 0.1619 - val_accuracy: 0.9688\n",
            "Epoch 273/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0265 - accuracy: 0.9864 - val_loss: 0.1505 - val_accuracy: 0.9697\n",
            "Epoch 274/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0320 - accuracy: 0.9840 - val_loss: 0.1489 - val_accuracy: 0.9706\n",
            "Epoch 275/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0282 - accuracy: 0.9861 - val_loss: 0.1581 - val_accuracy: 0.9692\n",
            "Epoch 276/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0264 - accuracy: 0.9870 - val_loss: 0.1489 - val_accuracy: 0.9711\n",
            "Epoch 277/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0250 - accuracy: 0.9876 - val_loss: 0.1587 - val_accuracy: 0.9683\n",
            "Epoch 278/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0265 - accuracy: 0.9864 - val_loss: 0.1554 - val_accuracy: 0.9670\n",
            "Epoch 279/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0277 - accuracy: 0.9864 - val_loss: 0.1512 - val_accuracy: 0.9679\n",
            "Epoch 280/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0270 - accuracy: 0.9868 - val_loss: 0.1458 - val_accuracy: 0.9720\n",
            "Epoch 281/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0257 - accuracy: 0.9880 - val_loss: 0.1542 - val_accuracy: 0.9688\n",
            "Epoch 282/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0269 - accuracy: 0.9871 - val_loss: 0.1511 - val_accuracy: 0.9688\n",
            "Epoch 283/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0256 - accuracy: 0.9865 - val_loss: 0.1577 - val_accuracy: 0.9674\n",
            "Epoch 284/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0269 - accuracy: 0.9868 - val_loss: 0.1395 - val_accuracy: 0.9701\n",
            "Epoch 285/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0275 - accuracy: 0.9861 - val_loss: 0.1429 - val_accuracy: 0.9692\n",
            "Epoch 286/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0265 - accuracy: 0.9869 - val_loss: 0.1477 - val_accuracy: 0.9683\n",
            "Epoch 287/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0268 - accuracy: 0.9871 - val_loss: 0.1518 - val_accuracy: 0.9697\n",
            "Epoch 288/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0264 - accuracy: 0.9865 - val_loss: 0.1555 - val_accuracy: 0.9711\n",
            "Epoch 289/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0266 - accuracy: 0.9875 - val_loss: 0.1421 - val_accuracy: 0.9688\n",
            "Epoch 290/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0249 - accuracy: 0.9878 - val_loss: 0.1436 - val_accuracy: 0.9720\n",
            "Epoch 291/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0239 - accuracy: 0.9883 - val_loss: 0.1475 - val_accuracy: 0.9701\n",
            "Epoch 292/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0264 - accuracy: 0.9861 - val_loss: 0.1475 - val_accuracy: 0.9711\n",
            "Epoch 293/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0263 - accuracy: 0.9858 - val_loss: 0.1405 - val_accuracy: 0.9724\n",
            "Epoch 294/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0258 - accuracy: 0.9878 - val_loss: 0.1474 - val_accuracy: 0.9674\n",
            "Epoch 295/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0262 - accuracy: 0.9868 - val_loss: 0.1521 - val_accuracy: 0.9706\n",
            "Epoch 296/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0255 - accuracy: 0.9878 - val_loss: 0.1601 - val_accuracy: 0.9679\n",
            "Epoch 297/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0294 - accuracy: 0.9862 - val_loss: 0.1554 - val_accuracy: 0.9724\n",
            "Epoch 298/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0274 - accuracy: 0.9875 - val_loss: 0.1569 - val_accuracy: 0.9701\n",
            "Epoch 299/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0280 - accuracy: 0.9861 - val_loss: 0.1593 - val_accuracy: 0.9697\n",
            "Epoch 300/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0270 - accuracy: 0.9863 - val_loss: 0.1579 - val_accuracy: 0.9679\n",
            "Epoch 301/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0262 - accuracy: 0.9858 - val_loss: 0.1599 - val_accuracy: 0.9674\n",
            "Epoch 302/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0279 - accuracy: 0.9867 - val_loss: 0.1501 - val_accuracy: 0.9701\n",
            "Epoch 303/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0258 - accuracy: 0.9875 - val_loss: 0.1500 - val_accuracy: 0.9697\n",
            "Epoch 304/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0253 - accuracy: 0.9874 - val_loss: 0.1553 - val_accuracy: 0.9706\n",
            "Epoch 305/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0256 - accuracy: 0.9880 - val_loss: 0.1517 - val_accuracy: 0.9715\n",
            "Epoch 306/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0254 - accuracy: 0.9867 - val_loss: 0.1544 - val_accuracy: 0.9720\n",
            "Epoch 307/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0252 - accuracy: 0.9879 - val_loss: 0.1474 - val_accuracy: 0.9729\n",
            "Epoch 308/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0258 - accuracy: 0.9877 - val_loss: 0.1471 - val_accuracy: 0.9724\n",
            "Epoch 309/500\n",
            "298/298 [==============================] - 3s 9ms/step - loss: 0.0268 - accuracy: 0.9877 - val_loss: 0.1418 - val_accuracy: 0.9679\n",
            "Epoch 310/500\n",
            "298/298 [==============================] - 3s 9ms/step - loss: 0.0266 - accuracy: 0.9859 - val_loss: 0.1445 - val_accuracy: 0.9729\n",
            "Epoch 311/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0268 - accuracy: 0.9874 - val_loss: 0.1542 - val_accuracy: 0.9701\n",
            "Epoch 312/500\n",
            "298/298 [==============================] - 3s 9ms/step - loss: 0.0256 - accuracy: 0.9869 - val_loss: 0.1391 - val_accuracy: 0.9742\n",
            "Epoch 313/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0277 - accuracy: 0.9856 - val_loss: 0.1384 - val_accuracy: 0.9701\n",
            "Epoch 314/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0257 - accuracy: 0.9874 - val_loss: 0.1557 - val_accuracy: 0.9720\n",
            "Epoch 315/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0260 - accuracy: 0.9863 - val_loss: 0.1537 - val_accuracy: 0.9715\n",
            "Epoch 316/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0260 - accuracy: 0.9867 - val_loss: 0.1495 - val_accuracy: 0.9720\n",
            "Epoch 317/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0257 - accuracy: 0.9875 - val_loss: 0.1589 - val_accuracy: 0.9692\n",
            "Epoch 318/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0262 - accuracy: 0.9880 - val_loss: 0.1555 - val_accuracy: 0.9697\n",
            "Epoch 319/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0255 - accuracy: 0.9869 - val_loss: 0.1517 - val_accuracy: 0.9720\n",
            "Epoch 320/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0256 - accuracy: 0.9877 - val_loss: 0.1461 - val_accuracy: 0.9688\n",
            "Epoch 321/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0263 - accuracy: 0.9871 - val_loss: 0.1560 - val_accuracy: 0.9679\n",
            "Epoch 322/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0260 - accuracy: 0.9879 - val_loss: 0.1620 - val_accuracy: 0.9724\n",
            "Epoch 323/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0255 - accuracy: 0.9873 - val_loss: 0.1465 - val_accuracy: 0.9720\n",
            "Epoch 324/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0253 - accuracy: 0.9881 - val_loss: 0.1500 - val_accuracy: 0.9692\n",
            "Epoch 325/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0231 - accuracy: 0.9887 - val_loss: 0.1682 - val_accuracy: 0.9706\n",
            "Epoch 326/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0249 - accuracy: 0.9886 - val_loss: 0.1474 - val_accuracy: 0.9706\n",
            "Epoch 327/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0262 - accuracy: 0.9876 - val_loss: 0.1510 - val_accuracy: 0.9706\n",
            "Epoch 328/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0246 - accuracy: 0.9865 - val_loss: 0.1516 - val_accuracy: 0.9711\n",
            "Epoch 329/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0245 - accuracy: 0.9887 - val_loss: 0.1670 - val_accuracy: 0.9701\n",
            "Epoch 330/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0290 - accuracy: 0.9856 - val_loss: 0.1472 - val_accuracy: 0.9701\n",
            "Epoch 331/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0254 - accuracy: 0.9874 - val_loss: 0.1589 - val_accuracy: 0.9711\n",
            "Epoch 332/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0240 - accuracy: 0.9877 - val_loss: 0.1559 - val_accuracy: 0.9733\n",
            "Epoch 333/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0264 - accuracy: 0.9873 - val_loss: 0.1490 - val_accuracy: 0.9724\n",
            "Epoch 334/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0253 - accuracy: 0.9867 - val_loss: 0.1540 - val_accuracy: 0.9697\n",
            "Epoch 335/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0257 - accuracy: 0.9877 - val_loss: 0.1629 - val_accuracy: 0.9697\n",
            "Epoch 336/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0240 - accuracy: 0.9874 - val_loss: 0.1580 - val_accuracy: 0.9697\n",
            "Epoch 337/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0252 - accuracy: 0.9864 - val_loss: 0.1511 - val_accuracy: 0.9674\n",
            "Epoch 338/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0231 - accuracy: 0.9876 - val_loss: 0.1514 - val_accuracy: 0.9697\n",
            "Epoch 339/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0238 - accuracy: 0.9878 - val_loss: 0.1516 - val_accuracy: 0.9706\n",
            "Epoch 340/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0253 - accuracy: 0.9859 - val_loss: 0.1595 - val_accuracy: 0.9724\n",
            "Epoch 341/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0247 - accuracy: 0.9871 - val_loss: 0.1578 - val_accuracy: 0.9711\n",
            "Epoch 342/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0263 - accuracy: 0.9865 - val_loss: 0.1489 - val_accuracy: 0.9701\n",
            "Epoch 343/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0261 - accuracy: 0.9873 - val_loss: 0.1646 - val_accuracy: 0.9742\n",
            "Epoch 344/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0257 - accuracy: 0.9869 - val_loss: 0.1604 - val_accuracy: 0.9720\n",
            "Epoch 345/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0271 - accuracy: 0.9873 - val_loss: 0.1609 - val_accuracy: 0.9701\n",
            "Epoch 346/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0252 - accuracy: 0.9868 - val_loss: 0.1656 - val_accuracy: 0.9697\n",
            "Epoch 347/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0255 - accuracy: 0.9881 - val_loss: 0.1657 - val_accuracy: 0.9724\n",
            "Epoch 348/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0240 - accuracy: 0.9895 - val_loss: 0.1553 - val_accuracy: 0.9729\n",
            "Epoch 349/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0242 - accuracy: 0.9883 - val_loss: 0.1609 - val_accuracy: 0.9701\n",
            "Epoch 350/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0260 - accuracy: 0.9873 - val_loss: 0.1694 - val_accuracy: 0.9701\n",
            "Epoch 351/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0235 - accuracy: 0.9881 - val_loss: 0.1628 - val_accuracy: 0.9647\n",
            "Epoch 352/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0265 - accuracy: 0.9859 - val_loss: 0.1523 - val_accuracy: 0.9729\n",
            "Epoch 353/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0264 - accuracy: 0.9858 - val_loss: 0.1562 - val_accuracy: 0.9679\n",
            "Epoch 354/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0249 - accuracy: 0.9877 - val_loss: 0.1683 - val_accuracy: 0.9692\n",
            "Epoch 355/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0266 - accuracy: 0.9863 - val_loss: 0.1742 - val_accuracy: 0.9665\n",
            "Epoch 356/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0271 - accuracy: 0.9861 - val_loss: 0.1626 - val_accuracy: 0.9697\n",
            "Epoch 357/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0247 - accuracy: 0.9873 - val_loss: 0.1560 - val_accuracy: 0.9701\n",
            "Epoch 358/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0241 - accuracy: 0.9881 - val_loss: 0.1589 - val_accuracy: 0.9697\n",
            "Epoch 359/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0240 - accuracy: 0.9878 - val_loss: 0.1558 - val_accuracy: 0.9747\n",
            "Epoch 360/500\n",
            "298/298 [==============================] - 3s 9ms/step - loss: 0.0251 - accuracy: 0.9859 - val_loss: 0.1449 - val_accuracy: 0.9706\n",
            "Epoch 361/500\n",
            "298/298 [==============================] - 3s 9ms/step - loss: 0.0258 - accuracy: 0.9861 - val_loss: 0.1473 - val_accuracy: 0.9674\n",
            "Epoch 362/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0247 - accuracy: 0.9879 - val_loss: 0.1567 - val_accuracy: 0.9706\n",
            "Epoch 363/500\n",
            "298/298 [==============================] - 3s 9ms/step - loss: 0.0232 - accuracy: 0.9884 - val_loss: 0.1609 - val_accuracy: 0.9670\n",
            "Epoch 364/500\n",
            "298/298 [==============================] - 3s 8ms/step - loss: 0.0247 - accuracy: 0.9888 - val_loss: 0.1776 - val_accuracy: 0.9701\n",
            "Epoch 365/500\n",
            "298/298 [==============================] - 3s 8ms/step - loss: 0.0235 - accuracy: 0.9879 - val_loss: 0.1596 - val_accuracy: 0.9692\n",
            "Epoch 366/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0242 - accuracy: 0.9873 - val_loss: 0.1661 - val_accuracy: 0.9711\n",
            "Epoch 367/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0250 - accuracy: 0.9877 - val_loss: 0.1674 - val_accuracy: 0.9688\n",
            "Epoch 368/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0273 - accuracy: 0.9875 - val_loss: 0.1646 - val_accuracy: 0.9720\n",
            "Epoch 369/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0234 - accuracy: 0.9877 - val_loss: 0.1683 - val_accuracy: 0.9724\n",
            "Epoch 370/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0253 - accuracy: 0.9861 - val_loss: 0.1624 - val_accuracy: 0.9711\n",
            "Epoch 371/500\n",
            "298/298 [==============================] - 3s 9ms/step - loss: 0.0260 - accuracy: 0.9873 - val_loss: 0.1570 - val_accuracy: 0.9692\n",
            "Epoch 372/500\n",
            "298/298 [==============================] - 3s 9ms/step - loss: 0.0238 - accuracy: 0.9884 - val_loss: 0.1587 - val_accuracy: 0.9701\n",
            "Epoch 373/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0238 - accuracy: 0.9882 - val_loss: 0.1615 - val_accuracy: 0.9724\n",
            "Epoch 374/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0246 - accuracy: 0.9881 - val_loss: 0.1640 - val_accuracy: 0.9706\n",
            "Epoch 375/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0255 - accuracy: 0.9875 - val_loss: 0.1650 - val_accuracy: 0.9679\n",
            "Epoch 376/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0248 - accuracy: 0.9875 - val_loss: 0.1597 - val_accuracy: 0.9706\n",
            "Epoch 377/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0237 - accuracy: 0.9874 - val_loss: 0.1657 - val_accuracy: 0.9706\n",
            "Epoch 378/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0237 - accuracy: 0.9875 - val_loss: 0.1691 - val_accuracy: 0.9720\n",
            "Epoch 379/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0257 - accuracy: 0.9875 - val_loss: 0.1582 - val_accuracy: 0.9724\n",
            "Epoch 380/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0246 - accuracy: 0.9879 - val_loss: 0.1562 - val_accuracy: 0.9733\n",
            "Epoch 381/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0246 - accuracy: 0.9885 - val_loss: 0.1642 - val_accuracy: 0.9729\n",
            "Epoch 382/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0235 - accuracy: 0.9886 - val_loss: 0.1654 - val_accuracy: 0.9711\n",
            "Epoch 383/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0240 - accuracy: 0.9880 - val_loss: 0.1586 - val_accuracy: 0.9701\n",
            "Epoch 384/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0236 - accuracy: 0.9887 - val_loss: 0.1708 - val_accuracy: 0.9697\n",
            "Epoch 385/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0241 - accuracy: 0.9881 - val_loss: 0.1689 - val_accuracy: 0.9706\n",
            "Epoch 386/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0248 - accuracy: 0.9879 - val_loss: 0.1691 - val_accuracy: 0.9683\n",
            "Epoch 387/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0238 - accuracy: 0.9876 - val_loss: 0.1692 - val_accuracy: 0.9688\n",
            "Epoch 388/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0244 - accuracy: 0.9876 - val_loss: 0.1566 - val_accuracy: 0.9688\n",
            "Epoch 389/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0226 - accuracy: 0.9884 - val_loss: 0.1628 - val_accuracy: 0.9688\n",
            "Epoch 390/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0236 - accuracy: 0.9887 - val_loss: 0.1715 - val_accuracy: 0.9679\n",
            "Epoch 391/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0259 - accuracy: 0.9878 - val_loss: 0.1828 - val_accuracy: 0.9715\n",
            "Epoch 392/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0250 - accuracy: 0.9883 - val_loss: 0.1689 - val_accuracy: 0.9688\n",
            "Epoch 393/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0256 - accuracy: 0.9875 - val_loss: 0.1738 - val_accuracy: 0.9674\n",
            "Epoch 394/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0244 - accuracy: 0.9876 - val_loss: 0.1594 - val_accuracy: 0.9692\n",
            "Epoch 395/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0232 - accuracy: 0.9888 - val_loss: 0.1650 - val_accuracy: 0.9679\n",
            "Epoch 396/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0255 - accuracy: 0.9871 - val_loss: 0.1624 - val_accuracy: 0.9720\n",
            "Epoch 397/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0235 - accuracy: 0.9893 - val_loss: 0.1639 - val_accuracy: 0.9711\n",
            "Epoch 398/500\n",
            "298/298 [==============================] - 3s 10ms/step - loss: 0.0246 - accuracy: 0.9878 - val_loss: 0.1572 - val_accuracy: 0.9724\n",
            "Epoch 399/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0241 - accuracy: 0.9887 - val_loss: 0.1561 - val_accuracy: 0.9692\n",
            "Epoch 400/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0232 - accuracy: 0.9880 - val_loss: 0.1629 - val_accuracy: 0.9701\n",
            "Epoch 401/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0268 - accuracy: 0.9875 - val_loss: 0.1591 - val_accuracy: 0.9692\n",
            "Epoch 402/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0246 - accuracy: 0.9875 - val_loss: 0.1593 - val_accuracy: 0.9729\n",
            "Epoch 403/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0241 - accuracy: 0.9889 - val_loss: 0.1650 - val_accuracy: 0.9711\n",
            "Epoch 404/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0259 - accuracy: 0.9868 - val_loss: 0.1583 - val_accuracy: 0.9688\n",
            "Epoch 405/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0237 - accuracy: 0.9880 - val_loss: 0.1719 - val_accuracy: 0.9706\n",
            "Epoch 406/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0252 - accuracy: 0.9873 - val_loss: 0.1621 - val_accuracy: 0.9711\n",
            "Epoch 407/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0258 - accuracy: 0.9865 - val_loss: 0.1637 - val_accuracy: 0.9715\n",
            "Epoch 408/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0244 - accuracy: 0.9877 - val_loss: 0.1593 - val_accuracy: 0.9692\n",
            "Epoch 409/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0232 - accuracy: 0.9884 - val_loss: 0.1542 - val_accuracy: 0.9720\n",
            "Epoch 410/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0235 - accuracy: 0.9891 - val_loss: 0.1535 - val_accuracy: 0.9711\n",
            "Epoch 411/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0232 - accuracy: 0.9874 - val_loss: 0.1575 - val_accuracy: 0.9692\n",
            "Epoch 412/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0255 - accuracy: 0.9873 - val_loss: 0.1556 - val_accuracy: 0.9706\n",
            "Epoch 413/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0237 - accuracy: 0.9880 - val_loss: 0.1632 - val_accuracy: 0.9701\n",
            "Epoch 414/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0267 - accuracy: 0.9862 - val_loss: 0.1512 - val_accuracy: 0.9697\n",
            "Epoch 415/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0257 - accuracy: 0.9866 - val_loss: 0.1522 - val_accuracy: 0.9706\n",
            "Epoch 416/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0238 - accuracy: 0.9888 - val_loss: 0.1497 - val_accuracy: 0.9742\n",
            "Epoch 417/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0238 - accuracy: 0.9879 - val_loss: 0.1513 - val_accuracy: 0.9738\n",
            "Epoch 418/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0232 - accuracy: 0.9876 - val_loss: 0.1507 - val_accuracy: 0.9738\n",
            "Epoch 419/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0230 - accuracy: 0.9880 - val_loss: 0.1641 - val_accuracy: 0.9701\n",
            "Epoch 420/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0266 - accuracy: 0.9884 - val_loss: 0.1518 - val_accuracy: 0.9674\n",
            "Epoch 421/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0232 - accuracy: 0.9890 - val_loss: 0.1555 - val_accuracy: 0.9720\n",
            "Epoch 422/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0225 - accuracy: 0.9879 - val_loss: 0.1594 - val_accuracy: 0.9715\n",
            "Epoch 423/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0238 - accuracy: 0.9877 - val_loss: 0.1607 - val_accuracy: 0.9679\n",
            "Epoch 424/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0233 - accuracy: 0.9887 - val_loss: 0.1561 - val_accuracy: 0.9701\n",
            "Epoch 425/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0245 - accuracy: 0.9864 - val_loss: 0.1608 - val_accuracy: 0.9715\n",
            "Epoch 426/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0231 - accuracy: 0.9885 - val_loss: 0.1568 - val_accuracy: 0.9724\n",
            "Epoch 427/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0239 - accuracy: 0.9879 - val_loss: 0.1582 - val_accuracy: 0.9715\n",
            "Epoch 428/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0234 - accuracy: 0.9875 - val_loss: 0.1613 - val_accuracy: 0.9692\n",
            "Epoch 429/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0236 - accuracy: 0.9878 - val_loss: 0.1664 - val_accuracy: 0.9701\n",
            "Epoch 430/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0250 - accuracy: 0.9873 - val_loss: 0.1684 - val_accuracy: 0.9688\n",
            "Epoch 431/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0243 - accuracy: 0.9874 - val_loss: 0.1639 - val_accuracy: 0.9697\n",
            "Epoch 432/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0228 - accuracy: 0.9881 - val_loss: 0.1596 - val_accuracy: 0.9697\n",
            "Epoch 433/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0225 - accuracy: 0.9881 - val_loss: 0.1646 - val_accuracy: 0.9715\n",
            "Epoch 434/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0216 - accuracy: 0.9893 - val_loss: 0.1634 - val_accuracy: 0.9711\n",
            "Epoch 435/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0246 - accuracy: 0.9876 - val_loss: 0.1495 - val_accuracy: 0.9697\n",
            "Epoch 436/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0222 - accuracy: 0.9884 - val_loss: 0.1711 - val_accuracy: 0.9701\n",
            "Epoch 437/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0252 - accuracy: 0.9869 - val_loss: 0.1580 - val_accuracy: 0.9701\n",
            "Epoch 438/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0256 - accuracy: 0.9884 - val_loss: 0.1519 - val_accuracy: 0.9724\n",
            "Epoch 439/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0231 - accuracy: 0.9880 - val_loss: 0.1581 - val_accuracy: 0.9701\n",
            "Epoch 440/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0237 - accuracy: 0.9887 - val_loss: 0.1533 - val_accuracy: 0.9724\n",
            "Epoch 441/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0230 - accuracy: 0.9891 - val_loss: 0.1610 - val_accuracy: 0.9697\n",
            "Epoch 442/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0231 - accuracy: 0.9878 - val_loss: 0.1501 - val_accuracy: 0.9715\n",
            "Epoch 443/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0241 - accuracy: 0.9885 - val_loss: 0.1676 - val_accuracy: 0.9688\n",
            "Epoch 444/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0232 - accuracy: 0.9890 - val_loss: 0.1517 - val_accuracy: 0.9701\n",
            "Epoch 445/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0254 - accuracy: 0.9876 - val_loss: 0.1582 - val_accuracy: 0.9715\n",
            "Epoch 446/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0231 - accuracy: 0.9891 - val_loss: 0.1602 - val_accuracy: 0.9729\n",
            "Epoch 447/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0235 - accuracy: 0.9880 - val_loss: 0.1619 - val_accuracy: 0.9701\n",
            "Epoch 448/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0233 - accuracy: 0.9892 - val_loss: 0.1544 - val_accuracy: 0.9674\n",
            "Epoch 449/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0248 - accuracy: 0.9875 - val_loss: 0.1705 - val_accuracy: 0.9679\n",
            "Epoch 450/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0243 - accuracy: 0.9874 - val_loss: 0.1609 - val_accuracy: 0.9674\n",
            "Epoch 451/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0221 - accuracy: 0.9883 - val_loss: 0.1679 - val_accuracy: 0.9670\n",
            "Epoch 452/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0237 - accuracy: 0.9881 - val_loss: 0.1548 - val_accuracy: 0.9720\n",
            "Epoch 453/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0232 - accuracy: 0.9896 - val_loss: 0.1560 - val_accuracy: 0.9738\n",
            "Epoch 454/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0226 - accuracy: 0.9881 - val_loss: 0.1643 - val_accuracy: 0.9688\n",
            "Epoch 455/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0247 - accuracy: 0.9875 - val_loss: 0.1561 - val_accuracy: 0.9701\n",
            "Epoch 456/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0226 - accuracy: 0.9883 - val_loss: 0.1565 - val_accuracy: 0.9697\n",
            "Epoch 457/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0249 - accuracy: 0.9874 - val_loss: 0.1596 - val_accuracy: 0.9706\n",
            "Epoch 458/500\n",
            "298/298 [==============================] - 3s 9ms/step - loss: 0.0246 - accuracy: 0.9878 - val_loss: 0.1536 - val_accuracy: 0.9715\n",
            "Epoch 459/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0219 - accuracy: 0.9888 - val_loss: 0.1650 - val_accuracy: 0.9724\n",
            "Epoch 460/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0227 - accuracy: 0.9886 - val_loss: 0.1573 - val_accuracy: 0.9733\n",
            "Epoch 461/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0222 - accuracy: 0.9897 - val_loss: 0.1524 - val_accuracy: 0.9692\n",
            "Epoch 462/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0240 - accuracy: 0.9876 - val_loss: 0.1449 - val_accuracy: 0.9715\n",
            "Epoch 463/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0228 - accuracy: 0.9893 - val_loss: 0.1606 - val_accuracy: 0.9697\n",
            "Epoch 464/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0253 - accuracy: 0.9878 - val_loss: 0.1562 - val_accuracy: 0.9711\n",
            "Epoch 465/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0220 - accuracy: 0.9893 - val_loss: 0.1546 - val_accuracy: 0.9720\n",
            "Epoch 466/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0230 - accuracy: 0.9886 - val_loss: 0.1512 - val_accuracy: 0.9715\n",
            "Epoch 467/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0242 - accuracy: 0.9873 - val_loss: 0.1494 - val_accuracy: 0.9733\n",
            "Epoch 468/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0239 - accuracy: 0.9883 - val_loss: 0.1516 - val_accuracy: 0.9692\n",
            "Epoch 469/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0229 - accuracy: 0.9886 - val_loss: 0.1497 - val_accuracy: 0.9720\n",
            "Epoch 470/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0224 - accuracy: 0.9880 - val_loss: 0.1521 - val_accuracy: 0.9715\n",
            "Epoch 471/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0217 - accuracy: 0.9889 - val_loss: 0.1517 - val_accuracy: 0.9701\n",
            "Epoch 472/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0233 - accuracy: 0.9881 - val_loss: 0.1561 - val_accuracy: 0.9715\n",
            "Epoch 473/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0224 - accuracy: 0.9892 - val_loss: 0.1564 - val_accuracy: 0.9697\n",
            "Epoch 474/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0251 - accuracy: 0.9875 - val_loss: 0.1506 - val_accuracy: 0.9688\n",
            "Epoch 475/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0225 - accuracy: 0.9885 - val_loss: 0.1563 - val_accuracy: 0.9715\n",
            "Epoch 476/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0248 - accuracy: 0.9890 - val_loss: 0.1547 - val_accuracy: 0.9742\n",
            "Epoch 477/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0245 - accuracy: 0.9884 - val_loss: 0.1475 - val_accuracy: 0.9742\n",
            "Epoch 478/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0246 - accuracy: 0.9871 - val_loss: 0.1513 - val_accuracy: 0.9738\n",
            "Epoch 479/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0244 - accuracy: 0.9880 - val_loss: 0.1603 - val_accuracy: 0.9733\n",
            "Epoch 480/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0230 - accuracy: 0.9879 - val_loss: 0.1512 - val_accuracy: 0.9720\n",
            "Epoch 481/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0233 - accuracy: 0.9885 - val_loss: 0.1523 - val_accuracy: 0.9742\n",
            "Epoch 482/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0221 - accuracy: 0.9877 - val_loss: 0.1553 - val_accuracy: 0.9729\n",
            "Epoch 483/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0238 - accuracy: 0.9865 - val_loss: 0.1512 - val_accuracy: 0.9711\n",
            "Epoch 484/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0224 - accuracy: 0.9890 - val_loss: 0.1558 - val_accuracy: 0.9711\n",
            "Epoch 485/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0225 - accuracy: 0.9881 - val_loss: 0.1782 - val_accuracy: 0.9697\n",
            "Epoch 486/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0232 - accuracy: 0.9891 - val_loss: 0.1558 - val_accuracy: 0.9711\n",
            "Epoch 487/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0243 - accuracy: 0.9874 - val_loss: 0.1487 - val_accuracy: 0.9715\n",
            "Epoch 488/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0231 - accuracy: 0.9880 - val_loss: 0.1509 - val_accuracy: 0.9729\n",
            "Epoch 489/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0229 - accuracy: 0.9873 - val_loss: 0.1489 - val_accuracy: 0.9733\n",
            "Epoch 490/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0214 - accuracy: 0.9893 - val_loss: 0.1646 - val_accuracy: 0.9706\n",
            "Epoch 491/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0235 - accuracy: 0.9877 - val_loss: 0.1578 - val_accuracy: 0.9733\n",
            "Epoch 492/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0234 - accuracy: 0.9886 - val_loss: 0.1465 - val_accuracy: 0.9711\n",
            "Epoch 493/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0206 - accuracy: 0.9887 - val_loss: 0.1636 - val_accuracy: 0.9711\n",
            "Epoch 494/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0230 - accuracy: 0.9880 - val_loss: 0.1533 - val_accuracy: 0.9706\n",
            "Epoch 495/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0247 - accuracy: 0.9877 - val_loss: 0.1499 - val_accuracy: 0.9692\n",
            "Epoch 496/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0223 - accuracy: 0.9892 - val_loss: 0.1594 - val_accuracy: 0.9697\n",
            "Epoch 497/500\n",
            "298/298 [==============================] - 3s 8ms/step - loss: 0.0221 - accuracy: 0.9889 - val_loss: 0.1619 - val_accuracy: 0.9701\n",
            "Epoch 498/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0204 - accuracy: 0.9900 - val_loss: 0.1601 - val_accuracy: 0.9715\n",
            "Epoch 499/500\n",
            "298/298 [==============================] - 3s 9ms/step - loss: 0.0249 - accuracy: 0.9893 - val_loss: 0.1634 - val_accuracy: 0.9683\n",
            "Epoch 500/500\n",
            "298/298 [==============================] - 3s 9ms/step - loss: 0.0223 - accuracy: 0.9888 - val_loss: 0.1480 - val_accuracy: 0.9697\n",
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm (LSTM)                 (None, 1, 64)             24320     \n",
            "                                                                 \n",
            " dropout_8 (Dropout)         (None, 1, 64)             0         \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 32)                12416     \n",
            "                                                                 \n",
            " dropout_9 (Dropout)         (None, 32)                0         \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 36,769\n",
            "Trainable params: 36,769\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Define the model architecture\n",
        "model_lstm = Sequential()\n",
        "model_lstm.add(LSTM(64, input_shape=(train_data_3d.shape[1], train_data_3d.shape[2]), return_sequences=True))\n",
        "model_lstm.add(Dropout(0.2))\n",
        "model_lstm.add(LSTM(32))\n",
        "model_lstm.add(Dropout(0.2))\n",
        "model_lstm.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "model_lstm.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model_lstm.fit(train_data_3d, train_labels, validation_data=(test_data_3d, test_labels), epochs=500, batch_size=33)\n",
        "model_lstm.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqCFtgcrwGkx"
      },
      "source": [
        "### 5. GRU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PoscElJcwGkx",
        "outputId": "7c571781-ed81-4047-e6b0-514bac3d4836"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "298/298 [==============================] - 12s 13ms/step - loss: 0.2824 - accuracy: 0.8964 - val_loss: 0.1922 - val_accuracy: 0.9240\n",
            "Epoch 2/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.1861 - accuracy: 0.9279 - val_loss: 0.1860 - val_accuracy: 0.9294\n",
            "Epoch 3/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.1736 - accuracy: 0.9293 - val_loss: 0.1867 - val_accuracy: 0.9276\n",
            "Epoch 4/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.1632 - accuracy: 0.9357 - val_loss: 0.1658 - val_accuracy: 0.9349\n",
            "Epoch 5/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.1543 - accuracy: 0.9377 - val_loss: 0.1628 - val_accuracy: 0.9313\n",
            "Epoch 6/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.1458 - accuracy: 0.9406 - val_loss: 0.1581 - val_accuracy: 0.9322\n",
            "Epoch 7/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.1412 - accuracy: 0.9419 - val_loss: 0.1491 - val_accuracy: 0.9380\n",
            "Epoch 8/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.1344 - accuracy: 0.9433 - val_loss: 0.1459 - val_accuracy: 0.9430\n",
            "Epoch 9/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.1277 - accuracy: 0.9460 - val_loss: 0.1414 - val_accuracy: 0.9426\n",
            "Epoch 10/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.1250 - accuracy: 0.9462 - val_loss: 0.1414 - val_accuracy: 0.9448\n",
            "Epoch 11/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.1171 - accuracy: 0.9499 - val_loss: 0.1465 - val_accuracy: 0.9453\n",
            "Epoch 12/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.1185 - accuracy: 0.9495 - val_loss: 0.1338 - val_accuracy: 0.9444\n",
            "Epoch 13/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.1114 - accuracy: 0.9528 - val_loss: 0.1285 - val_accuracy: 0.9502\n",
            "Epoch 14/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.1098 - accuracy: 0.9551 - val_loss: 0.1230 - val_accuracy: 0.9530\n",
            "Epoch 15/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.1050 - accuracy: 0.9557 - val_loss: 0.1370 - val_accuracy: 0.9493\n",
            "Epoch 16/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.1054 - accuracy: 0.9564 - val_loss: 0.1198 - val_accuracy: 0.9534\n",
            "Epoch 17/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0993 - accuracy: 0.9578 - val_loss: 0.1200 - val_accuracy: 0.9534\n",
            "Epoch 18/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0971 - accuracy: 0.9578 - val_loss: 0.1212 - val_accuracy: 0.9539\n",
            "Epoch 19/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0941 - accuracy: 0.9593 - val_loss: 0.1184 - val_accuracy: 0.9530\n",
            "Epoch 20/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0922 - accuracy: 0.9607 - val_loss: 0.1127 - val_accuracy: 0.9570\n",
            "Epoch 21/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0906 - accuracy: 0.9614 - val_loss: 0.1153 - val_accuracy: 0.9561\n",
            "Epoch 22/500\n",
            "298/298 [==============================] - 3s 8ms/step - loss: 0.0925 - accuracy: 0.9612 - val_loss: 0.1184 - val_accuracy: 0.9548\n",
            "Epoch 23/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0858 - accuracy: 0.9648 - val_loss: 0.1144 - val_accuracy: 0.9611\n",
            "Epoch 24/500\n",
            "298/298 [==============================] - 3s 8ms/step - loss: 0.0842 - accuracy: 0.9640 - val_loss: 0.1115 - val_accuracy: 0.9584\n",
            "Epoch 25/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0843 - accuracy: 0.9635 - val_loss: 0.1088 - val_accuracy: 0.9579\n",
            "Epoch 26/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0846 - accuracy: 0.9623 - val_loss: 0.1107 - val_accuracy: 0.9602\n",
            "Epoch 27/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0810 - accuracy: 0.9662 - val_loss: 0.1172 - val_accuracy: 0.9525\n",
            "Epoch 28/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0798 - accuracy: 0.9659 - val_loss: 0.1125 - val_accuracy: 0.9597\n",
            "Epoch 29/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0800 - accuracy: 0.9657 - val_loss: 0.1061 - val_accuracy: 0.9602\n",
            "Epoch 30/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0762 - accuracy: 0.9678 - val_loss: 0.1061 - val_accuracy: 0.9643\n",
            "Epoch 31/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0776 - accuracy: 0.9678 - val_loss: 0.1127 - val_accuracy: 0.9625\n",
            "Epoch 32/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0764 - accuracy: 0.9677 - val_loss: 0.1052 - val_accuracy: 0.9602\n",
            "Epoch 33/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0742 - accuracy: 0.9673 - val_loss: 0.1063 - val_accuracy: 0.9597\n",
            "Epoch 34/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0751 - accuracy: 0.9691 - val_loss: 0.1090 - val_accuracy: 0.9611\n",
            "Epoch 35/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0718 - accuracy: 0.9689 - val_loss: 0.1060 - val_accuracy: 0.9629\n",
            "Epoch 36/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0716 - accuracy: 0.9703 - val_loss: 0.1079 - val_accuracy: 0.9625\n",
            "Epoch 37/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0697 - accuracy: 0.9697 - val_loss: 0.1067 - val_accuracy: 0.9602\n",
            "Epoch 38/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0689 - accuracy: 0.9710 - val_loss: 0.1055 - val_accuracy: 0.9597\n",
            "Epoch 39/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0711 - accuracy: 0.9707 - val_loss: 0.1127 - val_accuracy: 0.9575\n",
            "Epoch 40/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0687 - accuracy: 0.9718 - val_loss: 0.1106 - val_accuracy: 0.9620\n",
            "Epoch 41/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0676 - accuracy: 0.9717 - val_loss: 0.1092 - val_accuracy: 0.9620\n",
            "Epoch 42/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0669 - accuracy: 0.9724 - val_loss: 0.1054 - val_accuracy: 0.9634\n",
            "Epoch 43/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0660 - accuracy: 0.9705 - val_loss: 0.1093 - val_accuracy: 0.9611\n",
            "Epoch 44/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0688 - accuracy: 0.9710 - val_loss: 0.1025 - val_accuracy: 0.9661\n",
            "Epoch 45/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0656 - accuracy: 0.9722 - val_loss: 0.1103 - val_accuracy: 0.9607\n",
            "Epoch 46/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0614 - accuracy: 0.9745 - val_loss: 0.1054 - val_accuracy: 0.9625\n",
            "Epoch 47/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0628 - accuracy: 0.9716 - val_loss: 0.1029 - val_accuracy: 0.9652\n",
            "Epoch 48/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0623 - accuracy: 0.9717 - val_loss: 0.1020 - val_accuracy: 0.9643\n",
            "Epoch 49/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0619 - accuracy: 0.9722 - val_loss: 0.1055 - val_accuracy: 0.9656\n",
            "Epoch 50/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0640 - accuracy: 0.9735 - val_loss: 0.0975 - val_accuracy: 0.9683\n",
            "Epoch 51/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0584 - accuracy: 0.9754 - val_loss: 0.1018 - val_accuracy: 0.9670\n",
            "Epoch 52/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0621 - accuracy: 0.9747 - val_loss: 0.1044 - val_accuracy: 0.9661\n",
            "Epoch 53/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0584 - accuracy: 0.9746 - val_loss: 0.1031 - val_accuracy: 0.9661\n",
            "Epoch 54/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0570 - accuracy: 0.9742 - val_loss: 0.1112 - val_accuracy: 0.9629\n",
            "Epoch 55/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0559 - accuracy: 0.9763 - val_loss: 0.1052 - val_accuracy: 0.9674\n",
            "Epoch 56/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0584 - accuracy: 0.9735 - val_loss: 0.1094 - val_accuracy: 0.9616\n",
            "Epoch 57/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0533 - accuracy: 0.9760 - val_loss: 0.0997 - val_accuracy: 0.9670\n",
            "Epoch 58/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0576 - accuracy: 0.9754 - val_loss: 0.1014 - val_accuracy: 0.9670\n",
            "Epoch 59/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0564 - accuracy: 0.9765 - val_loss: 0.1055 - val_accuracy: 0.9670\n",
            "Epoch 60/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0541 - accuracy: 0.9763 - val_loss: 0.1104 - val_accuracy: 0.9674\n",
            "Epoch 61/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0545 - accuracy: 0.9774 - val_loss: 0.1090 - val_accuracy: 0.9634\n",
            "Epoch 62/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0578 - accuracy: 0.9746 - val_loss: 0.1011 - val_accuracy: 0.9661\n",
            "Epoch 63/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0527 - accuracy: 0.9779 - val_loss: 0.1097 - val_accuracy: 0.9656\n",
            "Epoch 64/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0565 - accuracy: 0.9746 - val_loss: 0.1082 - val_accuracy: 0.9638\n",
            "Epoch 65/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0562 - accuracy: 0.9757 - val_loss: 0.1058 - val_accuracy: 0.9674\n",
            "Epoch 66/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0537 - accuracy: 0.9755 - val_loss: 0.1055 - val_accuracy: 0.9638\n",
            "Epoch 67/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0538 - accuracy: 0.9762 - val_loss: 0.1032 - val_accuracy: 0.9665\n",
            "Epoch 68/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0548 - accuracy: 0.9761 - val_loss: 0.0962 - val_accuracy: 0.9683\n",
            "Epoch 69/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0538 - accuracy: 0.9765 - val_loss: 0.0979 - val_accuracy: 0.9711\n",
            "Epoch 70/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0517 - accuracy: 0.9772 - val_loss: 0.0987 - val_accuracy: 0.9665\n",
            "Epoch 71/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0532 - accuracy: 0.9759 - val_loss: 0.0982 - val_accuracy: 0.9674\n",
            "Epoch 72/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0534 - accuracy: 0.9776 - val_loss: 0.0983 - val_accuracy: 0.9661\n",
            "Epoch 73/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0513 - accuracy: 0.9779 - val_loss: 0.1029 - val_accuracy: 0.9688\n",
            "Epoch 74/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0506 - accuracy: 0.9782 - val_loss: 0.1002 - val_accuracy: 0.9692\n",
            "Epoch 75/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0517 - accuracy: 0.9776 - val_loss: 0.1024 - val_accuracy: 0.9711\n",
            "Epoch 76/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0516 - accuracy: 0.9787 - val_loss: 0.1017 - val_accuracy: 0.9665\n",
            "Epoch 77/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0477 - accuracy: 0.9798 - val_loss: 0.1087 - val_accuracy: 0.9692\n",
            "Epoch 78/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0504 - accuracy: 0.9776 - val_loss: 0.1014 - val_accuracy: 0.9697\n",
            "Epoch 79/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0511 - accuracy: 0.9794 - val_loss: 0.1017 - val_accuracy: 0.9706\n",
            "Epoch 80/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0499 - accuracy: 0.9775 - val_loss: 0.0981 - val_accuracy: 0.9674\n",
            "Epoch 81/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0502 - accuracy: 0.9787 - val_loss: 0.0980 - val_accuracy: 0.9697\n",
            "Epoch 82/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0483 - accuracy: 0.9768 - val_loss: 0.1033 - val_accuracy: 0.9679\n",
            "Epoch 83/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0516 - accuracy: 0.9784 - val_loss: 0.1019 - val_accuracy: 0.9665\n",
            "Epoch 84/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0474 - accuracy: 0.9796 - val_loss: 0.1086 - val_accuracy: 0.9656\n",
            "Epoch 85/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0488 - accuracy: 0.9786 - val_loss: 0.0992 - val_accuracy: 0.9688\n",
            "Epoch 86/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0461 - accuracy: 0.9811 - val_loss: 0.0970 - val_accuracy: 0.9697\n",
            "Epoch 87/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0497 - accuracy: 0.9792 - val_loss: 0.1016 - val_accuracy: 0.9701\n",
            "Epoch 88/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0462 - accuracy: 0.9811 - val_loss: 0.1045 - val_accuracy: 0.9697\n",
            "Epoch 89/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0482 - accuracy: 0.9798 - val_loss: 0.1025 - val_accuracy: 0.9688\n",
            "Epoch 90/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0455 - accuracy: 0.9802 - val_loss: 0.1086 - val_accuracy: 0.9683\n",
            "Epoch 91/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0469 - accuracy: 0.9799 - val_loss: 0.1049 - val_accuracy: 0.9674\n",
            "Epoch 92/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0453 - accuracy: 0.9798 - val_loss: 0.1051 - val_accuracy: 0.9692\n",
            "Epoch 93/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0462 - accuracy: 0.9781 - val_loss: 0.1043 - val_accuracy: 0.9688\n",
            "Epoch 94/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0435 - accuracy: 0.9808 - val_loss: 0.1027 - val_accuracy: 0.9688\n",
            "Epoch 95/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0426 - accuracy: 0.9811 - val_loss: 0.1113 - val_accuracy: 0.9665\n",
            "Epoch 96/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0482 - accuracy: 0.9794 - val_loss: 0.1065 - val_accuracy: 0.9701\n",
            "Epoch 97/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0476 - accuracy: 0.9787 - val_loss: 0.1035 - val_accuracy: 0.9674\n",
            "Epoch 98/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0436 - accuracy: 0.9807 - val_loss: 0.1035 - val_accuracy: 0.9697\n",
            "Epoch 99/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0476 - accuracy: 0.9795 - val_loss: 0.1045 - val_accuracy: 0.9688\n",
            "Epoch 100/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0454 - accuracy: 0.9807 - val_loss: 0.1090 - val_accuracy: 0.9670\n",
            "Epoch 101/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0440 - accuracy: 0.9794 - val_loss: 0.1105 - val_accuracy: 0.9688\n",
            "Epoch 102/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0442 - accuracy: 0.9807 - val_loss: 0.1062 - val_accuracy: 0.9665\n",
            "Epoch 103/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0452 - accuracy: 0.9804 - val_loss: 0.1038 - val_accuracy: 0.9688\n",
            "Epoch 104/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0429 - accuracy: 0.9813 - val_loss: 0.1033 - val_accuracy: 0.9674\n",
            "Epoch 105/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0407 - accuracy: 0.9810 - val_loss: 0.1086 - val_accuracy: 0.9665\n",
            "Epoch 106/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0426 - accuracy: 0.9808 - val_loss: 0.1065 - val_accuracy: 0.9688\n",
            "Epoch 107/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0450 - accuracy: 0.9797 - val_loss: 0.1049 - val_accuracy: 0.9679\n",
            "Epoch 108/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0417 - accuracy: 0.9820 - val_loss: 0.1126 - val_accuracy: 0.9683\n",
            "Epoch 109/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0430 - accuracy: 0.9814 - val_loss: 0.1028 - val_accuracy: 0.9674\n",
            "Epoch 110/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0425 - accuracy: 0.9817 - val_loss: 0.0979 - val_accuracy: 0.9679\n",
            "Epoch 111/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0434 - accuracy: 0.9795 - val_loss: 0.1064 - val_accuracy: 0.9711\n",
            "Epoch 112/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0428 - accuracy: 0.9817 - val_loss: 0.1040 - val_accuracy: 0.9724\n",
            "Epoch 113/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0426 - accuracy: 0.9816 - val_loss: 0.1121 - val_accuracy: 0.9661\n",
            "Epoch 114/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0426 - accuracy: 0.9829 - val_loss: 0.1002 - val_accuracy: 0.9701\n",
            "Epoch 115/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0404 - accuracy: 0.9815 - val_loss: 0.1121 - val_accuracy: 0.9706\n",
            "Epoch 116/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0427 - accuracy: 0.9806 - val_loss: 0.1034 - val_accuracy: 0.9688\n",
            "Epoch 117/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0405 - accuracy: 0.9823 - val_loss: 0.1021 - val_accuracy: 0.9670\n",
            "Epoch 118/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0403 - accuracy: 0.9825 - val_loss: 0.1034 - val_accuracy: 0.9692\n",
            "Epoch 119/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0401 - accuracy: 0.9829 - val_loss: 0.1143 - val_accuracy: 0.9715\n",
            "Epoch 120/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0379 - accuracy: 0.9838 - val_loss: 0.1071 - val_accuracy: 0.9715\n",
            "Epoch 121/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0414 - accuracy: 0.9807 - val_loss: 0.1103 - val_accuracy: 0.9688\n",
            "Epoch 122/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0385 - accuracy: 0.9820 - val_loss: 0.1109 - val_accuracy: 0.9706\n",
            "Epoch 123/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0400 - accuracy: 0.9832 - val_loss: 0.1074 - val_accuracy: 0.9679\n",
            "Epoch 124/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0391 - accuracy: 0.9829 - val_loss: 0.1123 - val_accuracy: 0.9701\n",
            "Epoch 125/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0398 - accuracy: 0.9838 - val_loss: 0.1019 - val_accuracy: 0.9729\n",
            "Epoch 126/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0378 - accuracy: 0.9833 - val_loss: 0.1141 - val_accuracy: 0.9679\n",
            "Epoch 127/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0396 - accuracy: 0.9837 - val_loss: 0.1166 - val_accuracy: 0.9706\n",
            "Epoch 128/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0386 - accuracy: 0.9826 - val_loss: 0.1080 - val_accuracy: 0.9701\n",
            "Epoch 129/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0380 - accuracy: 0.9838 - val_loss: 0.1159 - val_accuracy: 0.9692\n",
            "Epoch 130/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0389 - accuracy: 0.9826 - val_loss: 0.1095 - val_accuracy: 0.9715\n",
            "Epoch 131/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0399 - accuracy: 0.9838 - val_loss: 0.1133 - val_accuracy: 0.9692\n",
            "Epoch 132/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0371 - accuracy: 0.9836 - val_loss: 0.1086 - val_accuracy: 0.9706\n",
            "Epoch 133/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0391 - accuracy: 0.9822 - val_loss: 0.1132 - val_accuracy: 0.9688\n",
            "Epoch 134/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0362 - accuracy: 0.9836 - val_loss: 0.1094 - val_accuracy: 0.9679\n",
            "Epoch 135/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0399 - accuracy: 0.9816 - val_loss: 0.1085 - val_accuracy: 0.9724\n",
            "Epoch 136/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0374 - accuracy: 0.9830 - val_loss: 0.1058 - val_accuracy: 0.9706\n",
            "Epoch 137/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0406 - accuracy: 0.9810 - val_loss: 0.1075 - val_accuracy: 0.9679\n",
            "Epoch 138/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0358 - accuracy: 0.9837 - val_loss: 0.1107 - val_accuracy: 0.9683\n",
            "Epoch 139/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0365 - accuracy: 0.9829 - val_loss: 0.1102 - val_accuracy: 0.9715\n",
            "Epoch 140/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0361 - accuracy: 0.9850 - val_loss: 0.1148 - val_accuracy: 0.9679\n",
            "Epoch 141/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0362 - accuracy: 0.9846 - val_loss: 0.1119 - val_accuracy: 0.9711\n",
            "Epoch 142/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0374 - accuracy: 0.9844 - val_loss: 0.1145 - val_accuracy: 0.9683\n",
            "Epoch 143/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0377 - accuracy: 0.9818 - val_loss: 0.1123 - val_accuracy: 0.9701\n",
            "Epoch 144/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0384 - accuracy: 0.9819 - val_loss: 0.1062 - val_accuracy: 0.9697\n",
            "Epoch 145/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0356 - accuracy: 0.9828 - val_loss: 0.1142 - val_accuracy: 0.9697\n",
            "Epoch 146/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0381 - accuracy: 0.9827 - val_loss: 0.1132 - val_accuracy: 0.9661\n",
            "Epoch 147/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0361 - accuracy: 0.9840 - val_loss: 0.1163 - val_accuracy: 0.9643\n",
            "Epoch 148/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0383 - accuracy: 0.9825 - val_loss: 0.1156 - val_accuracy: 0.9706\n",
            "Epoch 149/500\n",
            "298/298 [==============================] - 3s 9ms/step - loss: 0.0370 - accuracy: 0.9841 - val_loss: 0.1153 - val_accuracy: 0.9683\n",
            "Epoch 150/500\n",
            "298/298 [==============================] - 3s 9ms/step - loss: 0.0339 - accuracy: 0.9848 - val_loss: 0.1138 - val_accuracy: 0.9683\n",
            "Epoch 151/500\n",
            "298/298 [==============================] - 3s 9ms/step - loss: 0.0357 - accuracy: 0.9836 - val_loss: 0.1193 - val_accuracy: 0.9679\n",
            "Epoch 152/500\n",
            "298/298 [==============================] - 3s 9ms/step - loss: 0.0379 - accuracy: 0.9837 - val_loss: 0.1123 - val_accuracy: 0.9674\n",
            "Epoch 153/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0406 - accuracy: 0.9827 - val_loss: 0.1110 - val_accuracy: 0.9674\n",
            "Epoch 154/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0351 - accuracy: 0.9837 - val_loss: 0.1205 - val_accuracy: 0.9670\n",
            "Epoch 155/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0354 - accuracy: 0.9848 - val_loss: 0.1066 - val_accuracy: 0.9724\n",
            "Epoch 156/500\n",
            "298/298 [==============================] - 3s 9ms/step - loss: 0.0354 - accuracy: 0.9828 - val_loss: 0.1084 - val_accuracy: 0.9688\n",
            "Epoch 157/500\n",
            "298/298 [==============================] - 3s 9ms/step - loss: 0.0355 - accuracy: 0.9839 - val_loss: 0.1214 - val_accuracy: 0.9720\n",
            "Epoch 158/500\n",
            "298/298 [==============================] - 3s 9ms/step - loss: 0.0374 - accuracy: 0.9844 - val_loss: 0.1134 - val_accuracy: 0.9729\n",
            "Epoch 159/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0350 - accuracy: 0.9851 - val_loss: 0.1055 - val_accuracy: 0.9701\n",
            "Epoch 160/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0361 - accuracy: 0.9848 - val_loss: 0.1048 - val_accuracy: 0.9692\n",
            "Epoch 161/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0350 - accuracy: 0.9840 - val_loss: 0.1084 - val_accuracy: 0.9674\n",
            "Epoch 162/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0398 - accuracy: 0.9835 - val_loss: 0.1052 - val_accuracy: 0.9674\n",
            "Epoch 163/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0378 - accuracy: 0.9831 - val_loss: 0.1062 - val_accuracy: 0.9674\n",
            "Epoch 164/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0346 - accuracy: 0.9846 - val_loss: 0.1138 - val_accuracy: 0.9697\n",
            "Epoch 165/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0347 - accuracy: 0.9845 - val_loss: 0.1108 - val_accuracy: 0.9679\n",
            "Epoch 166/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0348 - accuracy: 0.9848 - val_loss: 0.1084 - val_accuracy: 0.9706\n",
            "Epoch 167/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0345 - accuracy: 0.9843 - val_loss: 0.1120 - val_accuracy: 0.9674\n",
            "Epoch 168/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0329 - accuracy: 0.9858 - val_loss: 0.1046 - val_accuracy: 0.9711\n",
            "Epoch 169/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0358 - accuracy: 0.9835 - val_loss: 0.1121 - val_accuracy: 0.9661\n",
            "Epoch 170/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0345 - accuracy: 0.9836 - val_loss: 0.1116 - val_accuracy: 0.9688\n",
            "Epoch 171/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0353 - accuracy: 0.9834 - val_loss: 0.1069 - val_accuracy: 0.9661\n",
            "Epoch 172/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0342 - accuracy: 0.9844 - val_loss: 0.1155 - val_accuracy: 0.9724\n",
            "Epoch 173/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0336 - accuracy: 0.9850 - val_loss: 0.1097 - val_accuracy: 0.9697\n",
            "Epoch 174/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0362 - accuracy: 0.9828 - val_loss: 0.1086 - val_accuracy: 0.9679\n",
            "Epoch 175/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0349 - accuracy: 0.9854 - val_loss: 0.1061 - val_accuracy: 0.9706\n",
            "Epoch 176/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0343 - accuracy: 0.9850 - val_loss: 0.1108 - val_accuracy: 0.9701\n",
            "Epoch 177/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0343 - accuracy: 0.9843 - val_loss: 0.1126 - val_accuracy: 0.9697\n",
            "Epoch 178/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0332 - accuracy: 0.9851 - val_loss: 0.1102 - val_accuracy: 0.9697\n",
            "Epoch 179/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0315 - accuracy: 0.9862 - val_loss: 0.1095 - val_accuracy: 0.9701\n",
            "Epoch 180/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0329 - accuracy: 0.9852 - val_loss: 0.1120 - val_accuracy: 0.9697\n",
            "Epoch 181/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0307 - accuracy: 0.9866 - val_loss: 0.1067 - val_accuracy: 0.9683\n",
            "Epoch 182/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0328 - accuracy: 0.9860 - val_loss: 0.1107 - val_accuracy: 0.9697\n",
            "Epoch 183/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0325 - accuracy: 0.9856 - val_loss: 0.1256 - val_accuracy: 0.9670\n",
            "Epoch 184/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0336 - accuracy: 0.9837 - val_loss: 0.1131 - val_accuracy: 0.9692\n",
            "Epoch 185/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0352 - accuracy: 0.9837 - val_loss: 0.1076 - val_accuracy: 0.9688\n",
            "Epoch 186/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0341 - accuracy: 0.9844 - val_loss: 0.1123 - val_accuracy: 0.9701\n",
            "Epoch 187/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0323 - accuracy: 0.9854 - val_loss: 0.1221 - val_accuracy: 0.9711\n",
            "Epoch 188/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0336 - accuracy: 0.9848 - val_loss: 0.1151 - val_accuracy: 0.9697\n",
            "Epoch 189/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0340 - accuracy: 0.9854 - val_loss: 0.1080 - val_accuracy: 0.9643\n",
            "Epoch 190/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0326 - accuracy: 0.9850 - val_loss: 0.1151 - val_accuracy: 0.9665\n",
            "Epoch 191/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0316 - accuracy: 0.9851 - val_loss: 0.1106 - val_accuracy: 0.9701\n",
            "Epoch 192/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0337 - accuracy: 0.9849 - val_loss: 0.1146 - val_accuracy: 0.9679\n",
            "Epoch 193/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0320 - accuracy: 0.9854 - val_loss: 0.1164 - val_accuracy: 0.9679\n",
            "Epoch 194/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0351 - accuracy: 0.9838 - val_loss: 0.1128 - val_accuracy: 0.9701\n",
            "Epoch 195/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0326 - accuracy: 0.9859 - val_loss: 0.1074 - val_accuracy: 0.9711\n",
            "Epoch 196/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0313 - accuracy: 0.9861 - val_loss: 0.1133 - val_accuracy: 0.9720\n",
            "Epoch 197/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0332 - accuracy: 0.9861 - val_loss: 0.1174 - val_accuracy: 0.9661\n",
            "Epoch 198/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0332 - accuracy: 0.9837 - val_loss: 0.1163 - val_accuracy: 0.9697\n",
            "Epoch 199/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0334 - accuracy: 0.9847 - val_loss: 0.1140 - val_accuracy: 0.9711\n",
            "Epoch 200/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0313 - accuracy: 0.9862 - val_loss: 0.1153 - val_accuracy: 0.9674\n",
            "Epoch 201/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0341 - accuracy: 0.9848 - val_loss: 0.1185 - val_accuracy: 0.9692\n",
            "Epoch 202/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0339 - accuracy: 0.9853 - val_loss: 0.1169 - val_accuracy: 0.9701\n",
            "Epoch 203/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0316 - accuracy: 0.9861 - val_loss: 0.1126 - val_accuracy: 0.9674\n",
            "Epoch 204/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0325 - accuracy: 0.9841 - val_loss: 0.1128 - val_accuracy: 0.9661\n",
            "Epoch 205/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0345 - accuracy: 0.9828 - val_loss: 0.1246 - val_accuracy: 0.9733\n",
            "Epoch 206/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0307 - accuracy: 0.9855 - val_loss: 0.1106 - val_accuracy: 0.9692\n",
            "Epoch 207/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0322 - accuracy: 0.9855 - val_loss: 0.1138 - val_accuracy: 0.9692\n",
            "Epoch 208/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0304 - accuracy: 0.9857 - val_loss: 0.1123 - val_accuracy: 0.9720\n",
            "Epoch 209/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0335 - accuracy: 0.9845 - val_loss: 0.1017 - val_accuracy: 0.9688\n",
            "Epoch 210/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0303 - accuracy: 0.9864 - val_loss: 0.1142 - val_accuracy: 0.9697\n",
            "Epoch 211/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0293 - accuracy: 0.9867 - val_loss: 0.1170 - val_accuracy: 0.9688\n",
            "Epoch 212/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0333 - accuracy: 0.9848 - val_loss: 0.1122 - val_accuracy: 0.9706\n",
            "Epoch 213/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0343 - accuracy: 0.9833 - val_loss: 0.1043 - val_accuracy: 0.9747\n",
            "Epoch 214/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0308 - accuracy: 0.9863 - val_loss: 0.0997 - val_accuracy: 0.9683\n",
            "Epoch 215/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0325 - accuracy: 0.9852 - val_loss: 0.1021 - val_accuracy: 0.9715\n",
            "Epoch 216/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0320 - accuracy: 0.9853 - val_loss: 0.1089 - val_accuracy: 0.9729\n",
            "Epoch 217/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0306 - accuracy: 0.9862 - val_loss: 0.1099 - val_accuracy: 0.9729\n",
            "Epoch 218/500\n",
            "298/298 [==============================] - 3s 9ms/step - loss: 0.0322 - accuracy: 0.9854 - val_loss: 0.1084 - val_accuracy: 0.9692\n",
            "Epoch 219/500\n",
            "298/298 [==============================] - 3s 9ms/step - loss: 0.0297 - accuracy: 0.9848 - val_loss: 0.1089 - val_accuracy: 0.9701\n",
            "Epoch 220/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0303 - accuracy: 0.9871 - val_loss: 0.1082 - val_accuracy: 0.9733\n",
            "Epoch 221/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0326 - accuracy: 0.9855 - val_loss: 0.1120 - val_accuracy: 0.9747\n",
            "Epoch 222/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0317 - accuracy: 0.9857 - val_loss: 0.1182 - val_accuracy: 0.9711\n",
            "Epoch 223/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0322 - accuracy: 0.9862 - val_loss: 0.1176 - val_accuracy: 0.9742\n",
            "Epoch 224/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0314 - accuracy: 0.9860 - val_loss: 0.1132 - val_accuracy: 0.9683\n",
            "Epoch 225/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0327 - accuracy: 0.9841 - val_loss: 0.1141 - val_accuracy: 0.9697\n",
            "Epoch 226/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0327 - accuracy: 0.9853 - val_loss: 0.1107 - val_accuracy: 0.9715\n",
            "Epoch 227/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0306 - accuracy: 0.9862 - val_loss: 0.1087 - val_accuracy: 0.9688\n",
            "Epoch 228/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0306 - accuracy: 0.9864 - val_loss: 0.1121 - val_accuracy: 0.9706\n",
            "Epoch 229/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0307 - accuracy: 0.9842 - val_loss: 0.1145 - val_accuracy: 0.9720\n",
            "Epoch 230/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0309 - accuracy: 0.9861 - val_loss: 0.1102 - val_accuracy: 0.9688\n",
            "Epoch 231/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0302 - accuracy: 0.9851 - val_loss: 0.1063 - val_accuracy: 0.9652\n",
            "Epoch 232/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0308 - accuracy: 0.9856 - val_loss: 0.1145 - val_accuracy: 0.9733\n",
            "Epoch 233/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0330 - accuracy: 0.9853 - val_loss: 0.1049 - val_accuracy: 0.9729\n",
            "Epoch 234/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0301 - accuracy: 0.9850 - val_loss: 0.1059 - val_accuracy: 0.9715\n",
            "Epoch 235/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0310 - accuracy: 0.9854 - val_loss: 0.1179 - val_accuracy: 0.9715\n",
            "Epoch 236/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0302 - accuracy: 0.9868 - val_loss: 0.1139 - val_accuracy: 0.9701\n",
            "Epoch 237/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0297 - accuracy: 0.9861 - val_loss: 0.1174 - val_accuracy: 0.9670\n",
            "Epoch 238/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0325 - accuracy: 0.9852 - val_loss: 0.1129 - val_accuracy: 0.9692\n",
            "Epoch 239/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0321 - accuracy: 0.9859 - val_loss: 0.1044 - val_accuracy: 0.9724\n",
            "Epoch 240/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0290 - accuracy: 0.9859 - val_loss: 0.1137 - val_accuracy: 0.9720\n",
            "Epoch 241/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0304 - accuracy: 0.9856 - val_loss: 0.1144 - val_accuracy: 0.9701\n",
            "Epoch 242/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0308 - accuracy: 0.9860 - val_loss: 0.1121 - val_accuracy: 0.9701\n",
            "Epoch 243/500\n",
            "298/298 [==============================] - 3s 9ms/step - loss: 0.0304 - accuracy: 0.9856 - val_loss: 0.1079 - val_accuracy: 0.9711\n",
            "Epoch 244/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0300 - accuracy: 0.9856 - val_loss: 0.1170 - val_accuracy: 0.9733\n",
            "Epoch 245/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0330 - accuracy: 0.9849 - val_loss: 0.1060 - val_accuracy: 0.9688\n",
            "Epoch 246/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0295 - accuracy: 0.9860 - val_loss: 0.1133 - val_accuracy: 0.9715\n",
            "Epoch 247/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0293 - accuracy: 0.9853 - val_loss: 0.1170 - val_accuracy: 0.9711\n",
            "Epoch 248/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0298 - accuracy: 0.9857 - val_loss: 0.1158 - val_accuracy: 0.9706\n",
            "Epoch 249/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0290 - accuracy: 0.9856 - val_loss: 0.1137 - val_accuracy: 0.9715\n",
            "Epoch 250/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0310 - accuracy: 0.9862 - val_loss: 0.1152 - val_accuracy: 0.9692\n",
            "Epoch 251/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0300 - accuracy: 0.9862 - val_loss: 0.1151 - val_accuracy: 0.9711\n",
            "Epoch 252/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0290 - accuracy: 0.9852 - val_loss: 0.1108 - val_accuracy: 0.9711\n",
            "Epoch 253/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0307 - accuracy: 0.9873 - val_loss: 0.1150 - val_accuracy: 0.9688\n",
            "Epoch 254/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0300 - accuracy: 0.9850 - val_loss: 0.1108 - val_accuracy: 0.9697\n",
            "Epoch 255/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0320 - accuracy: 0.9862 - val_loss: 0.1181 - val_accuracy: 0.9679\n",
            "Epoch 256/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0296 - accuracy: 0.9857 - val_loss: 0.1159 - val_accuracy: 0.9683\n",
            "Epoch 257/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0283 - accuracy: 0.9880 - val_loss: 0.1230 - val_accuracy: 0.9706\n",
            "Epoch 258/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0279 - accuracy: 0.9874 - val_loss: 0.1132 - val_accuracy: 0.9715\n",
            "Epoch 259/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0290 - accuracy: 0.9853 - val_loss: 0.1114 - val_accuracy: 0.9711\n",
            "Epoch 260/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0274 - accuracy: 0.9868 - val_loss: 0.1129 - val_accuracy: 0.9706\n",
            "Epoch 261/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0300 - accuracy: 0.9870 - val_loss: 0.1139 - val_accuracy: 0.9697\n",
            "Epoch 262/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0284 - accuracy: 0.9878 - val_loss: 0.1170 - val_accuracy: 0.9715\n",
            "Epoch 263/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0326 - accuracy: 0.9858 - val_loss: 0.1053 - val_accuracy: 0.9706\n",
            "Epoch 264/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0310 - accuracy: 0.9849 - val_loss: 0.1101 - val_accuracy: 0.9674\n",
            "Epoch 265/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0291 - accuracy: 0.9869 - val_loss: 0.1098 - val_accuracy: 0.9751\n",
            "Epoch 266/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0288 - accuracy: 0.9867 - val_loss: 0.1171 - val_accuracy: 0.9697\n",
            "Epoch 267/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0282 - accuracy: 0.9877 - val_loss: 0.1150 - val_accuracy: 0.9706\n",
            "Epoch 268/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0303 - accuracy: 0.9857 - val_loss: 0.1112 - val_accuracy: 0.9688\n",
            "Epoch 269/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0289 - accuracy: 0.9856 - val_loss: 0.1108 - val_accuracy: 0.9711\n",
            "Epoch 270/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0292 - accuracy: 0.9861 - val_loss: 0.1115 - val_accuracy: 0.9688\n",
            "Epoch 271/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0319 - accuracy: 0.9849 - val_loss: 0.1084 - val_accuracy: 0.9688\n",
            "Epoch 272/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0279 - accuracy: 0.9881 - val_loss: 0.1127 - val_accuracy: 0.9692\n",
            "Epoch 273/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0298 - accuracy: 0.9869 - val_loss: 0.1270 - val_accuracy: 0.9674\n",
            "Epoch 274/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0289 - accuracy: 0.9860 - val_loss: 0.1226 - val_accuracy: 0.9697\n",
            "Epoch 275/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0305 - accuracy: 0.9856 - val_loss: 0.1125 - val_accuracy: 0.9715\n",
            "Epoch 276/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0284 - accuracy: 0.9861 - val_loss: 0.1173 - val_accuracy: 0.9656\n",
            "Epoch 277/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0274 - accuracy: 0.9876 - val_loss: 0.1207 - val_accuracy: 0.9701\n",
            "Epoch 278/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0273 - accuracy: 0.9855 - val_loss: 0.1167 - val_accuracy: 0.9692\n",
            "Epoch 279/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0275 - accuracy: 0.9861 - val_loss: 0.1123 - val_accuracy: 0.9697\n",
            "Epoch 280/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0309 - accuracy: 0.9853 - val_loss: 0.1127 - val_accuracy: 0.9706\n",
            "Epoch 281/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0292 - accuracy: 0.9875 - val_loss: 0.1148 - val_accuracy: 0.9724\n",
            "Epoch 282/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0299 - accuracy: 0.9860 - val_loss: 0.1149 - val_accuracy: 0.9724\n",
            "Epoch 283/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0293 - accuracy: 0.9866 - val_loss: 0.1102 - val_accuracy: 0.9697\n",
            "Epoch 284/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0281 - accuracy: 0.9881 - val_loss: 0.1174 - val_accuracy: 0.9711\n",
            "Epoch 285/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0277 - accuracy: 0.9871 - val_loss: 0.1101 - val_accuracy: 0.9733\n",
            "Epoch 286/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0287 - accuracy: 0.9868 - val_loss: 0.1110 - val_accuracy: 0.9706\n",
            "Epoch 287/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0285 - accuracy: 0.9865 - val_loss: 0.1112 - val_accuracy: 0.9697\n",
            "Epoch 288/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0282 - accuracy: 0.9864 - val_loss: 0.1193 - val_accuracy: 0.9697\n",
            "Epoch 289/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0291 - accuracy: 0.9860 - val_loss: 0.1120 - val_accuracy: 0.9692\n",
            "Epoch 290/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0270 - accuracy: 0.9871 - val_loss: 0.1193 - val_accuracy: 0.9711\n",
            "Epoch 291/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0290 - accuracy: 0.9857 - val_loss: 0.1208 - val_accuracy: 0.9688\n",
            "Epoch 292/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0285 - accuracy: 0.9864 - val_loss: 0.1184 - val_accuracy: 0.9697\n",
            "Epoch 293/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0255 - accuracy: 0.9877 - val_loss: 0.1254 - val_accuracy: 0.9729\n",
            "Epoch 294/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0279 - accuracy: 0.9874 - val_loss: 0.1205 - val_accuracy: 0.9715\n",
            "Epoch 295/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0278 - accuracy: 0.9870 - val_loss: 0.1177 - val_accuracy: 0.9674\n",
            "Epoch 296/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0288 - accuracy: 0.9861 - val_loss: 0.1169 - val_accuracy: 0.9706\n",
            "Epoch 297/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0281 - accuracy: 0.9869 - val_loss: 0.1192 - val_accuracy: 0.9720\n",
            "Epoch 298/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0275 - accuracy: 0.9867 - val_loss: 0.1186 - val_accuracy: 0.9688\n",
            "Epoch 299/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0276 - accuracy: 0.9850 - val_loss: 0.1267 - val_accuracy: 0.9733\n",
            "Epoch 300/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0282 - accuracy: 0.9868 - val_loss: 0.1182 - val_accuracy: 0.9733\n",
            "Epoch 301/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0269 - accuracy: 0.9866 - val_loss: 0.1242 - val_accuracy: 0.9715\n",
            "Epoch 302/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0275 - accuracy: 0.9877 - val_loss: 0.1217 - val_accuracy: 0.9697\n",
            "Epoch 303/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0274 - accuracy: 0.9868 - val_loss: 0.1179 - val_accuracy: 0.9706\n",
            "Epoch 304/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0272 - accuracy: 0.9870 - val_loss: 0.1153 - val_accuracy: 0.9747\n",
            "Epoch 305/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0280 - accuracy: 0.9863 - val_loss: 0.1264 - val_accuracy: 0.9724\n",
            "Epoch 306/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0277 - accuracy: 0.9866 - val_loss: 0.1219 - val_accuracy: 0.9706\n",
            "Epoch 307/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0290 - accuracy: 0.9878 - val_loss: 0.1218 - val_accuracy: 0.9688\n",
            "Epoch 308/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0279 - accuracy: 0.9862 - val_loss: 0.1237 - val_accuracy: 0.9733\n",
            "Epoch 309/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0271 - accuracy: 0.9873 - val_loss: 0.1231 - val_accuracy: 0.9697\n",
            "Epoch 310/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0276 - accuracy: 0.9867 - val_loss: 0.1172 - val_accuracy: 0.9701\n",
            "Epoch 311/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0259 - accuracy: 0.9880 - val_loss: 0.1204 - val_accuracy: 0.9688\n",
            "Epoch 312/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0286 - accuracy: 0.9858 - val_loss: 0.1114 - val_accuracy: 0.9697\n",
            "Epoch 313/500\n",
            "298/298 [==============================] - 3s 8ms/step - loss: 0.0279 - accuracy: 0.9871 - val_loss: 0.1170 - val_accuracy: 0.9701\n",
            "Epoch 314/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0266 - accuracy: 0.9883 - val_loss: 0.1173 - val_accuracy: 0.9729\n",
            "Epoch 315/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0286 - accuracy: 0.9866 - val_loss: 0.1167 - val_accuracy: 0.9733\n",
            "Epoch 316/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0274 - accuracy: 0.9870 - val_loss: 0.1130 - val_accuracy: 0.9711\n",
            "Epoch 317/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0268 - accuracy: 0.9868 - val_loss: 0.1164 - val_accuracy: 0.9738\n",
            "Epoch 318/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0279 - accuracy: 0.9863 - val_loss: 0.1158 - val_accuracy: 0.9701\n",
            "Epoch 319/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0283 - accuracy: 0.9870 - val_loss: 0.1235 - val_accuracy: 0.9706\n",
            "Epoch 320/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0268 - accuracy: 0.9871 - val_loss: 0.1206 - val_accuracy: 0.9701\n",
            "Epoch 321/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0274 - accuracy: 0.9875 - val_loss: 0.1157 - val_accuracy: 0.9711\n",
            "Epoch 322/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0288 - accuracy: 0.9857 - val_loss: 0.1246 - val_accuracy: 0.9697\n",
            "Epoch 323/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0269 - accuracy: 0.9853 - val_loss: 0.1234 - val_accuracy: 0.9683\n",
            "Epoch 324/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0248 - accuracy: 0.9893 - val_loss: 0.1147 - val_accuracy: 0.9729\n",
            "Epoch 325/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0274 - accuracy: 0.9865 - val_loss: 0.1156 - val_accuracy: 0.9715\n",
            "Epoch 326/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0267 - accuracy: 0.9870 - val_loss: 0.1174 - val_accuracy: 0.9697\n",
            "Epoch 327/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0280 - accuracy: 0.9865 - val_loss: 0.1183 - val_accuracy: 0.9697\n",
            "Epoch 328/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0278 - accuracy: 0.9868 - val_loss: 0.1209 - val_accuracy: 0.9701\n",
            "Epoch 329/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0292 - accuracy: 0.9860 - val_loss: 0.1142 - val_accuracy: 0.9688\n",
            "Epoch 330/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0262 - accuracy: 0.9881 - val_loss: 0.1229 - val_accuracy: 0.9720\n",
            "Epoch 331/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0270 - accuracy: 0.9877 - val_loss: 0.1308 - val_accuracy: 0.9674\n",
            "Epoch 332/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0285 - accuracy: 0.9869 - val_loss: 0.1294 - val_accuracy: 0.9701\n",
            "Epoch 333/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0282 - accuracy: 0.9862 - val_loss: 0.1248 - val_accuracy: 0.9706\n",
            "Epoch 334/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0268 - accuracy: 0.9873 - val_loss: 0.1263 - val_accuracy: 0.9683\n",
            "Epoch 335/500\n",
            "298/298 [==============================] - 3s 8ms/step - loss: 0.0274 - accuracy: 0.9870 - val_loss: 0.1189 - val_accuracy: 0.9697\n",
            "Epoch 336/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0263 - accuracy: 0.9877 - val_loss: 0.1228 - val_accuracy: 0.9679\n",
            "Epoch 337/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0262 - accuracy: 0.9874 - val_loss: 0.1230 - val_accuracy: 0.9724\n",
            "Epoch 338/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0257 - accuracy: 0.9881 - val_loss: 0.1260 - val_accuracy: 0.9720\n",
            "Epoch 339/500\n",
            "298/298 [==============================] - 3s 9ms/step - loss: 0.0282 - accuracy: 0.9858 - val_loss: 0.1219 - val_accuracy: 0.9738\n",
            "Epoch 340/500\n",
            "298/298 [==============================] - 3s 8ms/step - loss: 0.0260 - accuracy: 0.9883 - val_loss: 0.1340 - val_accuracy: 0.9715\n",
            "Epoch 341/500\n",
            "298/298 [==============================] - 3s 8ms/step - loss: 0.0253 - accuracy: 0.9881 - val_loss: 0.1263 - val_accuracy: 0.9733\n",
            "Epoch 342/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0282 - accuracy: 0.9863 - val_loss: 0.1353 - val_accuracy: 0.9701\n",
            "Epoch 343/500\n",
            "298/298 [==============================] - 3s 9ms/step - loss: 0.0285 - accuracy: 0.9868 - val_loss: 0.1227 - val_accuracy: 0.9711\n",
            "Epoch 344/500\n",
            "298/298 [==============================] - 3s 8ms/step - loss: 0.0266 - accuracy: 0.9864 - val_loss: 0.1245 - val_accuracy: 0.9706\n",
            "Epoch 345/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0274 - accuracy: 0.9862 - val_loss: 0.1233 - val_accuracy: 0.9711\n",
            "Epoch 346/500\n",
            "298/298 [==============================] - 3s 9ms/step - loss: 0.0284 - accuracy: 0.9861 - val_loss: 0.1214 - val_accuracy: 0.9697\n",
            "Epoch 347/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0265 - accuracy: 0.9875 - val_loss: 0.1259 - val_accuracy: 0.9692\n",
            "Epoch 348/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0249 - accuracy: 0.9874 - val_loss: 0.1305 - val_accuracy: 0.9715\n",
            "Epoch 349/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0279 - accuracy: 0.9885 - val_loss: 0.1267 - val_accuracy: 0.9674\n",
            "Epoch 350/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0285 - accuracy: 0.9868 - val_loss: 0.1251 - val_accuracy: 0.9720\n",
            "Epoch 351/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0273 - accuracy: 0.9854 - val_loss: 0.1258 - val_accuracy: 0.9733\n",
            "Epoch 352/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0260 - accuracy: 0.9869 - val_loss: 0.1213 - val_accuracy: 0.9701\n",
            "Epoch 353/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0256 - accuracy: 0.9868 - val_loss: 0.1303 - val_accuracy: 0.9697\n",
            "Epoch 354/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0253 - accuracy: 0.9883 - val_loss: 0.1284 - val_accuracy: 0.9670\n",
            "Epoch 355/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0265 - accuracy: 0.9865 - val_loss: 0.1213 - val_accuracy: 0.9706\n",
            "Epoch 356/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0255 - accuracy: 0.9875 - val_loss: 0.1293 - val_accuracy: 0.9683\n",
            "Epoch 357/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0242 - accuracy: 0.9887 - val_loss: 0.1255 - val_accuracy: 0.9729\n",
            "Epoch 358/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0272 - accuracy: 0.9860 - val_loss: 0.1190 - val_accuracy: 0.9742\n",
            "Epoch 359/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0267 - accuracy: 0.9880 - val_loss: 0.1277 - val_accuracy: 0.9729\n",
            "Epoch 360/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0258 - accuracy: 0.9875 - val_loss: 0.1254 - val_accuracy: 0.9711\n",
            "Epoch 361/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0255 - accuracy: 0.9874 - val_loss: 0.1300 - val_accuracy: 0.9724\n",
            "Epoch 362/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0245 - accuracy: 0.9881 - val_loss: 0.1217 - val_accuracy: 0.9711\n",
            "Epoch 363/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0266 - accuracy: 0.9868 - val_loss: 0.1307 - val_accuracy: 0.9679\n",
            "Epoch 364/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0267 - accuracy: 0.9870 - val_loss: 0.1278 - val_accuracy: 0.9701\n",
            "Epoch 365/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0267 - accuracy: 0.9871 - val_loss: 0.1281 - val_accuracy: 0.9711\n",
            "Epoch 366/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0254 - accuracy: 0.9883 - val_loss: 0.1286 - val_accuracy: 0.9720\n",
            "Epoch 367/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0241 - accuracy: 0.9880 - val_loss: 0.1360 - val_accuracy: 0.9701\n",
            "Epoch 368/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0243 - accuracy: 0.9881 - val_loss: 0.1457 - val_accuracy: 0.9697\n",
            "Epoch 369/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0275 - accuracy: 0.9864 - val_loss: 0.1271 - val_accuracy: 0.9706\n",
            "Epoch 370/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0275 - accuracy: 0.9879 - val_loss: 0.1353 - val_accuracy: 0.9706\n",
            "Epoch 371/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0287 - accuracy: 0.9865 - val_loss: 0.1298 - val_accuracy: 0.9715\n",
            "Epoch 372/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0253 - accuracy: 0.9875 - val_loss: 0.1391 - val_accuracy: 0.9665\n",
            "Epoch 373/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0254 - accuracy: 0.9867 - val_loss: 0.1338 - val_accuracy: 0.9679\n",
            "Epoch 374/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0252 - accuracy: 0.9869 - val_loss: 0.1289 - val_accuracy: 0.9715\n",
            "Epoch 375/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0242 - accuracy: 0.9884 - val_loss: 0.1309 - val_accuracy: 0.9715\n",
            "Epoch 376/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0246 - accuracy: 0.9867 - val_loss: 0.1317 - val_accuracy: 0.9720\n",
            "Epoch 377/500\n",
            "298/298 [==============================] - 3s 8ms/step - loss: 0.0273 - accuracy: 0.9869 - val_loss: 0.1367 - val_accuracy: 0.9692\n",
            "Epoch 378/500\n",
            "298/298 [==============================] - 3s 8ms/step - loss: 0.0258 - accuracy: 0.9873 - val_loss: 0.1369 - val_accuracy: 0.9692\n",
            "Epoch 379/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0245 - accuracy: 0.9876 - val_loss: 0.1382 - val_accuracy: 0.9706\n",
            "Epoch 380/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0285 - accuracy: 0.9873 - val_loss: 0.1194 - val_accuracy: 0.9692\n",
            "Epoch 381/500\n",
            "298/298 [==============================] - 3s 8ms/step - loss: 0.0243 - accuracy: 0.9868 - val_loss: 0.1236 - val_accuracy: 0.9738\n",
            "Epoch 382/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0247 - accuracy: 0.9880 - val_loss: 0.1331 - val_accuracy: 0.9692\n",
            "Epoch 383/500\n",
            "298/298 [==============================] - 3s 9ms/step - loss: 0.0252 - accuracy: 0.9879 - val_loss: 0.1353 - val_accuracy: 0.9720\n",
            "Epoch 384/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0234 - accuracy: 0.9880 - val_loss: 0.1333 - val_accuracy: 0.9715\n",
            "Epoch 385/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0257 - accuracy: 0.9884 - val_loss: 0.1336 - val_accuracy: 0.9724\n",
            "Epoch 386/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0254 - accuracy: 0.9878 - val_loss: 0.1315 - val_accuracy: 0.9724\n",
            "Epoch 387/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0252 - accuracy: 0.9873 - val_loss: 0.1324 - val_accuracy: 0.9715\n",
            "Epoch 388/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0266 - accuracy: 0.9879 - val_loss: 0.1296 - val_accuracy: 0.9683\n",
            "Epoch 389/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0258 - accuracy: 0.9874 - val_loss: 0.1319 - val_accuracy: 0.9711\n",
            "Epoch 390/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0254 - accuracy: 0.9878 - val_loss: 0.1301 - val_accuracy: 0.9701\n",
            "Epoch 391/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0264 - accuracy: 0.9860 - val_loss: 0.1346 - val_accuracy: 0.9706\n",
            "Epoch 392/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0251 - accuracy: 0.9880 - val_loss: 0.1351 - val_accuracy: 0.9688\n",
            "Epoch 393/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0259 - accuracy: 0.9877 - val_loss: 0.1362 - val_accuracy: 0.9692\n",
            "Epoch 394/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0241 - accuracy: 0.9885 - val_loss: 0.1338 - val_accuracy: 0.9683\n",
            "Epoch 395/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0261 - accuracy: 0.9870 - val_loss: 0.1202 - val_accuracy: 0.9711\n",
            "Epoch 396/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0231 - accuracy: 0.9883 - val_loss: 0.1288 - val_accuracy: 0.9706\n",
            "Epoch 397/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0243 - accuracy: 0.9875 - val_loss: 0.1262 - val_accuracy: 0.9688\n",
            "Epoch 398/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0261 - accuracy: 0.9887 - val_loss: 0.1235 - val_accuracy: 0.9706\n",
            "Epoch 399/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0240 - accuracy: 0.9876 - val_loss: 0.1280 - val_accuracy: 0.9688\n",
            "Epoch 400/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0254 - accuracy: 0.9873 - val_loss: 0.1342 - val_accuracy: 0.9670\n",
            "Epoch 401/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0261 - accuracy: 0.9873 - val_loss: 0.1188 - val_accuracy: 0.9715\n",
            "Epoch 402/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0253 - accuracy: 0.9874 - val_loss: 0.1225 - val_accuracy: 0.9751\n",
            "Epoch 403/500\n",
            "298/298 [==============================] - 3s 9ms/step - loss: 0.0235 - accuracy: 0.9884 - val_loss: 0.1284 - val_accuracy: 0.9733\n",
            "Epoch 404/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0253 - accuracy: 0.9871 - val_loss: 0.1293 - val_accuracy: 0.9679\n",
            "Epoch 405/500\n",
            "298/298 [==============================] - 3s 10ms/step - loss: 0.0271 - accuracy: 0.9870 - val_loss: 0.1261 - val_accuracy: 0.9711\n",
            "Epoch 406/500\n",
            "298/298 [==============================] - 3s 9ms/step - loss: 0.0264 - accuracy: 0.9879 - val_loss: 0.1271 - val_accuracy: 0.9715\n",
            "Epoch 407/500\n",
            "298/298 [==============================] - 3s 9ms/step - loss: 0.0247 - accuracy: 0.9864 - val_loss: 0.1253 - val_accuracy: 0.9715\n",
            "Epoch 408/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0251 - accuracy: 0.9875 - val_loss: 0.1217 - val_accuracy: 0.9711\n",
            "Epoch 409/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0244 - accuracy: 0.9880 - val_loss: 0.1303 - val_accuracy: 0.9733\n",
            "Epoch 410/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0255 - accuracy: 0.9879 - val_loss: 0.1293 - val_accuracy: 0.9720\n",
            "Epoch 411/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0270 - accuracy: 0.9862 - val_loss: 0.1191 - val_accuracy: 0.9720\n",
            "Epoch 412/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0247 - accuracy: 0.9874 - val_loss: 0.1224 - val_accuracy: 0.9729\n",
            "Epoch 413/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0251 - accuracy: 0.9874 - val_loss: 0.1349 - val_accuracy: 0.9706\n",
            "Epoch 414/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0242 - accuracy: 0.9877 - val_loss: 0.1272 - val_accuracy: 0.9701\n",
            "Epoch 415/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0245 - accuracy: 0.9884 - val_loss: 0.1344 - val_accuracy: 0.9701\n",
            "Epoch 416/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0249 - accuracy: 0.9875 - val_loss: 0.1344 - val_accuracy: 0.9715\n",
            "Epoch 417/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0251 - accuracy: 0.9870 - val_loss: 0.1345 - val_accuracy: 0.9724\n",
            "Epoch 418/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0236 - accuracy: 0.9882 - val_loss: 0.1298 - val_accuracy: 0.9711\n",
            "Epoch 419/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0251 - accuracy: 0.9865 - val_loss: 0.1436 - val_accuracy: 0.9720\n",
            "Epoch 420/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0259 - accuracy: 0.9878 - val_loss: 0.1374 - val_accuracy: 0.9674\n",
            "Epoch 421/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0251 - accuracy: 0.9879 - val_loss: 0.1298 - val_accuracy: 0.9679\n",
            "Epoch 422/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0244 - accuracy: 0.9875 - val_loss: 0.1345 - val_accuracy: 0.9674\n",
            "Epoch 423/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0256 - accuracy: 0.9882 - val_loss: 0.1300 - val_accuracy: 0.9706\n",
            "Epoch 424/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0260 - accuracy: 0.9865 - val_loss: 0.1224 - val_accuracy: 0.9711\n",
            "Epoch 425/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0241 - accuracy: 0.9882 - val_loss: 0.1349 - val_accuracy: 0.9715\n",
            "Epoch 426/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0262 - accuracy: 0.9862 - val_loss: 0.1238 - val_accuracy: 0.9724\n",
            "Epoch 427/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0232 - accuracy: 0.9888 - val_loss: 0.1268 - val_accuracy: 0.9715\n",
            "Epoch 428/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0253 - accuracy: 0.9874 - val_loss: 0.1313 - val_accuracy: 0.9701\n",
            "Epoch 429/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0244 - accuracy: 0.9879 - val_loss: 0.1265 - val_accuracy: 0.9733\n",
            "Epoch 430/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0239 - accuracy: 0.9887 - val_loss: 0.1315 - val_accuracy: 0.9688\n",
            "Epoch 431/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0270 - accuracy: 0.9866 - val_loss: 0.1192 - val_accuracy: 0.9720\n",
            "Epoch 432/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0248 - accuracy: 0.9879 - val_loss: 0.1250 - val_accuracy: 0.9738\n",
            "Epoch 433/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0232 - accuracy: 0.9884 - val_loss: 0.1259 - val_accuracy: 0.9715\n",
            "Epoch 434/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0270 - accuracy: 0.9881 - val_loss: 0.1288 - val_accuracy: 0.9720\n",
            "Epoch 435/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0238 - accuracy: 0.9879 - val_loss: 0.1176 - val_accuracy: 0.9733\n",
            "Epoch 436/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0248 - accuracy: 0.9882 - val_loss: 0.1371 - val_accuracy: 0.9688\n",
            "Epoch 437/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0245 - accuracy: 0.9887 - val_loss: 0.1233 - val_accuracy: 0.9715\n",
            "Epoch 438/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0243 - accuracy: 0.9880 - val_loss: 0.1308 - val_accuracy: 0.9720\n",
            "Epoch 439/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0248 - accuracy: 0.9884 - val_loss: 0.1289 - val_accuracy: 0.9692\n",
            "Epoch 440/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0250 - accuracy: 0.9870 - val_loss: 0.1298 - val_accuracy: 0.9688\n",
            "Epoch 441/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0243 - accuracy: 0.9881 - val_loss: 0.1274 - val_accuracy: 0.9720\n",
            "Epoch 442/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0232 - accuracy: 0.9888 - val_loss: 0.1357 - val_accuracy: 0.9697\n",
            "Epoch 443/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0231 - accuracy: 0.9878 - val_loss: 0.1340 - val_accuracy: 0.9697\n",
            "Epoch 444/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0248 - accuracy: 0.9876 - val_loss: 0.1277 - val_accuracy: 0.9715\n",
            "Epoch 445/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0259 - accuracy: 0.9874 - val_loss: 0.1281 - val_accuracy: 0.9674\n",
            "Epoch 446/500\n",
            "298/298 [==============================] - 3s 9ms/step - loss: 0.0254 - accuracy: 0.9877 - val_loss: 0.1284 - val_accuracy: 0.9720\n",
            "Epoch 447/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0248 - accuracy: 0.9873 - val_loss: 0.1326 - val_accuracy: 0.9724\n",
            "Epoch 448/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0236 - accuracy: 0.9875 - val_loss: 0.1274 - val_accuracy: 0.9729\n",
            "Epoch 449/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0240 - accuracy: 0.9873 - val_loss: 0.1329 - val_accuracy: 0.9683\n",
            "Epoch 450/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0239 - accuracy: 0.9882 - val_loss: 0.1468 - val_accuracy: 0.9670\n",
            "Epoch 451/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0267 - accuracy: 0.9871 - val_loss: 0.1311 - val_accuracy: 0.9711\n",
            "Epoch 452/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0245 - accuracy: 0.9866 - val_loss: 0.1359 - val_accuracy: 0.9661\n",
            "Epoch 453/500\n",
            "298/298 [==============================] - 3s 9ms/step - loss: 0.0240 - accuracy: 0.9873 - val_loss: 0.1402 - val_accuracy: 0.9711\n",
            "Epoch 454/500\n",
            "298/298 [==============================] - 3s 9ms/step - loss: 0.0237 - accuracy: 0.9882 - val_loss: 0.1357 - val_accuracy: 0.9706\n",
            "Epoch 455/500\n",
            "298/298 [==============================] - 3s 9ms/step - loss: 0.0241 - accuracy: 0.9884 - val_loss: 0.1353 - val_accuracy: 0.9697\n",
            "Epoch 456/500\n",
            "298/298 [==============================] - 3s 9ms/step - loss: 0.0255 - accuracy: 0.9861 - val_loss: 0.1349 - val_accuracy: 0.9715\n",
            "Epoch 457/500\n",
            "298/298 [==============================] - 3s 9ms/step - loss: 0.0238 - accuracy: 0.9869 - val_loss: 0.1286 - val_accuracy: 0.9711\n",
            "Epoch 458/500\n",
            "298/298 [==============================] - 3s 9ms/step - loss: 0.0257 - accuracy: 0.9879 - val_loss: 0.1298 - val_accuracy: 0.9706\n",
            "Epoch 459/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0235 - accuracy: 0.9880 - val_loss: 0.1329 - val_accuracy: 0.9697\n",
            "Epoch 460/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0260 - accuracy: 0.9878 - val_loss: 0.1326 - val_accuracy: 0.9724\n",
            "Epoch 461/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0246 - accuracy: 0.9875 - val_loss: 0.1279 - val_accuracy: 0.9711\n",
            "Epoch 462/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0225 - accuracy: 0.9881 - val_loss: 0.1348 - val_accuracy: 0.9692\n",
            "Epoch 463/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0250 - accuracy: 0.9877 - val_loss: 0.1282 - val_accuracy: 0.9706\n",
            "Epoch 464/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0236 - accuracy: 0.9876 - val_loss: 0.1320 - val_accuracy: 0.9697\n",
            "Epoch 465/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0218 - accuracy: 0.9891 - val_loss: 0.1358 - val_accuracy: 0.9715\n",
            "Epoch 466/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0231 - accuracy: 0.9876 - val_loss: 0.1338 - val_accuracy: 0.9720\n",
            "Epoch 467/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0233 - accuracy: 0.9883 - val_loss: 0.1333 - val_accuracy: 0.9697\n",
            "Epoch 468/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0244 - accuracy: 0.9879 - val_loss: 0.1412 - val_accuracy: 0.9697\n",
            "Epoch 469/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0251 - accuracy: 0.9868 - val_loss: 0.1332 - val_accuracy: 0.9688\n",
            "Epoch 470/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0264 - accuracy: 0.9866 - val_loss: 0.1265 - val_accuracy: 0.9679\n",
            "Epoch 471/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0234 - accuracy: 0.9877 - val_loss: 0.1338 - val_accuracy: 0.9697\n",
            "Epoch 472/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0246 - accuracy: 0.9892 - val_loss: 0.1381 - val_accuracy: 0.9724\n",
            "Epoch 473/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0235 - accuracy: 0.9887 - val_loss: 0.1414 - val_accuracy: 0.9674\n",
            "Epoch 474/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0240 - accuracy: 0.9881 - val_loss: 0.1357 - val_accuracy: 0.9674\n",
            "Epoch 475/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0238 - accuracy: 0.9884 - val_loss: 0.1349 - val_accuracy: 0.9670\n",
            "Epoch 476/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0242 - accuracy: 0.9877 - val_loss: 0.1303 - val_accuracy: 0.9688\n",
            "Epoch 477/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0237 - accuracy: 0.9890 - val_loss: 0.1390 - val_accuracy: 0.9670\n",
            "Epoch 478/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0232 - accuracy: 0.9879 - val_loss: 0.1308 - val_accuracy: 0.9720\n",
            "Epoch 479/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0252 - accuracy: 0.9864 - val_loss: 0.1366 - val_accuracy: 0.9720\n",
            "Epoch 480/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0232 - accuracy: 0.9885 - val_loss: 0.1263 - val_accuracy: 0.9715\n",
            "Epoch 481/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0263 - accuracy: 0.9870 - val_loss: 0.1273 - val_accuracy: 0.9711\n",
            "Epoch 482/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0235 - accuracy: 0.9879 - val_loss: 0.1308 - val_accuracy: 0.9706\n",
            "Epoch 483/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0241 - accuracy: 0.9882 - val_loss: 0.1345 - val_accuracy: 0.9729\n",
            "Epoch 484/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0243 - accuracy: 0.9875 - val_loss: 0.1327 - val_accuracy: 0.9751\n",
            "Epoch 485/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0245 - accuracy: 0.9874 - val_loss: 0.1269 - val_accuracy: 0.9720\n",
            "Epoch 486/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0242 - accuracy: 0.9881 - val_loss: 0.1369 - val_accuracy: 0.9711\n",
            "Epoch 487/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0237 - accuracy: 0.9879 - val_loss: 0.1297 - val_accuracy: 0.9720\n",
            "Epoch 488/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0237 - accuracy: 0.9879 - val_loss: 0.1323 - val_accuracy: 0.9733\n",
            "Epoch 489/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0238 - accuracy: 0.9886 - val_loss: 0.1388 - val_accuracy: 0.9733\n",
            "Epoch 490/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0242 - accuracy: 0.9887 - val_loss: 0.1361 - val_accuracy: 0.9720\n",
            "Epoch 491/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0261 - accuracy: 0.9868 - val_loss: 0.1438 - val_accuracy: 0.9683\n",
            "Epoch 492/500\n",
            "298/298 [==============================] - 2s 8ms/step - loss: 0.0234 - accuracy: 0.9875 - val_loss: 0.1427 - val_accuracy: 0.9701\n",
            "Epoch 493/500\n",
            "298/298 [==============================] - 3s 9ms/step - loss: 0.0229 - accuracy: 0.9883 - val_loss: 0.1310 - val_accuracy: 0.9715\n",
            "Epoch 494/500\n",
            "298/298 [==============================] - 3s 10ms/step - loss: 0.0242 - accuracy: 0.9881 - val_loss: 0.1337 - val_accuracy: 0.9697\n",
            "Epoch 495/500\n",
            "298/298 [==============================] - 3s 9ms/step - loss: 0.0231 - accuracy: 0.9886 - val_loss: 0.1355 - val_accuracy: 0.9692\n",
            "Epoch 496/500\n",
            "298/298 [==============================] - 3s 9ms/step - loss: 0.0236 - accuracy: 0.9884 - val_loss: 0.1474 - val_accuracy: 0.9665\n",
            "Epoch 497/500\n",
            "298/298 [==============================] - 3s 8ms/step - loss: 0.0257 - accuracy: 0.9867 - val_loss: 0.1275 - val_accuracy: 0.9720\n",
            "Epoch 498/500\n",
            "298/298 [==============================] - 3s 8ms/step - loss: 0.0243 - accuracy: 0.9882 - val_loss: 0.1295 - val_accuracy: 0.9683\n",
            "Epoch 499/500\n",
            "298/298 [==============================] - 3s 9ms/step - loss: 0.0235 - accuracy: 0.9878 - val_loss: 0.1301 - val_accuracy: 0.9692\n",
            "Epoch 500/500\n",
            "298/298 [==============================] - 3s 8ms/step - loss: 0.0242 - accuracy: 0.9874 - val_loss: 0.1261 - val_accuracy: 0.9692\n",
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " gru (GRU)                   (None, 1, 64)             18432     \n",
            "                                                                 \n",
            " dropout_10 (Dropout)        (None, 1, 64)             0         \n",
            "                                                                 \n",
            " gru_1 (GRU)                 (None, 32)                9408      \n",
            "                                                                 \n",
            " dropout_11 (Dropout)        (None, 32)                0         \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 27,873\n",
            "Trainable params: 27,873\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Define the model architecture\n",
        "model_gru = Sequential()\n",
        "model_gru.add(GRU(64, input_shape=(train_data_3d.shape[1], train_data_3d.shape[2]), return_sequences=True))\n",
        "model_gru.add(Dropout(0.2))\n",
        "model_gru.add(GRU(32))\n",
        "model_gru.add(Dropout(0.2))\n",
        "model_gru.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "model_gru.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model_gru.fit(train_data_3d, train_labels, validation_data=(test_data_3d, test_labels), epochs=500, batch_size=33)\n",
        "model_gru.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXslNVQ2wGky"
      },
      "source": [
        "# Evaluating model performance using evaluation metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "C9EkOi6gwGky",
        "outputId": "9bbfad4d-22b7-4ab4-9170-cc0db2cf4b78"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Confusion matrix of the model lr : \n",
            "[[ 868   88]\n",
            " [  85 1170]]\n",
            "True Negative of the model lr -> 868\n",
            "False Positive of the model lr -> 88\n",
            "False Negative of the model lr -> 85\n",
            "True Positive of the model lr -> 1170\n",
            "Accuracy of the model lr -> 0.9217548620533695\n",
            "Misclassification rate of the model lr -> 0.07824513794663046\n",
            "Precision of the model lr -> 0.9300476947535771\n",
            "Recall of the model lr -> 0.9322709163346613\n",
            "F1 Measure of the model lr -> 0.931157978511739\n",
            "\n",
            "\n",
            "Confusion matrix of the model nb : \n",
            "[[955   1]\n",
            " [909 346]]\n",
            "True Negative of the model nb -> 955\n",
            "False Positive of the model nb -> 1\n",
            "False Negative of the model nb -> 909\n",
            "True Positive of the model nb -> 346\n",
            "Accuracy of the model nb -> 0.5884215287200362\n",
            "Misclassification rate of the model nb -> 0.41157847127996383\n",
            "Precision of the model nb -> 0.9971181556195965\n",
            "Recall of the model nb -> 0.2756972111553785\n",
            "F1 Measure of the model nb -> 0.4319600499375781\n",
            "\n",
            "\n",
            "Confusion matrix of the model dt : \n",
            "[[ 865   91]\n",
            " [  79 1176]]\n",
            "True Negative of the model dt -> 865\n",
            "False Positive of the model dt -> 91\n",
            "False Negative of the model dt -> 79\n",
            "True Positive of the model dt -> 1176\n",
            "Accuracy of the model dt -> 0.9231117141564903\n",
            "Misclassification rate of the model dt -> 0.07688828584350971\n",
            "Precision of the model dt -> 0.9281767955801105\n",
            "Recall of the model dt -> 0.9370517928286852\n",
            "F1 Measure of the model dt -> 0.9325931800158604\n",
            "\n",
            "\n",
            "Confusion matrix of the model svm : \n",
            "[[ 864   92]\n",
            " [  72 1183]]\n",
            "True Negative of the model svm -> 864\n",
            "False Positive of the model svm -> 92\n",
            "False Negative of the model svm -> 72\n",
            "True Positive of the model svm -> 1183\n",
            "Accuracy of the model svm -> 0.9258254183627318\n",
            "Misclassification rate of the model svm -> 0.0741745816372682\n",
            "Precision of the model svm -> 0.927843137254902\n",
            "Recall of the model svm -> 0.9426294820717132\n",
            "F1 Measure of the model svm -> 0.9351778656126482\n",
            "\n",
            "\n",
            "Confusion matrix of the model knn : \n",
            "[[ 900   56]\n",
            " [ 109 1146]]\n",
            "True Negative of the model knn -> 900\n",
            "False Positive of the model knn -> 56\n",
            "False Negative of the model knn -> 109\n",
            "True Positive of the model knn -> 1146\n",
            "Accuracy of the model knn -> 0.9253731343283582\n",
            "Misclassification rate of the model knn -> 0.07462686567164178\n",
            "Precision of the model knn -> 0.9534109816971714\n",
            "Recall of the model knn -> 0.9131474103585657\n",
            "F1 Measure of the model knn -> 0.9328449328449329\n",
            "\n",
            "\n",
            "Confusion matrix of the model rf : \n",
            "[[ 863   93]\n",
            " [  63 1192]]\n",
            "True Negative of the model rf -> 863\n",
            "False Positive of the model rf -> 93\n",
            "False Negative of the model rf -> 63\n",
            "True Positive of the model rf -> 1192\n",
            "Accuracy of the model rf -> 0.9294436906377205\n",
            "Misclassification rate of the model rf -> 0.07055630936227952\n",
            "Precision of the model rf -> 0.9276264591439689\n",
            "Recall of the model rf -> 0.949800796812749\n",
            "F1 Measure of the model rf -> 0.9385826771653544\n",
            "\n",
            "\n",
            "Confusion matrix of the model bagging : \n",
            "[[ 896   60]\n",
            " [  95 1160]]\n",
            "True Negative of the model bagging -> 896\n",
            "False Positive of the model bagging -> 60\n",
            "False Negative of the model bagging -> 95\n",
            "True Positive of the model bagging -> 1160\n",
            "Accuracy of the model bagging -> 0.9298959746720941\n",
            "Misclassification rate of the model bagging -> 0.07010402532790594\n",
            "Precision of the model bagging -> 0.9508196721311475\n",
            "Recall of the model bagging -> 0.9243027888446215\n",
            "F1 Measure of the model bagging -> 0.9373737373737373\n",
            "\n",
            "\n",
            "Confusion matrix of the model xgb : \n",
            "[[ 918   38]\n",
            " [  29 1226]]\n",
            "True Negative of the model xgb -> 918\n",
            "False Positive of the model xgb -> 38\n",
            "False Negative of the model xgb -> 29\n",
            "True Positive of the model xgb -> 1226\n",
            "Accuracy of the model xgb -> 0.9696969696969697\n",
            "Misclassification rate of the model xgb -> 0.030303030303030276\n",
            "Precision of the model xgb -> 0.9699367088607594\n",
            "Recall of the model xgb -> 0.9768924302788845\n",
            "F1 Measure of the model xgb -> 0.9734021437078206\n",
            "\n",
            "\n",
            "Confusion matrix of the model ada : \n",
            "[[ 878   78]\n",
            " [  71 1184]]\n",
            "True Negative of the model ada -> 878\n",
            "False Positive of the model ada -> 78\n",
            "False Negative of the model ada -> 71\n",
            "True Positive of the model ada -> 1184\n",
            "Accuracy of the model ada -> 0.9326096788783356\n",
            "Misclassification rate of the model ada -> 0.06739032112166443\n",
            "Precision of the model ada -> 0.9381933438985737\n",
            "Recall of the model ada -> 0.9434262948207172\n",
            "F1 Measure of the model ada -> 0.9408025427095748\n",
            "\n",
            "\n",
            "Confusion matrix of the model cb : \n",
            "[[ 902   54]\n",
            " [  31 1224]]\n",
            "True Negative of the model cb -> 902\n",
            "False Positive of the model cb -> 54\n",
            "False Negative of the model cb -> 31\n",
            "True Positive of the model cb -> 1224\n",
            "Accuracy of the model cb -> 0.9615558570782451\n",
            "Misclassification rate of the model cb -> 0.03844414292175491\n",
            "Precision of the model cb -> 0.9577464788732394\n",
            "Recall of the model cb -> 0.9752988047808765\n",
            "F1 Measure of the model cb -> 0.9664429530201342\n",
            "\n",
            "\n",
            "Confusion matrix of the model gb : \n",
            "[[ 896   60]\n",
            " [  56 1199]]\n",
            "True Negative of the model gb -> 896\n",
            "False Positive of the model gb -> 60\n",
            "False Negative of the model gb -> 56\n",
            "True Positive of the model gb -> 1199\n",
            "Accuracy of the model gb -> 0.947535052012664\n",
            "Misclassification rate of the model gb -> 0.05246494798733603\n",
            "Precision of the model gb -> 0.9523431294678316\n",
            "Recall of the model gb -> 0.9553784860557769\n",
            "F1 Measure of the model gb -> 0.9538583929992045\n",
            "\n",
            "\n",
            "Confusion matrix of the model stacking : \n",
            "[[ 885   71]\n",
            " [  57 1198]]\n",
            "True Negative of the model stacking -> 885\n",
            "False Positive of the model stacking -> 71\n",
            "False Negative of the model stacking -> 57\n",
            "True Positive of the model stacking -> 1198\n",
            "Accuracy of the model stacking -> 0.942107643600181\n",
            "Misclassification rate of the model stacking -> 0.057892356399819045\n",
            "Precision of the model stacking -> 0.9440504334121356\n",
            "Recall of the model stacking -> 0.9545816733067729\n",
            "F1 Measure of the model stacking -> 0.9492868462757528\n",
            "\n",
            "\n",
            "Confusion matrix of the model vh : \n",
            "[[ 886   70]\n",
            " [  83 1172]]\n",
            "True Negative of the model vh -> 886\n",
            "False Positive of the model vh -> 70\n",
            "False Negative of the model vh -> 83\n",
            "True Positive of the model vh -> 1172\n",
            "Accuracy of the model vh -> 0.9308005427408412\n",
            "Misclassification rate of the model vh -> 0.06919945725915877\n",
            "Precision of the model vh -> 0.9436392914653784\n",
            "Recall of the model vh -> 0.9338645418326693\n",
            "F1 Measure of the model vh -> 0.9387264717661192\n",
            "\n",
            "\n",
            "Confusion matrix of the model vs : \n",
            "[[ 895   61]\n",
            " [  93 1162]]\n",
            "True Negative of the model vs -> 895\n",
            "False Positive of the model vs -> 61\n",
            "False Negative of the model vs -> 93\n",
            "True Positive of the model vs -> 1162\n",
            "Accuracy of the model vs -> 0.9303482587064676\n",
            "Misclassification rate of the model vs -> 0.06965174129353235\n",
            "Precision of the model vs -> 0.9501226492232215\n",
            "Recall of the model vs -> 0.9258964143426295\n",
            "F1 Measure of the model vs -> 0.9378531073446327\n",
            "\n",
            "\n",
            "Confusion matrix of the model blend : \n",
            "[[ 885   71]\n",
            " [  73 1182]]\n",
            "True Negative of the model blend -> 885\n",
            "False Positive of the model blend -> 71\n",
            "False Negative of the model blend -> 73\n",
            "True Positive of the model blend -> 1182\n",
            "Accuracy of the model blend -> 0.9348710990502035\n",
            "Misclassification rate of the model blend -> 0.06512890094979651\n",
            "Precision of the model blend -> 0.9433359936153233\n",
            "Recall of the model blend -> 0.9418326693227091\n",
            "F1 Measure of the model blend -> 0.9425837320574163\n",
            "\n",
            "\n",
            "70/70 [==============================] - 0s 1ms/step\n",
            "Confusion matrix of the model ann : \n",
            "[[ 890   66]\n",
            " [  41 1214]]\n",
            "True Negative of the model ann -> 890\n",
            "False Positive of the model ann -> 66\n",
            "False Negative of the model ann -> 41\n",
            "True Positive of the model ann -> 1214\n",
            "Accuracy of the model ann -> 0.9516056083220262\n",
            "Misclassification rate of the model ann -> 0.04839439167797377\n",
            "Precision of the model ann -> 0.9484375\n",
            "Recall of the model ann -> 0.9673306772908367\n",
            "F1 Measure of the model ann -> 0.9577909270216963\n",
            "\n",
            "\n",
            "Confusion matrix of the model mlp : \n",
            "[[ 904   52]\n",
            " [  27 1228]]\n",
            "True Negative of the model mlp -> 904\n",
            "False Positive of the model mlp -> 52\n",
            "False Negative of the model mlp -> 27\n",
            "True Positive of the model mlp -> 1228\n",
            "Accuracy of the model mlp -> 0.9642695612844867\n",
            "Misclassification rate of the model mlp -> 0.03573043871551329\n",
            "Precision of the model mlp -> 0.959375\n",
            "Recall of the model mlp -> 0.9784860557768924\n",
            "F1 Measure of the model mlp -> 0.9688362919132151\n",
            "\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\chand\\Python_Environment\\python\\lib\\site-packages\\sklearn\\base.py:432: UserWarning: X has feature names, but MLPClassifier was fitted without feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "70/70 [==============================] - 0s 2ms/step\n",
            "Confusion matrix of the model rnn : \n",
            "[[ 913   43]\n",
            " [  30 1225]]\n",
            "True Negative of the model rnn -> 913\n",
            "False Positive of the model rnn -> 43\n",
            "False Negative of the model rnn -> 30\n",
            "True Positive of the model rnn -> 1225\n",
            "Accuracy of the model rnn -> 0.9669832654907282\n",
            "Misclassification rate of the model rnn -> 0.03301673450927178\n",
            "Precision of the model rnn -> 0.9660883280757098\n",
            "Recall of the model rnn -> 0.9760956175298805\n",
            "F1 Measure of the model rnn -> 0.9710661910424098\n",
            "\n",
            "\n",
            "70/70 [==============================] - 0s 2ms/step\n",
            "Confusion matrix of the model lstm : \n",
            "[[ 917   39]\n",
            " [  28 1227]]\n",
            "True Negative of the model lstm -> 917\n",
            "False Positive of the model lstm -> 39\n",
            "False Negative of the model lstm -> 28\n",
            "True Positive of the model lstm -> 1227\n",
            "Accuracy of the model lstm -> 0.9696969696969697\n",
            "Misclassification rate of the model lstm -> 0.030303030303030276\n",
            "Precision of the model lstm -> 0.9691943127962085\n",
            "Recall of the model lstm -> 0.9776892430278884\n",
            "F1 Measure of the model lstm -> 0.9734232447441491\n",
            "\n",
            "\n",
            "70/70 [==============================] - 0s 2ms/step\n",
            "Confusion matrix of the model gru : \n",
            "[[ 920   36]\n",
            " [  32 1223]]\n",
            "True Negative of the model gru -> 920\n",
            "False Positive of the model gru -> 36\n",
            "False Negative of the model gru -> 32\n",
            "True Positive of the model gru -> 1223\n",
            "Accuracy of the model gru -> 0.9692446856625961\n",
            "Misclassification rate of the model gru -> 0.03075531433740386\n",
            "Precision of the model gru -> 0.971405877680699\n",
            "Recall of the model gru -> 0.9745019920318725\n",
            "F1 Measure of the model gru -> 0.9729514717581543\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "model_names = ml_names + em_names + dl_names\n",
        "\n",
        "def eval_function():\n",
        "    for model in model_names:\n",
        "        name = model\n",
        "        \n",
        "        if name == \"blend\":\n",
        "            model_prediction = eval(f\"model_{model}.predict(meta_predictions)\")\n",
        "        elif name == \"bagging\":\n",
        "            model_prediction = np.round(eval(f\"model_{model}\"))\n",
        "        elif name == \"vs\":\n",
        "            model_proba = eval(f\"model_{model}.predict_proba(test_data)\")\n",
        "            model_prediction = np.argmax(model_proba, axis=1)\n",
        "        elif name == \"ann\":\n",
        "            model_prediction = np.where(eval(f\"model_{model}.predict(test_data_2d)\")> 0.5, 1, 0)\n",
        "        elif name == \"rnn\" or name == \"lstm\" or name == \"gru\":\n",
        "            model_prediction = np.where(eval(f\"model_{model}.predict(test_data_3d)\")> 0.5, 1, 0)\n",
        "        else:\n",
        "            model_prediction = eval(f\"model_{model}.predict(test_data)\")\n",
        "        \n",
        "        model_cm = confusion_matrix(test_labels, model_prediction)    \n",
        "        tn, fp, fn, tp = model_cm.ravel()\n",
        "        \n",
        "        model_accuracy = accuracy_score(test_labels, model_prediction)\n",
        "        model_misclassification = 1 - accuracy_score(test_labels, model_prediction)\n",
        "        model_precision = precision_score(test_labels, model_prediction)\n",
        "        model_recall = recall_score(test_labels, model_prediction)\n",
        "        model_f1 = f1_score(test_labels, model_prediction)\n",
        "\n",
        "        print(f\"Confusion matrix of the model {name} : \\n{model_cm}\")\n",
        "        print(f'True Negative of the model {name} -> {tn}')\n",
        "        print(f'False Positive of the model {name} -> {fp}')\n",
        "        print(f'False Negative of the model {name} -> {fn}')\n",
        "        print(f'True Positive of the model {name} -> {tp}')\n",
        "        print(f\"Accuracy of the model {name} -> {model_accuracy}\")\n",
        "        print(f\"Misclassification rate of the model {name} -> {model_misclassification}\")\n",
        "        print(f\"Precision of the model {name} -> {model_precision}\")\n",
        "        print(f\"Recall of the model {name} -> {model_recall}\")\n",
        "        print(f\"F1 Measure of the model {name} -> {model_f1}\")\n",
        "        print(\"\\n\")\n",
        "\n",
        "\n",
        "eval_function()\n",
        "print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mf9WcMcjwGkz"
      },
      "source": [
        "# Saving the trained models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFcX0obJwGkz"
      },
      "source": [
        "### Machine learning models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "Uvoi-EBAwGkz"
      },
      "outputs": [],
      "source": [
        "for model in ml_names:\n",
        "    with open(f'ml_models/model_{model}.pkl', 'wb') as f:\n",
        "        pickle.dump(model, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qzaNwoUwGkz"
      },
      "source": [
        "### Ensemble learning models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3rUFrSbKwGkz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "outputId": "d1e7b9bd-7be6-4335-9464-809dc0c936aa"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-a6b9f63c85a3>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mem_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'em_models/model_{model}.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'em_names' is not defined"
          ]
        }
      ],
      "source": [
        "for model in em_names:\n",
        "    with open(f'em_models/model_{model}.pkl', 'wb') as f:\n",
        "        pickle.dump(model, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EehWGH1iwGk0"
      },
      "source": [
        "### Deep learning models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "1movM0MkwGk0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "4973727b-d207-435a-9b68-61635a56a257"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-3b888cca3da0>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdl_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"mlp\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'dl_models/model_{model}.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m             \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'dl_names' is not defined"
          ]
        }
      ],
      "source": [
        "for model in dl_names:\n",
        "    if model == \"mlp\":\n",
        "        with open(f'dl_models/model_{model}.pkl', 'wb') as f:\n",
        "            pickle.dump(model, f)\n",
        "    else:\n",
        "        eval(f\"model_{model}.save('dl_models\\model_{model}.h5')\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IC6n0PQywGk0"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "4bec3b09dd8819171e9e3f7ca9c005a3ddb6766c271b29f60f1c1399645b0b98"
      }
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}